{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot Tutor de Matemáticas y Física basado en Transformer\n",
    "\n",
    "**Informe Final — Curso de Profundización en Deep Learning**\n",
    "\n",
    "Melissa Cardona — Carrera de Física, 2026"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nota para el profesor\n",
    "\n",
    "Este informe documenta el diseño, implementación y evaluación de un **Transformer Encoder–Decoder** entrenado desde cero para resolver problemas de matemáticas y física con soluciones paso a paso.\n",
    "\n",
    "**Para probar el chatbot interactivo**, abra el notebook de demo:\n",
    "\n",
    "```\n",
    "notebooks/03_demo_profesor.ipynb\n",
    "```\n",
    "\n",
    "El modelo ya viene entrenado en `checkpoints/`. No es necesario reentrenar.\n",
    "\n",
    "**Instrucciones rápidas:**\n",
    "1. Clonar o descargar desde: `https://github.com/MelissaCardona2003/Chat-bot-de-matemacticas-y-f-sica-con-TensorFlow.git`\n",
    "2. Crear entorno: `python -m venv .venv && source .venv/bin/activate && pip install -r requirements.txt`\n",
    "3. Abrir: `jupyter notebook notebooks/03_demo_profesor.ipynb`\n",
    "4. Ejecutar todas las celdas — la interfaz Gradio se abrirá automáticamente.\n",
    "\n",
    "Funciona en **CPU**. No se requiere GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Resumen\n",
    "\n",
    "Se presenta el diseño, implementación y evaluación de un modelo **Transformer Encoder–Decoder** construido completamente desde cero en TensorFlow, sin utilizar modelos pre-entrenados ni APIs externas. El sistema recibe problemas de matemáticas (aritmética, álgebra) y física (cinemática, dinámica, termodinámica, circuitos eléctricos) y genera soluciones paso a paso.\n",
    "\n",
    "El modelo (**TransformerV3**) cuenta con **~10,514,849 parámetros**, utiliza tokenización **BPE** (SentencePiece, 4,000 tokens) e incluye una **cabeza de regresión numérica auxiliar** (Answer Head). Fue entrenado sobre un dataset curado de **6,881 problemas** mediante una estrategia innovadora de **tres fases** diseñada para resolver el problema de colapso de cross-attention.\n",
    "\n",
    "Los resultados muestran una **token accuracy del 73.8%** en validación y un **exact match del 3.0%** (3/100). El logro técnico principal es la **cross-attention selectiva** (entropía normalizada 0.52–0.74), que representa una mejora significativa respecto a versiones anteriores donde la cross-attention estaba completamente colapsada (entropía ≈ 1.0). Esto demuestra que el decoder atiende selectivamente al problema de entrada.\n",
    "\n",
    "El proyecto pasó por tres versiones iterativas (v1: character-level → v2: BPE → v3: three-phase + answer head) e incluye un pipeline completo: recolección de datos, tokenización BPE, entrenamiento en tres fases con diversity loss, evaluación rigurosa, y despliegue con interfaz interactiva Gradio.\n",
    "\n",
    "**Palabras clave**: Transformer, Encoder-Decoder, Attention, Cross-Attention, Deep Learning, matemáticas, física, resolución de problemas, TensorFlow, BPE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. Introducción\n",
    "\n",
    "## 1.1 Motivación\n",
    "\n",
    "La resolución automática de problemas de matemáticas y física mediante modelos de lenguaje es una de las fronteras más activas de la inteligencia artificial. Sistemas como GPT-4, Gemini y Claude han demostrado capacidades notables en razonamiento matemático, pero operan con cientos de miles de millones de parámetros, infraestructura masiva y datasets propietarios.\n",
    "\n",
    "Este proyecto nace de una pregunta pedagógica fundamental: **¿qué se puede lograr construyendo un Transformer desde cero, con recursos limitados, para resolver problemas de matemáticas y física?**\n",
    "\n",
    "La motivación es doble:\n",
    "\n",
    "1. **Pedagógica**: Implementar un Transformer completo (no solo usar `transformers.AutoModel`) fuerza a comprender cada componente: atención escalada, positional encoding, máscaras causales, residual connections, layer normalization, y el flujo de información entre encoder y decoder.\n",
    "\n",
    "2. **Científica**: Explorar las limitaciones de un modelo compacto (~10.5M parámetros, tokenización BPE) en tareas que requieren razonamiento numérico, diagnosticar y resolver problemas de entrenamiento como el colapso de cross-attention, y entender *por qué* los LLMs modernos necesitan escalas mucho mayores.\n",
    "\n",
    "## 1.2 Formulación del problema\n",
    "\n",
    "El problema se formula como una transformación secuencia-a-secuencia. Dado un problema $x = (x_1, x_2, \\dots, x_T)$ en lenguaje natural, el objetivo es generar una solución $y = (y_1, y_2, \\dots, y_{T'})$ que contenga los pasos de resolución y la respuesta final.\n",
    "\n",
    "Formalmente, se busca maximizar la probabilidad condicional:\n",
    "\n",
    "$$\n",
    "y^* = \\arg\\max_y P(y \\mid x) = \\arg\\max_y \\prod_{t=1}^{T'} P(y_t \\mid y_{<t}, x)\n",
    "$$\n",
    "\n",
    "donde cada $y_t$ es un token BPE generado condicionado en el problema completo $x$ (a través del encoder) y en los tokens previamente generados $y_{<t}$ (a través del decoder autoregresivo).\n",
    "\n",
    "## 1.3 Alcance\n",
    "\n",
    "El sistema abarca dos dominios:\n",
    "\n",
    "- **Matemáticas**: Problemas aritméticos y algebraicos de nivel escolar (sumas, restas, multiplicaciones, proporciones, porcentajes) con razonamiento verbal (*word problems*).\n",
    "- **Física**: Problemas de cinemática, dinámica newtoniana, termodinámica y circuitos eléctricos con aplicación directa de fórmulas.\n",
    "\n",
    "El formato de salida esperado es:\n",
    "```\n",
    "Step 1: Identify the given values...\n",
    "Step 2: Apply the relevant formula...\n",
    "Step 3: Calculate...\n",
    "Answer: [valor numérico]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Entorno y Herramientas\n",
    "\n",
    "## 2.1 Framework de Deep Learning\n",
    "\n",
    "- **TensorFlow 2.x / Keras**: Framework principal para la implementación del modelo, incluyendo todas las capas (`tf.keras.layers`), el loop de entrenamiento con `GradientTape`, y la gestión de checkpoints.\n",
    "\n",
    "## 2.2 Cómputo numérico y evaluación\n",
    "\n",
    "- **NumPy**: Operaciones vectoriales, muestreo probabilístico (top-k sampling), cálculo de métricas.\n",
    "- **SymPy**: Validación simbólica de respuestas matemáticas (equivalencia algebraica).\n",
    "- **Matplotlib**: Visualización de curvas de entrenamiento y distribuciones de atención.\n",
    "\n",
    "## 2.3 Datos\n",
    "\n",
    "- **HuggingFace Datasets**: Descarga de GSM8K y MATH directamente desde el hub.\n",
    "- **JSON**: Formato de almacenamiento para todos los datasets procesados.\n",
    "- **Funciones propias**: Generación paramétrica de problemas de física, conversión de formatos, filtrado de calidad.\n",
    "\n",
    "## 2.4 Interfaz\n",
    "\n",
    "- **Gradio 4.x**: Interfaz web interactiva para demostración del chatbot.\n",
    "\n",
    "## 2.5 Hardware\n",
    "\n",
    "- **GPU**: NVIDIA RTX 5060 (arquitectura Blackwell, 8 GB VRAM)\n",
    "- **Nota**: La GPU Blackwell requirió workarounds específicos para XLA y operaciones de cast (ver Sección 5.6).\n",
    "- **Inferencia**: Funcional en CPU sin modificaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. Marco Teórico\n",
    "\n",
    "## 3.1 La arquitectura Transformer\n",
    "\n",
    "El Transformer, introducido por Vaswani et al. (2017), reemplaza las redes recurrentes por un mecanismo de **atención puro** que permite procesar secuencias completas en paralelo. La arquitectura consta de un encoder y un decoder, ambos compuestos por capas apiladas de atención y redes feed-forward.\n",
    "\n",
    "### 3.1.1 Scaled Dot-Product Attention\n",
    "\n",
    "El mecanismo fundamental es la atención escalada por producto punto. Dadas las matrices de queries $Q$, keys $K$ y values $V$:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\!\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right) V\n",
    "$$\n",
    "\n",
    "donde $d_k$ es la dimensión de las keys. El factor $\\frac{1}{\\sqrt{d_k}}$ previene que los productos punto crezcan demasiado para valores grandes de $d_k$, lo cual empujaría al softmax hacia regiones de gradiente pequeño.\n",
    "\n",
    "### 3.1.2 Multi-Head Attention\n",
    "\n",
    "En lugar de aplicar una sola función de atención, se proyectan $Q$, $K$ y $V$ en $h$ subespacios diferentes:\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h) W^O\n",
    "$$\n",
    "\n",
    "donde cada cabeza se calcula como:\n",
    "\n",
    "$$\n",
    "\\text{head}_i = \\text{Attention}(Q W_i^Q, K W_i^K, V W_i^V)\n",
    "$$\n",
    "\n",
    "con $W_i^Q, W_i^K \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$, $W_i^V \\in \\mathbb{R}^{d_{\\text{model}} \\times d_v}$ y $W^O \\in \\mathbb{R}^{hd_v \\times d_{\\text{model}}}$.\n",
    "\n",
    "En nuestra implementación (TransformerV3): $d_{\\text{model}} = 256$, $h = 8$, por lo tanto $d_k = d_v = 256/8 = 32$.\n",
    "\n",
    "### 3.1.3 Positional Encoding\n",
    "\n",
    "Dado que el Transformer no posee recurrencia ni convoluciones, se inyecta información posicional mediante funciones sinusoidales:\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i)} = \\sin\\!\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i+1)} = \\cos\\!\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)\n",
    "$$\n",
    "\n",
    "Estas funciones permiten al modelo inferir posiciones relativas, ya que $PE_{pos+k}$ puede expresarse como transformación lineal de $PE_{pos}$.\n",
    "\n",
    "### 3.1.4 Estructura del Encoder\n",
    "\n",
    "Cada capa del encoder consta de:\n",
    "\n",
    "1. **Multi-Head Self-Attention**: Cada posición atiende a todas las demás posiciones de la entrada.\n",
    "2. **Add & Layer Normalization**: Conexión residual seguida de normalización.\n",
    "3. **Feed-Forward Network (FFN)**: Dos capas densas con activación ReLU:\n",
    "\n",
    "$$\n",
    "\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2\n",
    "$$\n",
    "\n",
    "con $W_1 \\in \\mathbb{R}^{d_{\\text{model}} \\times d_{ff}}$ y $W_2 \\in \\mathbb{R}^{d_{ff} \\times d_{\\text{model}}}$. En nuestro caso, $d_{ff} = 1024$.\n",
    "\n",
    "4. **Add & Layer Normalization**.\n",
    "\n",
    "Se aplica dropout ($p=0.2$) después de cada sub-capa.\n",
    "\n",
    "### 3.1.5 Estructura del Decoder\n",
    "\n",
    "Cada capa del decoder añade una sub-capa adicional:\n",
    "\n",
    "1. **Masked Multi-Head Self-Attention**: Atención causal que impide que la posición $t$ atienda a posiciones futuras $t' > t$. Se implementa con una máscara *look-ahead*:\n",
    "\n",
    "$$\n",
    "M_{ij} = \\begin{cases} 0 & \\text{si } i \\geq j \\\\ -\\infty & \\text{si } i < j \\end{cases}\n",
    "$$\n",
    "\n",
    "2. **Cross-Attention** (Encoder–Decoder): Las queries provienen del decoder, mientras que las keys y values provienen del encoder. Esto permite al decoder \"leer\" el problema de entrada.\n",
    "\n",
    "3. **FFN** + Add & Norm (igual que en el encoder).\n",
    "\n",
    "### 3.1.6 Capa de salida\n",
    "\n",
    "La salida del decoder pasa por una capa lineal que produce logits sobre el vocabulario:\n",
    "\n",
    "$$\n",
    "\\text{logits}_t = W_{\\text{out}} \\cdot h_t^{(L)} + b_{\\text{out}} \\in \\mathbb{R}^{|\\mathcal{V}|}\n",
    "$$\n",
    "\n",
    "donde $|\\mathcal{V}| = 4000$ es el tamaño del vocabulario BPE.\n",
    "\n",
    "## 3.2 Función de pérdida\n",
    "\n",
    "Se utiliza entropía cruzada categórica dispersa con **label smoothing** ($\\epsilon = 0.1$):\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -\\sum_{t=1}^{T'} \\left[ (1 - \\epsilon) \\log P(y_t^{\\text{true}}) + \\frac{\\epsilon}{|\\mathcal{V}|} \\sum_{k} \\log P(k) \\right] \\cdot \\mathbb{1}[y_t \\neq \\text{PAD}]\n",
    "$$\n",
    "\n",
    "El label smoothing previene que el modelo se vuelva excesivamente confiado en sus predicciones, actuando como regularizador.\n",
    "\n",
    "Las posiciones correspondientes al token `<PAD>` se enmascaran para no contribuir a la pérdida.\n",
    "\n",
    "## 3.3 Learning Rate Schedule\n",
    "\n",
    "Se utiliza el esquema propuesto en el paper original (Vaswani et al., 2017):\n",
    "\n",
    "$$\n",
    "lr = d_{\\text{model}}^{-0.5} \\cdot \\min\\!\\left(\\text{step}^{-0.5}, \\; \\text{step} \\cdot \\text{warmup}^{-1.5}\\right)\n",
    "$$\n",
    "\n",
    "con $\\text{warmup\\_steps} = 1000$. El learning rate crece linealmente durante los primeros 1000 pasos y luego decae proporcionalmente a $\\text{step}^{-0.5}$. Se incluye un factor de escala $s$ para las fases de fine-tuning:\n",
    "\n",
    "En la Fase 3 se usa $s = 0.1$ para fine-tuning suave.\n",
    "\n",
    "$$\n",
    "\n",
    "lr(\\text{step}) = s \\cdot d_{\\text{model}}^{-0.5} \\cdot \\min(\\text{step}^{-0.5}, \\text{step} \\cdot \\text{warmup}^{-1.5})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. Datasets y Preparación de Datos\n",
    "\n",
    "## 4.1 Fuentes de datos\n",
    "\n",
    "La construcción del dataset fue uno de los retos principales del proyecto. Se requieren pares **(problema, solución paso a paso)** donde la solución contenga razonamiento explícito y una respuesta final.\n",
    "\n",
    "### 4.1.1 GSM8K (Grade School Math 8K)\n",
    "\n",
    "Dataset de OpenAI (Cobbe et al., 2021) con 8,792 problemas aritméticos verbales de nivel escolar. Cada problema incluye una solución con razonamiento paso a paso y una respuesta final numérica.\n",
    "\n",
    "- **Conversión**: Se descarga vía HuggingFace y se convierte al esquema unificado del proyecto con `data/convert_gsm8k.py`.\n",
    "- **Limpieza**: Eliminación de anotaciones tipo `#### ANSWER`, normalización de formato, filtrado por longitud.\n",
    "- **Resultado**: 8,638 problemas limpios (7,319 train + 1,319 test).\n",
    "\n",
    "### 4.1.2 MATH Dataset con soluciones LLM\n",
    "\n",
    "El dataset MATH (Hendrycks et al., 2021) contiene problemas de competencia matemática con soluciones en LaTeX. Dado que las soluciones originales usan notación LaTeX compleja incompatible con tokenización carácter a carácter, se implementó un pipeline alternativo:\n",
    "\n",
    "1. **Descarga y filtrado** (`data/download_dataset.py`, `data/filter_dataset.py`): Se seleccionaron niveles 1-3 en álgebra, combinatoria y prealgebra.\n",
    "2. **Generación de soluciones** (`data/generate_math_solutions_llm.py`): Se utilizó un LLM para generar soluciones en texto plano paso a paso, reemplazando el LaTeX por lenguaje natural.\n",
    "3. **Resultado**: 1,895 problemas con soluciones legibles.\n",
    "\n",
    "### 4.1.3 Problemas de Física (Generación Paramétrica)\n",
    "\n",
    "No se encontró un dataset público de física análogo a GSM8K. Se optó por generar problemas de forma paramétrica (`data/generate_physics_templates.py`):\n",
    "\n",
    "Se definieron templates en 5 áreas:\n",
    "\n",
    "| Área | Fórmulas | Ejemplo |\n",
    "|------|----------|---------|\n",
    "| Cinemática | $v = v_0 + at$, $x = v_0 t + \\frac{1}{2}at^2$ | \"A car accelerates at 3 m/s² for 5s...\" |\n",
    "| Dinámica | $F = ma$, $W = Fd$ | \"A 10 kg box is pushed with 50 N...\" |\n",
    "| Termodinámica | $Q = mc\\Delta T$ | \"Heat needed for 2 kg water, ΔT=30°C...\" |\n",
    "| Circuitos | $V = IR$, $P = IV$ | \"Circuit with 12V and 4Ω...\" |\n",
    "| Caída libre | $h = \\frac{1}{2}gt^2$ | \"Object dropped from 80 m...\" |\n",
    "\n",
    "Cada template genera problemas con parámetros aleatorios dentro de rangos físicamente razonables, junto con la solución correcta calculada analíticamente.\n",
    "\n",
    "- **Resultado**: 2,035 problemas de física (2,019 paramétricos + 16 manuales).\n",
    "\n",
    "## 4.2 Esquema Unificado\n",
    "\n",
    "Todos los datos se normalizan a un esquema JSON común (`data/schema.py`):\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"problem\": \"A car accelerates from rest at 3 m/s² for 5 seconds...\",\n",
    "  \"solution\": \"Step 1: Identify... Step 2: Apply... Answer: 15 m/s\",\n",
    "  \"answer\": \"15\",\n",
    "  \"domain\": \"physics\",\n",
    "  \"source\": \"physics_templates\",\n",
    "  \"topic\": \"kinematics\",\n",
    "  \"split\": \"train\"\n",
    "}\n",
    "```\n",
    "\n",
    "La función de validación verifica que cada entrada tenga los campos requeridos, que `problem` y `solution` no estén vacíos, que la longitud sea razonable, y que el `domain` sea `math` o `physics`.\n",
    "\n",
    "## 4.3 Dataset Final: Easy Subset\n",
    "\n",
    "Para la versión v3, se seleccionó un subconjunto curado de problemas de longitud corta/media (`combined_easy.json`) para optimizar el aprendizaje con un modelo compacto:\n",
    "\n",
    "| | Math | Physics | Total |\n",
    "|-----|------|---------|-------|\n",
    "| Train | ~4,800 | ~930 | **5,729** |\n",
    "| Val | ~420 | ~89 | **509** |\n",
    "| Test | ~550 | ~93 | **643** |\n",
    "| **Total** | **~5,770** | **~1,111** | **6,881** |\n",
    "\n",
    "## 4.4 Tokenización BPE\n",
    "\n",
    "Se implementó un tokenizador **BPE (Byte Pair Encoding)** usando SentencePiece (`data/subword_tokenizer.py`) con vocabulario de 4,000 tokens:\n",
    "\n",
    "$$\n",
    "\\mathcal{V} = \\{\\texttt{<PAD>}, \\texttt{<START>}, \\texttt{<END>}, \\texttt{<UNK>}\\} \\cup \\{\\text{subword}_1, \\dots, \\text{subword}_{3996}\\}\n",
    "$$\n",
    "\n",
    "El tokenizador BPE se entrena con `split_digits=True` para que cada dígito sea un token individual, y `byte_fallback=True` para manejar caracteres desconocidos.\n",
    "\n",
    "**Ventaja sobre tokenización carácter a carácter**: Un problema de 100 palabras se tokeniza en ~30-50 tokens BPE, vs ~500 tokens con caracteres. Esto reduce la longitud de secuencia ~5× y permite al modelo operar a nivel de concepto.\n",
    "\n",
    "## 4.5 Construcción de `tf.data.Dataset`\n",
    "\n",
    "El `DatasetBuilder` (`data/dataset_builder.py`) construye pipelines eficientes:\n",
    "\n",
    "1. Carga el JSON del dataset.\n",
    "2. Tokeniza problemas y soluciones con BPE y padding a longitudes fijas (`max_encoder_len=128`, `max_decoder_len=256`).\n",
    "3. Extrae el **valor numérico de la respuesta** para el answer head.\n",
    "4. Construye pares `((encoder_input, decoder_input), (decoder_target, answer_value))`.\n",
    "5. Aplica shuffle, batching y prefetch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5. Implementación\n",
    "\n",
    "Todo el código fuente se organizó como un paquete Python (`transformer_math_physics_tutor/`) con módulos separados para cada componente.\n",
    "\n",
    "## 5.1 Multi-Head Attention (`models/multihead_attention.py`)\n",
    "\n",
    "La implementación sigue fielmente el paper original. La clase `MultiHeadAttention` recibe `d_model` y `num_heads`, crea las proyecciones $W^Q, W^K, W^V, W^O$ como capas `Dense`, y ejecuta:\n",
    "\n",
    "1. Proyección: $Q' = QW^Q$, $K' = KW^K$, $V' = VW^V$\n",
    "2. Reshape a `(batch, heads, seq_len, depth)` donde `depth = d_model / num_heads = 32`\n",
    "3. Scaled dot-product attention con máscara opcional\n",
    "4. Concatenación de cabezas y proyección de salida\n",
    "\n",
    "## 5.2 Encoder y Decoder (`models/encoder_layer.py`, `models/decoder_layer.py`)\n",
    "\n",
    "- **EncoderLayer**: Self-attention → Add & Norm → FFN → Add & Norm\n",
    "- **DecoderLayer**: Masked self-attention → Add & Norm → Cross-attention → Add & Norm → FFN → Add & Norm\n",
    "\n",
    "El decoder devuelve además un diccionario de pesos de atención, lo cual permite análisis post-hoc de qué tokens del encoder son más atendidos.\n",
    "\n",
    "## 5.3 TransformerV3 (`models/transformer_v3.py`)\n",
    "\n",
    "La clase `TransformerV3` extiende el `Transformer` base añadiendo una **cabeza de regresión numérica** (Answer Head) que predice el valor numérico de la respuesta directamente desde la salida del encoder:\n",
    "\n",
    "1. **Mean pooling** de la salida del encoder (enmascarando tokens PAD)\n",
    "2. **MLP de 2 capas**: `Dense(d_model, ReLU) → Dropout(0.1) → Dense(1)`\n",
    "3. **Huber loss** sobre valores escalados por `ANSWER_SCALE=1000`\n",
    "\n",
    "Esto fuerza al encoder a codificar información numérica explotable, lo cual mejora la calidad de las representaciones para la cross-attention del decoder.\n",
    "\n",
    "El forward pass: `model((encoder_input, decoder_input))` → `(logits, answer_pred)` donde logits tiene shape `(batch, seq_len, vocab_size=4000)` y answer_pred tiene shape `(batch,)`.\n",
    "## 5.4 Loop de Entrenamiento (`training/trainer.py`)\n",
    "Integra:\n",
    "Se implementó un `TransformerTrainerV3` con `tf.GradientTape` en lugar de `model.fit()`, lo que permite:\n",
    "- Positional encoding sinusoidal\n",
    "- **Loss combinada**: `loss = seq_loss + λ₁ · answer_loss + λ₂ · diversity_loss`\n",
    "  - `seq_loss`: Cross-entropy con label smoothing\n",
    "  - `answer_loss`: Huber loss para regresión numérica del answer head\n",
    "  - `diversity_loss`: Penaliza distribución uniforme de cross-attention (fuerza selectividad)\n",
    "- **Decoder token masking**: Reemplazo aleatorio del **35%** de los tokens del decoder input por IDs aleatorios. Esto obliga al modelo a usar cross-attention para obtener información del encoder.\n",
    "- **Gradient clipping** (global norm = 1.0) para estabilidad.\n",
    "- **Checkpointing** con rotación.\n",
    "Implementa el esquema del paper original con un factor de escala $s$:\n",
    "- **Congelamiento selectivo de capas** para entrenamiento por fases.\n",
    "\n",
    "lr(\\text{step}) = s \\cdot d_{\\text{model}}^{-0.5} \\cdot \\min(\\text{step}^{-0.5}, \\text{step} \\cdot \\text{warmup}^{-1.5})\n",
    "\n",
    "Implementa el esquema del paper original:\n",
    "Con `warmup_steps=1000`, el LR máximo alcanza aproximadamente $4.4 \\times 10^{-4}$. El factor $s=0.1$ se usa en la Fase 3 (fine-tuning).\n",
    "$$\n",
    "lr(\\text{step}) = d_{\\text{model}}^{-0.5} \\cdot \\min(\\text{step}^{-0.5}, \\text{step} \\cdot \\text{warmup}^{-1.5})\n",
    "$$\n",
    "\n",
    "Con `warmup_steps=2000`, el LR máximo alcanza aproximadamente $3.7 \\times 10^{-4}$.\n",
    "\n",
    "## 5.6 Workarounds para GPU Blackwell\n",
    "3. **XLA Dropout** (`models/xla_dropout.py`): Implementación de dropout compatible con la compilación XLA, usando `tf.random.uniform` en lugar de `tf.nn.dropout`.\n",
    "\n",
    "4. **`@tf.function` con `input_signature` fijo**: Para evitar re-tracing excesivo en GPUs Blackwell.\n",
    "La GPU NVIDIA RTX 5060 (basada en la arquitectura Blackwell) presentó problemas con operaciones XLA y `tf.cast`, que fallaban con errores de segmentación. Se implementaron dos workarounds:\n",
    "Con `warmup_steps=2000`, el LR máximo alcanza aproximadamente $3.7 \\times 10^{-4}$.3. **XLA Dropout** (`models/xla_dropout.py`): Implementación de dropout compatible con la compilación XLA, usando `tf.random.stateless_uniform` en lugar de `tf.nn.dropout`.\n",
    "\n",
    "\n",
    "2. **Cast patch**: Wrapper de `tf.cast` que ejecuta conversiones de tipo en CPU para evitar el bug de Blackwell en modo eager.\n",
    "\n",
    "1. **XLA flags**: Configuración de `XLA_FLAGS` y `TF_XLA_FLAGS` para apuntar al directorio correcto de CUDA 12.8.\n",
    "## 5.6 Workarounds para GPU Blackwell1. **XLA flags**: Configuración de `XLA_FLAGS` y `TF_XLA_FLAGS` para apuntar al directorio correcto de CUDA 12.8.\n",
    "\n",
    "2. **Cast patch**: Wrapper de `tf.cast` que ejecuta conversiones de tipo en CPU para evitar el bug de Blackwell en modo eager.\n",
    "\n",
    "\n",
    "3. **XLA Dropout** (`models/xla_dropout.py`): Implementación de dropout compatible con la compilación XLA, usando `tf.random.stateless_uniform` en lugar de `tf.nn.dropout`.La GPU NVIDIA RTX 5060 (basada en la arquitectura Blackwell) presentó problemas con operaciones XLA y `tf.cast`, que fallaban con errores de segmentación. Se implementaron dos workarounds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Configuración del entorno ===\n",
    "import os, sys, json\n",
    "import numpy as np\n",
    "\n",
    "# Ruta del proyecto\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Cargar configuración y training history\n",
    "checkpoint_dir = os.path.join(project_root, 'checkpoints', 'v3_easy')\n",
    "\n",
    "with open(os.path.join(checkpoint_dir, 'config.json'), 'r') as f:\n",
    "    config_dict = json.load(f)\n",
    "\n",
    "with open(os.path.join(checkpoint_dir, 'training_history.json'), 'r') as f:\n",
    "    history_raw = json.load(f)\n",
    "    history = history_raw.get('phase3', history_raw)\n",
    "\n",
    "with open(os.path.join(checkpoint_dir, 'evaluation_report.json'), 'r') as f:\n",
    "    eval_report = json.load(f)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CONFIGURACION DEL MODELO (TransformerV3)\")\n",
    "print(\"=\" * 60)\n",
    "for k, v in config_dict.items():\n",
    "    print(f\"  {k:20s}: {v}\")\n",
    "print(f\"\\n  Tokenización: BPE (SentencePiece, {config_dict.get('vocab_size', 4000)} tokens)\")\n",
    "\n",
    "print(f\"  Entrenamiento: 3 fases (P1:30 + P2:100 + P3:50 épocas)\")print(f\"  Tiempo total fase 3: {history_raw.get('phase3_time_s', 0):.0f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6. Entrenamiento\n",
    "\n",
    "## 6.1 Configuración de hiperparámetros\n",
    "\n",
    "| Hiperparámetro | Valor | Justificación |\n",
    "|----------------|-------|---------------|\n",
    "| `d_model` | 256 | Balance entre capacidad y eficiencia |\n",
    "| `num_heads` | 8 | Estándar para $d_k = 32$ |\n",
    "| `num_layers` | 4 | Suficiente para capturar patrones sin overfitting |\n",
    "| `dff` | 1024 | Ratio 4:1 con $d_{\\text{model}}$ (como en el paper original) |\n",
    "| `dropout_rate` | 0.2 | Mayor que el default (0.1) por dataset pequeño |\n",
    "| `batch_size` | 32 | Limitado por GPU memory |\n",
    "| `warmup_steps` | 1000 | ~3 épocas de warmup |\n",
    "| `label_smoothing` | 0.1 | Regularización estándar |\n",
    "| `decoder_mask_rate` | 0.35 | Forzar uso de cross-attention (35%) |\n",
    "| `max_encoder_len` | 128 | Optimizado para dataset easy |\n",
    "| `max_decoder_len` | 256 | Optimizado para dataset easy |\n",
    "| `vocab_size` | 4,000 | BPE con SentencePiece |\n",
    "\n",
    "## 6.2 Estrategia de Entrenamiento en Tres Fases\n",
    "\n",
    "El entrenamiento en tres fases se diseñó para resolver el **colapso de cross-attention** (entropía normalizada ≈ 1.0), donde el decoder ignora completamente la salida del encoder.\n",
    "\n",
    "### Fase 1 — Pre-entrenamiento del Encoder (30 épocas)\n",
    "- **Congelar**: Decoder + Final Layer\n",
    "- **Entrenar**: Encoder + Answer Head\n",
    "- **Loss**: Solo `answer_loss` (Huber, weight=10.0)\n",
    "- **Objetivo**: Forzar al encoder a producir representaciones que codifiquen información numérica del problema\n",
    "\n",
    "### Fase 2 — Entrenamiento del Decoder (100 épocas)\n",
    "- **Reinicializar**: Pesos de cross-attention (Q, K, V) con Glorot uniform — rompe la simetría colapsada\n",
    "- **Congelar**: Encoder (preservar representaciones)\n",
    "- **Entrenar**: Decoder + Final Layer\n",
    "- **Loss**: `seq_loss + 5·answer_loss + 10·diversity_loss`\n",
    "- **Decoder masking**: 35%\n",
    "- **Objetivo**: El decoder aprende a atender selectivamente al encoder\n",
    "\n",
    "**Observación clave**: La val_accuracy (73.8%) es mayor que la train_accuracy (64.9%). Esto es típico con dropout alto (0.2) y decoder masking agresivo (35%), que penalizan fuertemente el entrenamiento pero no se aplican en validación.\n",
    "\n",
    "### Fase 3 — Fine-tuning Completo (50 épocas)\n",
    "\n",
    "- **Descongelar todo**: Todos los parámetros entrenables| Tiempo Fase 3 | ~1,792s (~30 min) |\n",
    "\n",
    "- **LR reducido**: `lr_scale = 0.1` para ajuste fino| Val Accuracy | **73.8%** |\n",
    "\n",
    "- **Loss**: `seq_loss + 5·answer_loss + 2·diversity_loss`| Val Loss | 2.383 |\n",
    "\n",
    "- **Objetivo**: Refinamiento conjunto de toda la red| Train Accuracy | 64.9% |\n",
    "\n",
    "| Train Loss (final) | 2.915 |\n",
    "\n",
    "### Resultados del entrenamiento (Fase 3 — última fase):|---------|-------|\n",
    "\n",
    "| Métrica | Valor |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Curvas de entrenamiento (Fase 3) ===\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "epochs = range(1, len(history.get('train_loss', [])) + 1)\n",
    "\n",
    "if history.get('train_loss'):\n",
    "    # Loss\n",
    "    axes[0].plot(epochs, history['train_loss'], label='Train', linewidth=2, color='#4f46e5')\n",
    "    axes[0].plot(epochs, history['val_loss'], label='Val', linewidth=2, color='#f97316')\n",
    "    axes[0].set_title('Loss (Fase 3)', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xlabel('Época')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Accuracy\n",
    "    axes[1].plot(epochs, history['train_accuracy'], label='Train', linewidth=2, color='#4f46e5')\n",
    "    axes[1].plot(epochs, history['val_accuracy'], label='Val', linewidth=2, color='#f97316')\n",
    "    axes[1].set_title('Token Accuracy (Fase 3)', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel('Época')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTrain Acc final: {history.get('train_accuracy',[-1])[-1]:.4f}\")\n",
    "print(f\"Val Acc final:   {history.get('val_accuracy',[-1])[-1]:.4f}\")\n",
    "print(f\"Val Loss final:  {history.get('val_loss',[-1])[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 7. Inferencia\n",
    "\n",
    "## 7.1 Generación autoregresiva\n",
    "\n",
    "Durante entrenamiento se usa **teacher forcing**: el decoder recibe la secuencia objetivo desplazada. Durante inferencia, el decoder opera de forma autoregresiva:\n",
    "\n",
    "1. Se codifica el problema con el encoder: $H = \\text{Encoder}(x)$\n",
    "2. Se inicializa el decoder con el token `<START>`\n",
    "3. En cada paso $t$:\n",
    "   - Se computan los logits: $z_t = \\text{Decoder}(y_{<t}, H)$\n",
    "   - Se selecciona el siguiente token $\\hat{y}_t$\n",
    "   - Se añade al decoder input\n",
    "4. Se repite hasta generar `<END>` o alcanzar `max_length=300`\n",
    "\n",
    "## 7.2 Estrategias de decodificación\n",
    "\n",
    "Se implementaron tres estrategias en `inference/generate.py`:\n",
    "\n",
    "### Greedy decoding\n",
    "$$\n",
    "\\hat{y}_t = \\arg\\max_k z_t[k]\n",
    "$$\n",
    "\n",
    "Determinístico pero propenso a generar secuencias repetitivas.\n",
    "\n",
    "### Top-k sampling\n",
    "Se restringen los logits a los $k$ tokens más probables, se aplica temperatura $\\tau$ y se muestrea:\n",
    "\n",
    "$$\n",
    "P(y_t = k) = \\frac{\\exp(z_t[k] / \\tau)}{\\sum_{j \\in \\text{top-}k} \\exp(z_t[j] / \\tau)}\n",
    "$$\n",
    "\n",
    "En la demo se usa $k=10$, $\\tau=0.3$.\n",
    "\n",
    "### Repetition penalty\n",
    "Para evitar bucles, se penalizan tokens ya generados:\n",
    "\n",
    "$$\n",
    "z_t[k] \\leftarrow \\begin{cases} z_t[k] / \\alpha & \\text{si } z_t[k] > 0 \\text{ y } k \\in \\text{generados} \\\\ z_t[k] \\cdot \\alpha & \\text{si } z_t[k] \\leq 0 \\text{ y } k \\in \\text{generados} \\end{cases}\n",
    "$$\n",
    "\n",
    "con $\\alpha = 1.3$. Adicionalmente, se implementa detección de n-gram repeat ($n=10$) para terminar anticipadamente si el modelo entra en un bucle.\n",
    "\n",
    "## 7.3 Métricas en tiempo real\n",
    "\n",
    "La demo calcula métricas basadas en los logits reales del modelo:\n",
    "\n",
    "- **Confianza**: $\\text{conf} = \\frac{1}{T}\\sum_{t=1}^{T} \\max_v P(v \\mid v_{<t})$ — promedio de la probabilidad máxima por paso.\n",
    "- **Perplexity**: $\\text{PPL} = \\exp\\!\\left(-\\frac{1}{T}\\sum_{t=1}^{T} \\log P(y_t \\mid y_{<t})\\right)$ — sobre los tokens seleccionados.\n",
    "\n",
    "## 7.4 Interfaz Gradio\n",
    "\n",
    "Se desarrolló una interfaz Gradio Blocks (`notebooks/03_demo_profesor.ipynb`) con:\n",
    "- Selector de dominio (math/physics)\n",
    "- Ejemplos pre-cargados por dominio\n",
    "- Métricas en tiempo real (confianza, perplexity, tiempo, tokens generados)\n",
    "- Sección de arquitectura del modelo\n",
    "- Curvas de entrenamiento\n",
    "- Resultados de evaluación\n",
    "- Análisis de limitaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 8. Evaluación y Resultados\n",
    "\n",
    "## 8.1 Métricas utilizadas\n",
    "\n",
    "Se evaluó el modelo con múltiples familias de métricas:\n",
    "\n",
    "### Token-level Accuracy\n",
    "Proporción de tokens BPE correctamente predichos (ignorando padding).\n",
    "\n",
    "### Exact Match (Answer:)\n",
    "Se extrae la línea `Answer:` de la predicción y la referencia, y se comparan. Se realiza comparación textual exacta y numérica con tolerancia (±0.5).\n",
    "\n",
    "### Cross-Attention Entropy\n",
    "\n",
    "Entropía normalizada de la distribución de cross-attention por capa del decoder. Valores < 0.85 indican atención selectiva.## 8.2 Resultados cuantitativos\n",
    "\n",
    "\n",
    "\n",
    "### Answer Head MetricsMAE y exact match del answer head de regresión (predicción directa del valor numérico, sin generación secuencial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Resultados de evaluacion ===\n",
    "ta = eval_report.get('token_accuracy', {})\n",
    "em = eval_report.get('exact_match', {})\n",
    "ce = eval_report.get('cross_attention_entropy', {})\n",
    "ah = eval_report.get('answer_head', {})\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RESULTADOS DE EVALUACION (TransformerV3)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n  Token Accuracy (validación): {ta.get('val_acc', 0):.1%}\")\n",
    "print(f\"  Token Accuracy (test):       {ta.get('test_acc', 0):.1%}\")\n",
    "print(f\"  Val Loss:                    {ta.get('val_loss', 0):.4f}\")\n",
    "print(f\"  Test Loss:                   {ta.get('test_loss', 0):.4f}\")\n",
    "\n",
    "print(f\"\\n  Answer Head MAE:             {ah.get('mae', 0):.1f}\")\n",
    "print(f\"  Answer Head Exact (±0.5):   {ah.get('exact_pct', 0):.1f}%\")\n",
    "\n",
    "textual_pct = em.get('textual_pct', 0)\n",
    "numeric_pct = em.get('numeric_pct', 0)\n",
    "print(f\"\\n  Exact Match (textual): {em.get('textual_correct', 0)}/{em.get('textual_total', 0)} = {textual_pct:.1f}%\")\n",
    "print(f\"  Exact Match (numérico ±0.5): {em.get('numeric_correct', 0)}/{em.get('numeric_total', 0)} = {numeric_pct:.1f}%\")\n",
    "\n",
    "print(f\"\\n  Cross-Attention Entropy (normalizada):\")\n",
    "\n",
    "for layer, entropy in sorted(ce.items()):        print(f\"    {domain}: {nc}/{nt} = {pct:.1f}%\")\n",
    "\n",
    "    status = 'SELECTIVA' if entropy < 0.85 else 'UNIFORME'        pct = nc / max(nt, 1) * 100\n",
    "\n",
    "    print(f\"    {layer}: {entropy:.3f} — {status}\")        nt = stats.get('numeric_total', stats.get('total', 1))\n",
    "\n",
    "        nc = stats.get('numeric_correct', stats.get('correct', 0))\n",
    "\n",
    "if em.get('by_domain'):    for domain, stats in sorted(em['by_domain'].items()):\n",
    "    print(f\"\\n  Por dominio:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Análisis de los resultados\n",
    "\n",
    "### Cross-attention: de colapsada a selectiva\n",
    "\n",
    "El logro técnico principal de v3 es la **resolución del colapso de cross-attention**. En las versiones v1 y v2, la entropía normalizada de la cross-attention era ≈1.0 (distribución uniforme), lo que significaba que el decoder ignoraba completamente el encoder. Mediante la estrategia de tres fases con reinicialización de cross-attention y diversity loss, se logró reducir la entropía a 0.52–0.74 (selectiva).\n",
    "\n",
    "### Token accuracy vs exact match\n",
    "\n",
    "La discrepancia entre 73.8% token accuracy y 3.0% exact match refleja que el modelo aprende el **formato y estructura** de las soluciones (*\"Step 1:... Step 2:... Answer:...\"*) pero no el **razonamiento numérico** para producir la respuesta correcta.\n",
    "\n",
    "Con tokenización BPE (vs character-level en v1), la longitud de secuencia se reduce ~5×, permitiendo al modelo operar a nivel de subpalabra. Sin embargo, el modelo aún no puede \"copiar\" números del problema al resultado — debe generar cada token de la respuesta desde el vocabulario fijo.\n",
    "\n",
    "### Answer Head\n",
    "\n",
    "El answer head alcanza 62.2% de exact match (±0.5) en predicción directa del valor numérico. Esto confirma que el **encoder sí codifica información numérica** del problema, pero esta información no se transfiere completamente al decoder durante la generación secuencial.\n",
    "\n",
    "### Limitación fundamental: ausencia de mecanismo de copia\n",
    "\n",
    "El Transformer estándar genera tokens de un vocabulario fijo. No tiene la capacidad de \"copiar\" tokens directamente de la entrada. En tareas donde la respuesta contiene números que aparecen en el problema (e.g., *\"A car travels at 60 km/h...\"* → *\"Answer: 180 km\"*), un mecanismo de copia (pointer network) permitiría al decoder copiar selectivamente tokens del encoder cuando sean relevantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 9. Limitaciones, Experimentos Fallidos y Trabajo Futuro\n",
    "\n",
    "## 9.1 Experimento V4: Mecanismo de copia (Pointer-Generator) — Resultado Negativo\n",
    "\n",
    "### Motivación\n",
    "\n",
    "Con V3 logrando solo 3% de exact match, la hipótesis era: \"el modelo no puede copiar números del problema al output porque genera cada token desde un vocabulario fijo\". Se implementó un **Pointer-Generator Network** (See et al. 2017) para permitir al decoder copiar tokens directamente del input del encoder.\n",
    "\n",
    "### Arquitectura V4 — Qué se implementó\n",
    "\n",
    "TransformerV4 extiende TransformerV3 con tres componentes adicionales:\n",
    "\n",
    "1. **Gate p_gen** (`Dense(1, sigmoid, bias_init=1.0)`): Para cada token del decoder, decide si generar del vocabulario (p_gen→1) o copiar del input (p_gen→0). El bias se inicializa en 1.0 para que comience generando.\n",
    "\n",
    "2. **Distribución de copia**: Se promedian los pesos de cross-attention de las últimas 2 capas del decoder (promediando los 8 heads). Esto produce una distribución de probabilidad sobre los tokens del input que indica \"de dónde copiar\".\n",
    "\n",
    "3. **Probabilidades finales blended**:\n",
    "   ```\n",
    "   P(token) = p_gen × P_vocab(token) + (1 - p_gen) × P_copy(token)\n",
    "   ```\n",
    "\n",
    "**Archivos creados** (todos en local, NO en GitHub):\n",
    "- `models/transformer_v4.py` — Arquitectura con gate p_gen y copy distribution\n",
    "- `training/trainer_v4.py` — Trainer con copy_loss (NLL sobre probabilidades) y monitoreo de p_gen\n",
    "- `inference/generate_v4.py` — Generación autoregresiva con probabilidades blended\n",
    "- `run_training_v4.py` — Entrenamiento en 3 fases con transfer learning desde V3\n",
    "- `resume_phase3.py` — Reanudación de Phase 3 tras OOM (batch_size 32→16)\n",
    "- `evaluation/evaluate_v4.py` — Evaluación completa con análisis de p_gen\n",
    "\n",
    "### Entrenamiento V4 — Protocolo de 3 fases\n",
    "\n",
    "| Fase | Épocas | Qué se entrena | Qué se congela | Resultado |\n",
    "|------|--------|----------------|----------------|-----------|\n",
    "| **1** | 30 | Encoder + Answer Head + p_gen gate | Decoder | Train Acc=75.3%, p_gen=0.981 |\n",
    "| **2** | 100 | Decoder + Final Layer + p_gen (cross-attn reinicializada) | Encoder | Train Acc=62.5%, p_gen=0.991 |\n",
    "| **3** | 50 | TODO desbloqueado (lr_scale=0.1) | Nada | Train Acc=64.5%, p_gen=0.991 |\n",
    "\n",
    "**Problema técnico en Phase 3**: El entrenamiento fue matado por el OOM killer de Linux (`SIGKILL`, Exit 137) en la época 15/50. Al desbloquear todos los parámetros, el optimizador Adam necesita almacenar estados m (media) y v (varianza) para CADA parámetro, triplicando el uso de memoria. **Solución**: Reducir batch_size de 32 a 16 y reiniciar desde los pesos de Phase 2.\n",
    "\n",
    "### Resultado: NEGATIVO — El copy mechanism NO funciona\n",
    "\n",
    "| Métrica | V3 (mejor) | V4 (Pointer-Generator) | Diferencia |\n",
    "|---|---|---|---|\n",
    "| Token Acc (val) | 73.8% | 72.5% | **-1.3%** |\n",
    "| Token Acc (test) | 69.9% | 68.9% | **-1.0%** |\n",
    "| Exact Match (textual) | 3.0% | 1.0% | **-2.0%** |\n",
    "| Exact Match (numérico) | 3.5% | 1.4% | **-2.1%** |\n",
    "| Answer Head MAE | 298.8 | 283.4 | +15.4 (mejor) |\n",
    "| p_gen promedio | — | 0.9948 | No copia |\n",
    "| Math exact match | N/A | 0/88 = 0% | — |\n",
    "| Physics exact match | N/A | 1/12 = 8.3% | — |\n",
    "\n",
    "**El síntoma más grave**: El modelo V4 genera fórmulas de física para problemas de matemáticas. Por ejemplo:\n",
    "- Input: *\"Natalia has 3 apples and buys 5 more\"* → Output: *\"Step 1: Use KE = ½mv². Step 2: KE = 0.5 × 10 × 5² = 125 J\"*\n",
    "- Esto indica que la Phase 2 (reinicialización de cross-attention + mecanismo de copia) confundió al decoder, y Phase 3 no recuperó la calidad.\n",
    "\n",
    "### Análisis detallado: ¿Por qué falló?\n",
    "\n",
    "**Causa raíz 1: BPE con `split_digits=True` elimina la necesidad de copiar**\n",
    "\n",
    "El copy mechanism fue diseñado originalmente para summarization (See et al. 2017), donde hay nombres propios y palabras raras que NO están en el vocabulario (OOV). En nuestro caso:\n",
    "- El tokenizer BPE con `split_digits=True` descompone CUALQUIER número en dígitos individuales: \"125\" → tokens [\"1\", \"2\", \"5\"]\n",
    "- Todos los dígitos 0-9 existen en el vocabulario de 4,000 tokens\n",
    "- **No hay tokens que necesiten copiarse** — el modelo YA puede generar cualquier número\n",
    "\n",
    "**Causa raíz 2: Sin supervisión explícita para p_gen**\n",
    "\n",
    "El gate p_gen se entrena solo por backpropagation del loss de generación. No hay una señal explícita que diga \"cuando el token target aparece en el input, p_gen debe ser bajo (copiar)\". Sin esta señal:\n",
    "- El gradiente toma el camino de menor resistencia\n",
    "- Generar desde el vocabulario (p_gen→1.0) es matemáticamente más estable que aprender a focalizar atención en tokens específicos\n",
    "- p_gen converge a 0.995 en las primeras épocas y nunca baja\n",
    "\n",
    "**Causa raíz 3: La cross-attention no es suficientemente selectiva para copiar**\n",
    "\n",
    "La distribución de copia requiere que la cross-attention se concentre en UN token específico del input (el que se quiere copiar). Pero la cross-attention de V3 distribuye atención sobre contexto amplio (entropía 0.52-0.74, no 0.0-0.1 como se necesitaría). Esto hace que la distribución de copia sea \"difusa\" — apunta a todo y a nada.\n",
    "\n",
    "**Causa raíz 4: El entrenamiento en 3 fases daña las representaciones V4**\n",
    "\n",
    "En V3, la Phase 2 reinicializa cross-attention y el modelo se recupera. En V4, el mecanismo de copia depende de una cross-attention selectiva que se destruye al reinicializar. Phase 3 intenta reparar todo simultáneamente (encoder + decoder + copy), pero con lr_scale=0.1 y batch_size=16, no hay suficiente señal de gradiente para que p_gen baje y el copy mechanism se active.\n",
    "\n",
    "### Lecciones aprendidas — Cuándo usar y NO usar copy mechanism\n",
    "\n",
    "| Situación | ¿Usar copy? | ¿Por qué? |\n",
    "|-----------|-------------|-----------|\n",
    "| Summarization con nombres propios | ✅ SÍ | Nombres no están en vocabulario |\n",
    "| Traducción con términos técnicos | ✅ SÍ | Términos OOV necesitan copiarse |\n",
    "| Resolución matemática con BPE | ❌ NO | BPE+split_digits cubre todos los números |\n",
    "| Modelo con vocab < 1000 | ✅ QUIZÁS | Vocabulario pequeño puede tener OOV |\n",
    "| Modelo con vocab > 4000 + split_digits | ❌ NO | Todo cubierto por vocabulario |\n",
    "\n",
    "**Regla general**: El copy mechanism solo aporta valor cuando existen tokens Out-Of-Vocabulary (OOV) frecuentes en el input que deben aparecer en el output. Si el tokenizer ya puede representar todos los tokens relevantes, el copy mechanism es redundante y puede dañar el entrenamiento.\n",
    "\n",
    "## 9.2 Comparación completa entre versiones\n",
    "\n",
    "| Aspecto | v1 | v2 | v3 ⭐ | v4 |\n",
    "|---------|----|----|----|----|\n",
    "| **Tokenización** | Character (135) | BPE (4000) | BPE (4000) | BPE (4000) |\n",
    "| **Parámetros** | 7.4M | 10.5M | 10.5M | 10.5M |\n",
    "| **Dataset** | 12,568 | 6,881 | 6,881 | 6,881 |\n",
    "| **Token Accuracy** | 82.1% | ~70% | **73.8%** | 68.9% |\n",
    "| **Exact Match** | 0% | 0% | **3.0%** | 1.0% |\n",
    "| **Cross-Attention** | Colapsada (1.0) | Colapsada (1.0) | **Selectiva (0.52-0.74)** | — |\n",
    "| **Answer Head MAE** | — | — | 298.8 | **283.4** |\n",
    "| **Innovación** | Baseline | BPE | Three-phase training | Copy mechanism (fallido) |\n",
    "| **Estado** | Descartado | Descartado | **En producción (GitHub)** | Experimental (solo local) |\n",
    "\n",
    "**V3 es el mejor modelo y el que está en GitHub para evaluación.**\n",
    "\n",
    "## 9.3 Mejoras propuestas (ordenadas por impacto esperado)\n",
    "\n",
    "### Alta prioridad (mayor impacto, esfuerzo medio)\n",
    "\n",
    "1. **Data augmentation numérica**: Generar variaciones de cada problema cambiando los números. Ej: \"María tiene 3 manzanas y compra 5 más\" → generar 10 variantes con números aleatorios. Expandiría el dataset de ~6,800 a ~60,000+ problemas. El modelo vería más combinaciones numéricas.\n",
    "\n",
    "2. **Curriculum learning**: Entrenar primero con problemas de 1 paso (suma/resta simple), luego 2 pasos, luego multi-paso. Facilita el aprendizaje gradual de razonamiento aritmético.\n",
    "\n",
    "### Media prioridad (impacto moderado, esfuerzo alto)\n",
    "\n",
    "3. **Calculator augmentation**: Agregar un módulo externo que detecte expresiones aritméticas en la salida y las calcule simbólicamente. Separa \"qué operación hacer\" (Transformer) de \"calcular el resultado\" (calculadora).\n",
    "\n",
    "4. **Pre-entrenamiento en corpus general**: Entrenar primero en texto general para desarrollar comprensión lingüística, luego fine-tuning en matemáticas/física.\n",
    "\n",
    "### Baja prioridad (investigación a largo plazo)\n",
    "\n",
    "5. **Escalado del modelo**: Aumentar a 50-100M parámetros con significativamente más datos.\n",
    "\n",
    "6. ~~Mecanismo de copia~~: **Implementado y descartado** (V4). No funciona con BPE+split_digits. NO intentar de nuevo a menos que se cambie a character-level tokenization o vocabulario muy pequeño.\n",
    "\n",
    "## 9.4 Valor de los resultados negativos\n",
    "\n",
    "El progreso de v1 a v4 demuestra un proceso científico riguroso:\n",
    "- **v1 → v2**: Identificar que la tokenización character-level era un cuello de botella → migrar a BPE\n",
    "- **v2 → v3**: Diagnosticar el colapso de cross-attention → diseñar entrenamiento en tres fases con reinicialización\n",
    "- **v3 → v4**: Hipótesis sobre mecanismo de copia → implementación completa → resultado negativo documentado → análisis de causas raíz → decisión informada de descartar\n",
    "\n",
    "Este proceso iterativo de diagnóstico, experimentación y **documentación honesta de resultados negativos** es representativo del trabajo real en investigación de deep learning. Un resultado negativo bien documentado es tan valioso como uno positivo: evita que otros (o yo misma en el futuro) repitan el mismo experimento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 10. Aprendizajes y Contribuciones Personales\n",
    "\n",
    "## 10.1 Proceso iterativo — Cuatro versiones del modelo\n",
    "\n",
    "### Versión 1: Character-level tokenization\n",
    "- Tokenización carácter a carácter (135 tokens).\n",
    "- Modelo: 4 capas, d_model=256, 8 heads (~7.4M params).\n",
    "- Dataset: 12,568 problemas (GSM8K + MATH + física).\n",
    "- Resultado: 82.1% token accuracy, 0% exact match, cross-attention colapsada.\n",
    "- **Lección**: La tokenización character-level es un cuello de botella fundamental — el modelo memoriza secuencias de caracteres sin entender la estructura semántica.\n",
    "\n",
    "### Versión 2: BPE tokenization\n",
    "- Migración a SentencePiece BPE (4,000 tokens, `split_digits=True`).\n",
    "- Modelo: ~10.5M params.\n",
    "- Dataset reducido a 6,881 problemas (easy subset).\n",
    "- Resultado: ~70% token accuracy, 0% exact match, cross-attention aún colapsada.\n",
    "- **Lección**: BPE mejora la representación pero no resuelve el colapso de cross-attention. El problema es que el decoder aprende a auto-completar sin leer el encoder.\n",
    "\n",
    "### Versión 3: Three-phase training + Answer Head ⭐ (Mejor modelo)\n",
    "- Entrenamiento en tres fases con reinicialización de cross-attention.\n",
    "- Answer Head para forzar al encoder a codificar información numérica.\n",
    "- Diversity loss para penalizar cross-attention uniforme.\n",
    "- Decoder masking agresivo (35%).\n",
    "- Resultado: 73.8% token accuracy, **3.0% exact match**, **cross-attention selectiva (0.52-0.74)**.\n",
    "- **Lección**: El diagnóstico profundo y la ingeniería de entrenamiento pueden resolver problemas que parecen arquitectónicos. A veces el problema no es la arquitectura sino cómo se entrena.\n",
    "\n",
    "### Versión 4: Pointer-Generator Network (Copy Mechanism) — Fallido\n",
    "- Implementación de mecanismo de copia basado en See et al. 2017.\n",
    "- Gate p_gen para decidir entre generar del vocabulario o copiar del input.\n",
    "- Entrenamiento en tres fases con transfer learning desde V3.\n",
    "- Resultado: 68.9% token accuracy, **1.0% exact match**, p_gen≈0.995 (el modelo NO copia).\n",
    "- **Lección**: No toda mejora teórica funciona en la práctica. El mecanismo de copia requiere tokens OOV para activarse; con BPE+split_digits, no existen OOV. Más importante: sin supervisión explícita para el gate p_gen, converge al camino de menor resistencia (generar siempre).\n",
    "\n",
    "## 10.2 Qué aprendí — Guía práctica\n",
    "\n",
    "### Sobre Transformers\n",
    "- La implementación from-scratch de Multi-Head Attention requiere manejo cuidadoso de shapes (`[batch, heads, seq_len, depth]`).\n",
    "- Las máscaras (padding, look-ahead, combined) son el aspecto más delicado del Transformer — un error aquí corrompe silenciosamente todo el entrenamiento.\n",
    "- El **colapso de cross-attention** es un problema real en modelos encoder-decoder pequeños: el decoder puede aprender a ignorar el encoder si el contexto autoregresivo es suficiente.\n",
    "- La reinicialización selectiva de pesos es una técnica poderosa para romper simetrías colapsadas.\n",
    "\n",
    "### Sobre mecanismos de copia — Cuándo SÍ y cuándo NO\n",
    "- ✅ **SÍ funciona**: Tasks de summarization con nombres propios OOV, traducción con términos técnicos no cubiertos por vocabulario.\n",
    "- ❌ **NO funciona**: Tasks con tokenización BPE+split_digits que ya cubre todos los tokens relevantes (como nuestro caso).\n",
    "- ❌ **NO funciona**: Sin supervisión explícita del gate p_gen — el gradiente no tiene incentivo para activar la copia.\n",
    "- **Clave**: Antes de implementar un copy mechanism, verificar que realmente existen tokens OOV que necesitan copiarse. Si el tokenizer ya puede representarlos, el copy es redundante.\n",
    "\n",
    "### Sobre datos\n",
    "- La calidad del dataset importa más que la cantidad — 6,881 problemas fáciles dan mejores resultados que 12,568 mixtos.\n",
    "- La generación paramétrica de problemas de física es una técnica efectiva para crear datasets balanceados.\n",
    "- La tokenización BPE con `split_digits=True` es **crítica** para tareas numéricas — permite representar cualquier número como secuencia de dígitos.\n",
    "\n",
    "### Sobre entrenamiento\n",
    "- El entrenamiento en fases (congelar/descongelar capas) permite diagnosticar y resolver problemas específicos — es una herramienta de debugging, no solo de optimización.\n",
    "- La diversity loss es efectiva para forzar cross-attention selectiva.\n",
    "- El decoder masking al 35% fuerza al modelo a depender del encoder.\n",
    "- `@tf.function` con `input_signature` fijo es esencial para evitar retracing en GPUs Blackwell (sm_120).\n",
    "- **OOM en Phase 3**: Al desbloquear todos los parámetros, Adam requiere 3× la memoria del modelo (weights + m + v). Solución: reducir batch_size.\n",
    "\n",
    "### Sobre GPU y sistemas\n",
    "- Las GPUs de nueva generación (Blackwell/sm_120) requieren workarounds específicos para XLA, tf.cast y dropout custom.\n",
    "- `TF_XLA_FLAGS=\"--tf_xla_auto_jit=2\"` es necesario para compilación XLA en Blackwell.\n",
    "- El OOM killer de Linux (`SIGKILL`, exit code 137) mata procesos sin aviso — monitorear con `dmesg` para diagnosticar.\n",
    "\n",
    "### Sobre metodología de investigación\n",
    "- **Formular hipótesis antes de implementar**: \"El modelo no copia números → agregar copy mechanism\" era una hipótesis razonable pero incorrecta.\n",
    "- **Verificar supuestos**: Debí verificar PRIMERO si realmente había tokens OOV que necesitaran copiarse (no los había con BPE+split_digits).\n",
    "- **Documentar resultados negativos**: Un experimento fallido bien documentado evita repetir errores y demuestra madurez científica.\n",
    "- **Saber cuándo parar**: V4 empeoró respecto a V3 → decisión correcta de revertir a V3 en lugar de seguir iterando sobre una dirección equivocada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 11. Conclusiones y Estado Actual del Proyecto\n",
    "\n",
    "## 11.1 Logros principales\n",
    "\n",
    "Se implementó exitosamente un **Transformer Encoder-Decoder desde cero** en TensorFlow para resolver problemas de matemáticas y física con soluciones paso a paso, con cuatro iteraciones del modelo (v1→v4).\n",
    "\n",
    "1. **Arquitectura completa from-scratch**: Scaled dot-product attention, multi-head attention, positional encoding sinusoidal, encoder-decoder con máscaras, answer head de regresión — todo implementado sin usar capas Transformer pre-hechas.\n",
    "\n",
    "2. **Pipeline de datos robusto**: Sistema modular para descargar, convertir, validar y combinar datasets de múltiples fuentes con tokenización BPE (SentencePiece).\n",
    "\n",
    "3. **Entrenamiento innovador en tres fases**: Estrategia de pre-entrenamiento de encoder → reinicialización de cross-attention → fine-tuning para resolver el colapso de cross-attention, con diversity loss y decoder masking agresivo.\n",
    "\n",
    "4. **Cross-attention selectiva**: Logro técnico principal — la entropía normalizada pasó de 1.0 (colapsada) a 0.52–0.74 (selectiva), demostrando que el decoder atiende al problema de entrada.\n",
    "\n",
    "5. **Primeros exact matches**: El modelo V3 logra 3.0% de exact match (vs 0% en v1/v2), confirmando que la arquitectura funciona.\n",
    "\n",
    "6. **Experimentación con copy mechanism (V4)**: Se implementó un Pointer-Generator Network. El resultado fue negativo (p_gen→1.0, el modelo no copia), pero la documentación rigurosa del fallo y sus causas demuestra madurez científica.\n",
    "\n",
    "7. **Evaluación rigurosa y honesta**: Se reportan métricas favorables (73.8% token accuracy, 62.2% answer head exact) y desfavorables (3% exact match en generación, copy mechanism fallido), con análisis detallado.\n",
    "\n",
    "8. **Despliegue funcional**: Interfaz Gradio interactiva con métricas de confianza en tiempo real.\n",
    "\n",
    "## 11.2 Estado actual del repositorio\n",
    "\n",
    "### En GitHub (versión para evaluación del profesor)\n",
    "\n",
    "El repositorio contiene el **modelo V3** — la mejor versión obtenida:\n",
    "\n",
    "| Componente | Descripción | Ubicación |\n",
    "|---|---|---|\n",
    "| Modelo V3 | TransformerV3 con Answer Head, 10.5M params | `models/transformer_v3.py` |\n",
    "| Pesos entrenados | 3 fases: 30+100+50 épocas | `checkpoints/v3_easy/model_weights.weights.h5` |\n",
    "| Tokenizer BPE | SentencePiece, 4000 tokens | `checkpoints/v2_subword/sp_tokenizer.model` |\n",
    "| Evaluación | Token acc + exact match + attention | `evaluation/evaluate.py` |\n",
    "| Demo Gradio | Interfaz interactiva | `notebooks/03_demo_profesor.ipynb` |\n",
    "| Entrenamiento | Script completo 3 fases | `run_training.py` |\n",
    "\n",
    "### En local (NO en GitHub) — Archivos experimentales V4\n",
    "\n",
    "Estos archivos se mantienen localmente como referencia del experimento V4:\n",
    "\n",
    "| Archivo | Descripción |\n",
    "|---|---|\n",
    "| `models/transformer_v4.py` | Arquitectura con gate p_gen + copy distribution |\n",
    "| `training/trainer_v4.py` | Trainer con copy_loss y monitoreo de p_gen |\n",
    "| `inference/generate_v4.py` | Generación autoregresiva con probabilidades blended |\n",
    "| `run_training_v4.py` | Entrenamiento 3 fases V4 (batch_size=32) |\n",
    "| `resume_phase3.py` | Reanudación Phase 3 tras OOM (batch_size=16) |\n",
    "| `evaluation/evaluate_v4.py` | Evaluación V4 con análisis p_gen |\n",
    "| `checkpoints/v4_copy/` | Pesos, configs, historial de entrenamiento V4 |\n",
    "\n",
    "## 11.3 Conclusión principal\n",
    "\n",
    "El valor del proyecto reside en:\n",
    "- **(a)** La **implementación completa from-scratch** de un pipeline de deep learning end-to-end\n",
    "- **(b)** El **proceso científico iterativo** de diagnóstico y solución (v1→v2→v3→v4)\n",
    "- **(c)** La resolución del **colapso de cross-attention** mediante ingeniería de entrenamiento\n",
    "- **(d)** La **documentación honesta de resultados negativos** (V4 copy mechanism)\n",
    "\n",
    "El proyecto demuestra que un Transformer de ~10.5M parámetros con tokenización BPE y entrenamiento en tres fases puede:\n",
    "- Aprender el **formato y estilo** de soluciones matemáticas y físicas (73.8% token accuracy)\n",
    "- Lograr **cross-attention selectiva** (un requisito fundamental para que el decoder \"lea\" el problema)\n",
    "- Producir algunos **exact matches** (3%) — limitados por la capacidad de razonamiento aritmético del modelo\n",
    "\n",
    "La limitación fundamental no es la ausencia de mecanismo de copia (como se demostró con V4), sino la **capacidad inherente de un Transformer estándar de 10.5M parámetros para realizar razonamiento aritmético exacto**.\n",
    "\n",
    "## 11.4 Roadmap — Cómo continuar mejorando el proyecto\n",
    "\n",
    "Si se desea retomar el proyecto en el futuro, estas son las acciones recomendadas en orden de prioridad:\n",
    "\n",
    "### Paso 1: Data Augmentation numérica (1-2 días de trabajo)\n",
    "```\n",
    "Objetivo: Expandir dataset de 6,881 → 60,000+ problemas\n",
    "Cómo: Para cada problema, generar N variantes cambiando los valores numéricos\n",
    "Ejemplo: \"María tiene 3 manzanas\" → \"María tiene 7 manzanas\", \"María tiene 12 manzanas\"...\n",
    "Script: Crear data/augment_numeric.py\n",
    "Entrenar: Mismo run_training.py con más datos\n",
    "Impacto esperado: Mejorar exact match de 3% a 8-15%\n",
    "```\n",
    "\n",
    "### Paso 2: Curriculum Learning (1 día adicional)\n",
    "```\n",
    "Objetivo: Entrenar primero con problemas fáciles, luego difíciles\n",
    "Cómo: Ordenar problemas por número de pasos (1 paso → 2 pasos → multi-paso)\n",
    "Modificar: training/trainer.py para cambiar el dataset entre fases\n",
    "Impacto esperado: Aprendizaje más estable, mejores representaciones internas\n",
    "```\n",
    "\n",
    "### Paso 3: Calculator Augmentation (2-3 días)\n",
    "```\n",
    "Objetivo: Delegar cálculos aritméticos a un módulo simbólico\n",
    "Cómo: Post-procesar la salida del modelo, detectar expresiones como \"15 × 6 =\"\n",
    "       y reemplazar con el resultado correcto usando sympy o eval seguro\n",
    "Script: Crear inference/calculator.py\n",
    "Impacto esperado: Exact match podría subir a 20-40% si el razonamiento es correcto\n",
    "```\n",
    "\n",
    "### Qué NO intentar de nuevo\n",
    "- ❌ **Copy mechanism (Pointer-Generator)**: Ya probado, no funciona con BPE+split_digits\n",
    "- ❌ **Aumentar épocas sin más datos**: El modelo ya converge; más épocas = overfitting\n",
    "- ❌ **Reducir dropout**: El modelo ya tiene 0.2; reducirlo aumenta overfitting sin mejorar exact match\n",
    "\n",
    "### Para retomar el proyecto\n",
    "1. Activar el entorno: `source .venv/bin/activate`\n",
    "2. Verificar GPU: `python -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\"`\n",
    "3. Ejecutar evaluación V3: `python evaluation/evaluate.py`\n",
    "4. Ejecutar demo: Abrir `notebooks/03_demo_profesor.ipynb` y ejecutar todas las celdas\n",
    "5. Ver archivos V4 (referencia): `ls models/transformer_v4.py training/trainer_v4.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 12. Referencias\n",
    "\n",
    "1. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2017). **Attention Is All You Need.** *Advances in Neural Information Processing Systems, 30* (NeurIPS 2017). https://arxiv.org/abs/1706.03762\n",
    "\n",
    "2. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., & Schulman, J. (2021). **Training Verifiers to Solve Math Word Problems.** https://arxiv.org/abs/2110.14168\n",
    "\n",
    "3. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., & Steinhardt, J. (2021). **Measuring Mathematical Problem Solving With the MATH Dataset.** *Advances in Neural Information Processing Systems, 34* (NeurIPS 2021). https://arxiv.org/abs/2103.03874\n",
    "\n",
    "4. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). **Language Models are Unsupervised Multitask Learners.** OpenAI.\n",
    "\n",
    "5. Brown, T. B., et al. (2020). **Language Models are Few-Shot Learners.** *Advances in Neural Information Processing Systems, 33* (NeurIPS 2020). https://arxiv.org/abs/2005.14165\n",
    "\n",
    "6. Wei, J., et al. (2022). **Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.** *Advances in Neural Information Processing Systems, 35* (NeurIPS 2022). https://arxiv.org/abs/2201.11903\n",
    "\n",
    "7. Ba, J. L., Kiros, J. R., & Hinton, G. E. (2016). **Layer Normalization.** https://arxiv.org/abs/1607.06450\n",
    "\n",
    "8. He, K., Zhang, X., Ren, S., & Sun, J. (2016). **Deep Residual Learning for Image Recognition.** *CVPR 2016*. https://arxiv.org/abs/1512.03385\n",
    "\n",
    "9. Sennrich, R., Haddow, B., & Birch, A. (2016). **Neural Machine Translation of Rare Words with Subword Units.** *ACL 2016*. https://arxiv.org/abs/1508.07909\n",
    "\n",
    "10. Kingma, D. P., & Ba, J. (2015). **Adam: A Method for Stochastic Optimization.** *ICLR 2015*. https://arxiv.org/abs/1412.6980\n",
    "\n",
    "11. See, A., Liu, P. J., & Manning, C. D. (2017). **Get To The Point: Summarization with Pointer-Generator Networks.** *ACL 2017*. https://arxiv.org/abs/1704.04368\n",
    "\n",
    "12. Vinyals, O., Fortunato, M., & Jaitly, N. (2015). **Pointer Networks.** *NeurIPS 2015*. https://arxiv.org/abs/1506.03134"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 0,
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
