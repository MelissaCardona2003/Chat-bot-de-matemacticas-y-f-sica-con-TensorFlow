# ğŸ“ Chatbot Tutor de MatemÃ¡ticas y FÃ­sica con Transformer

**Modelo Transformer Encoderâ€“Decoder implementado desde cero en TensorFlow**, entrenado para resolver problemas de **matemÃ¡ticas** (aritmÃ©tica, Ã¡lgebra) y **fÃ­sica** (cinemÃ¡tica, dinÃ¡mica, termodinÃ¡mica, circuitos) con soluciones paso a paso.

> Proyecto final del curso de profundizaciÃ³n en Deep Learning â€” Carrera de FÃ­sica  
> Melissa Cardona, 2026

---

## ğŸ“‹ DescripciÃ³n

Este proyecto implementa un **Transformer completo (Encoderâ€“Decoder)** desde cero, sin utilizar modelos pre-entrenados como GPT, BERT ni APIs externas. Todo el cÃ³digo de atenciÃ³n, positional encoding, capas encoder/decoder, y el loop de entrenamiento estÃ¡ escrito a mano en TensorFlow/Keras.

El modelo recibe un problema de matemÃ¡ticas o fÃ­sica como entrada y genera una soluciÃ³n paso a paso con el formato:

```
Step 1: Identify the variables...
Step 2: Apply the formula...
Answer: 42
```

### CaracterÃ­sticas principales

- **Arquitectura from-scratch**: Multi-Head Attention, Positional Encoding sinusoidal, Encoder-Decoder con residual connections
- **TransformerV3** con **Answer Head** (cabeza de regresiÃ³n numÃ©rica auxiliar)
- **10.5M parÃ¡metros** â€” modelo compacto para fines pedagÃ³gicos
- **TokenizaciÃ³n BPE** (SentencePiece, vocabulario de 4,000 tokens)
- **Entrenamiento en tres fases**: pre-entrenamiento de encoder â†’ entrenamiento de decoder con cross-attention reinicializada â†’ fine-tuning completo
- **Pipeline completo**: datos â†’ tokenizaciÃ³n â†’ entrenamiento â†’ evaluaciÃ³n â†’ interfaz Gradio
- **Interfaz interactiva** con Gradio Blocks para demostraciÃ³n

---

## ğŸ—ï¸ Arquitectura del Modelo

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  TRANSFORMER V3 (10.5M params)                   â”‚
â”‚                                                                  â”‚
â”‚  ENCODER (Ã—4 capas)              DECODER (Ã—4 capas)              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ Input Embedding   â”‚            â”‚ Output Embedding      â”‚        â”‚
â”‚  â”‚ + Pos. Encoding   â”‚            â”‚ + Pos. Encoding       â”‚        â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤        â”‚
â”‚  â”‚ Self-Attention    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ Masked Self-Attention â”‚        â”‚
â”‚  â”‚ (8 heads, d=256)  â”‚           â”‚ Cross-Attention â—€â”€â”€â”€â”€â”€â”¤        â”‚
â”‚  â”‚ Add & LayerNorm   â”‚           â”‚ Add & LayerNorm       â”‚        â”‚
â”‚  â”‚ FFN (1024)        â”‚           â”‚ FFN (1024)            â”‚        â”‚
â”‚  â”‚ Add & LayerNorm   â”‚           â”‚ Add & LayerNorm       â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚         â”‚                                 â”‚                      â”‚
â”‚    Answer Head                      Linear + Softmax             â”‚
â”‚    (MLP â†’ scalar)                  (vocab_size=4000)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### HiperparÃ¡metros

| ParÃ¡metro | Valor |
|-----------|-------|
| `d_model` | 256 |
| `num_heads` | 8 |
| `num_layers` | 4 (encoder) + 4 (decoder) |
| `dff` (feed-forward) | 1024 |
| `dropout_rate` | 0.2 |
| `vocab_size` | 4,000 (BPE / SentencePiece) |
| `max_encoder_len` | 128 tokens |
| `max_decoder_len` | 256 tokens |
| **Total parÃ¡metros** | **~10,514,849** |

### Entrenamiento en Tres Fases

El entrenamiento utiliza una estrategia de tres fases para resolver el problema de **colapso de cross-attention** (entropÃ­a=1.0):

| Fase | Ã‰pocas | DescripciÃ³n | Componentes entrenados |
|------|--------|-------------|----------------------|
| **Fase 1** | 30 | Pre-entrenamiento del encoder | Encoder + Answer Head (decoder congelado) |
| **Fase 2** | 100 | Entrenamiento del decoder | Decoder + Final Layer (encoder congelado, cross-attention reinicializada) |
| **Fase 3** | 50 | Fine-tuning completo | Todos los parÃ¡metros (lr_scale=0.1) |

**TÃ©cnicas utilizadas:**
- Optimizador Adam (Î²â‚=0.9, Î²â‚‚=0.98, Îµ=1e-9)
- Learning Rate: Warmup (1000 pasos) + inverse sqrt decay
- Loss combinada: seq2seq + answer regression (Huber) + diversity loss
- Decoder token masking (35%) para forzar uso de cross-attention
- Gradient clipping (global norm = 1.0)
- Label smoothing (0.1)
- GPU: NVIDIA RTX 5060 (Blackwell)

---

## ğŸ“Š Dataset

Subconjunto curado de **6,881 problemas** con soluciones paso a paso:

| Dominio | Train | Val | Test | Total |
|---------|-------|-----|------|-------|
| Math | ~4,800 | ~420 | ~550 | ~5,770 |
| Physics | ~930 | ~89 | ~93 | ~1,111 |
| **Total** | **5,729** | **509** | **643** | **6,881** |

Derivado de GSM8K, MATH (con soluciones LLM) y problemas de fÃ­sica generados paramÃ©tricamente.

---

## ğŸ“ˆ Resultados

| MÃ©trica | Valor |
|---------|-------|
| Token Accuracy (val) | **73.8%** |
| Token Accuracy (test) | **69.9%** |
| Train Accuracy (fase 3) | 64.9% |
| Val Loss | 2.383 |
| Exact Match (Answer:) | **3.0%** (3/100) |
| Exact Match numÃ©rico (Â±0.5) | **3.5%** (3/86) |
| Answer Head MAE | 298.8 |
| Answer Head Exact (Â±0.5) | 62.2% |

### Cross-Attention â€” Logro principal

| Capa | EntropÃ­a normalizada | Estado |
|------|---------------------|--------|
| Decoder Layer 1 | 0.742 | SELECTIVA |
| Decoder Layer 2 | 0.540 | SELECTIVA |
| Decoder Layer 3 | 0.523 | SELECTIVA |
| Decoder Layer 4 | 0.673 | SELECTIVA |

> **Logro clave**: La cross-attention pasÃ³ de colapsada (entropÃ­a â‰ˆ 1.0 en v1/v2) a **selectiva** (0.52â€“0.74), demostrando que el decoder atiende selectivamente al problema de entrada.

### EvoluciÃ³n del proyecto

| VersiÃ³n | TokenizaciÃ³n | Params | Token Acc | Exact Match | Cross-Attention |
|---------|-------------|--------|-----------|-------------|-----------------|
| v1 | Character (135) | 7.4M | 82.1% | 0% | Colapsada (1.0) |
| v2 | BPE (4000) | 10.5M | ~70% | 0% | Colapsada (1.0) |
| **v3** | **BPE (4000)** | **10.5M** | **73.8%** | **3.0%** | **Selectiva (0.52-0.74)** |

---

## ğŸ“ Estructura del Repositorio

```
transformer_math_physics_tutor/
â”œâ”€â”€ models/                          # Arquitectura Transformer from-scratch
â”‚   â”œâ”€â”€ transformer.py               #   Base Encoder-Decoder (clase padre)
â”‚   â”œâ”€â”€ transformer_v3.py            #   TransformerV3 con Answer Head
â”‚   â”œâ”€â”€ multihead_attention.py       #   Scaled Dot-Product + Multi-Head Attention
â”‚   â”œâ”€â”€ encoder_layer.py             #   Capa encoder (Self-Attn + FFN)
â”‚   â”œâ”€â”€ decoder_layer.py             #   Capa decoder (Masked Self-Attn + Cross-Attn + FFN)
â”‚   â”œâ”€â”€ positional_encoding.py       #   Positional encoding sinusoidal
â”‚   â”œâ”€â”€ xla_dropout.py               #   Dropout compatible con XLA/Blackwell
â”‚   â””â”€â”€ config.py                    #   ConfiguraciÃ³n del modelo (dataclass)
â”‚
â”œâ”€â”€ data/                            # Pipeline de datos
â”‚   â”œâ”€â”€ combined_easy.json           #   Dataset curado (6,881 problemas)
â”‚   â”œâ”€â”€ subword_tokenizer.py         #   Tokenizador BPE (SentencePiece, 4000 tokens)
â”‚   â””â”€â”€ dataset_builder.py           #   Constructor de tf.data.Dataset
â”‚
â”œâ”€â”€ training/                        # Loop de entrenamiento
â”‚   â”œâ”€â”€ trainer.py                   #   TransformerTrainerV3 (GradientTape, 3-phase)
â”‚   â”œâ”€â”€ losses.py                    #   Loss combinada + diversity loss
â”‚   â”œâ”€â”€ metrics.py                   #   Exact match + validaciÃ³n simbÃ³lica
â”‚   â””â”€â”€ scheduler.py                 #   Learning rate: warmup + inverse sqrt + scale
â”‚
â”œâ”€â”€ inference/                       # GeneraciÃ³n de respuestas
â”‚   â””â”€â”€ generate.py                  #   GeneraciÃ³n autoregresiva (greedy, top-k, beam search)
â”‚
â”œâ”€â”€ evaluation/                      # EvaluaciÃ³n del modelo
â”‚   â””â”€â”€ evaluate.py                  #   Token accuracy + exact match + cross-attention entropy
â”‚
â”œâ”€â”€ notebooks/                       # Notebooks de demostraciÃ³n
â”‚   â”œâ”€â”€ 01_exploracion_datos.ipynb   #   ExploraciÃ³n y anÃ¡lisis del dataset
â”‚   â”œâ”€â”€ 02_entrenamiento.ipynb       #   Notebook de entrenamiento
â”‚   â””â”€â”€ 03_demo_profesor.ipynb       #   â­ DEMO: Chatbot con interfaz Gradio
â”‚
â”œâ”€â”€ informe final/                   # Informe acadÃ©mico del proyecto
â”‚   â””â”€â”€ Informe_MelissaCardona_ChatbotMathPhysics.ipynb
â”‚
â”œâ”€â”€ checkpoints/v3_easy/             # Modelo entrenado (listo para usar)
â”‚   â”œâ”€â”€ model_weights.weights.h5     #   Pesos del modelo
â”‚   â”œâ”€â”€ config.json                  #   ConfiguraciÃ³n
â”‚   â”œâ”€â”€ sp_tokenizer.model           #   Modelo SentencePiece (BPE)
â”‚   â”œâ”€â”€ training_history.json        #   Historia (3 fases)
â”‚   â””â”€â”€ evaluation_report.json       #   MÃ©tricas de evaluaciÃ³n
â”‚
â”œâ”€â”€ run_training.py                  # Script de entrenamiento (3 fases)
â”œâ”€â”€ test_all.py                      # Suite de tests
â””â”€â”€ requirements.txt                 # Dependencias Python
```

---

## ğŸš€ GuÃ­a RÃ¡pida â€” Para el Profesor

### âš ï¸ El modelo YA viene entrenado. No necesita reentrenar nada.

#### 1. Descargar el proyecto

```bash
git clone https://github.com/MelissaCardona2003/Chat-bot-de-matemacticas-y-f-sica-con-TensorFlow.git
cd Chat-bot-de-matemacticas-y-f-sica-con-TensorFlow
```

#### 2. Crear entorno e instalar dependencias

```bash
python -m venv .venv
source .venv/bin/activate   # Linux/Mac
pip install -r requirements.txt
```

> Funciona en **CPU**. No necesita GPU.

#### 3. Ejecutar la demo

```bash
jupyter notebook notebooks/03_demo_profesor.ipynb
```

Ejecutar todas las celdas (Shift+Enter). La interfaz Gradio se abrirÃ¡ automÃ¡ticamente.

---

## âš ï¸ Limitaciones y Trabajo Futuro

1. **3% Exact Match**: Formato correcto pero valores numÃ©ricos generalmente incorrectos
2. **Sin mecanismo de copia**: El Transformer estÃ¡ndar no puede copiar tokens directamente del input
3. **Dataset limitado**: 6,881 problemas es pequeÃ±o para razonamiento matemÃ¡tico

### Â¿QuÃ© SÃ demuestra?

- âœ… Transformer Encoder-Decoder completo from-scratch
- âœ… Pipeline de datos robusto con tokenizaciÃ³n BPE
- âœ… Entrenamiento avanzado en tres fases con reinicializaciÃ³n de cross-attention
- âœ… **Cross-attention selectiva** â€” logro tÃ©cnico significativo
- âœ… EvaluaciÃ³n rigurosa y honesta
- âœ… Interfaz interactiva Gradio

---

## ğŸ“š Referencias

- Vaswani et al., *"Attention Is All You Need"*, NeurIPS 2017
- Cobbe et al., *"Training Verifiers to Solve Math Word Problems"* (GSM8K), 2021
- Hendrycks et al., *"Measuring Mathematical Problem Solving With the MATH Dataset"*, NeurIPS 2021
- Radford et al., *"Language Models are Unsupervised Multitask Learners"* (GPT-2), 2019

---

## ğŸ› ï¸ TecnologÃ­as

- **TensorFlow 2.x** â€” Deep learning
- **SentencePiece** â€” TokenizaciÃ³n BPE
- **Gradio** â€” Interfaz web
- **NumPy / Matplotlib** â€” CÃ¡lculo y visualizaciÃ³n
- **SymPy** â€” ValidaciÃ³n simbÃ³lica

---

*Melissa Cardona â€” 2026*
