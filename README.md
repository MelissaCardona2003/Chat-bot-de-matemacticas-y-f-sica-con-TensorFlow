# üéì Chatbot Tutor de Matem√°ticas y F√≠sica con Transformer

**Modelo Transformer Encoder‚ÄìDecoder implementado desde cero en TensorFlow**, entrenado para resolver problemas de **matem√°ticas** (aritm√©tica, √°lgebra) y **f√≠sica** (cinem√°tica, din√°mica, termodin√°mica, circuitos) con soluciones paso a paso.

> Proyecto final del curso de profundizaci√≥n en Deep Learning ‚Äî Carrera de F√≠sica  
> Melissa Cardona, 2026

---

## üìã Descripci√≥n

Este proyecto implementa un **Transformer completo (Encoder‚ÄìDecoder)** desde cero, sin utilizar modelos pre-entrenados como GPT, BERT ni APIs externas. Todo el c√≥digo de atenci√≥n, positional encoding, capas encoder/decoder, y el loop de entrenamiento est√° escrito a mano en TensorFlow/Keras.

El modelo recibe un problema de matem√°ticas o f√≠sica como entrada y genera una soluci√≥n paso a paso con el formato:

```
Step 1: Identify the variables...
Step 2: Apply the formula...
Answer: 42
```

### Caracter√≠sticas principales

- **Arquitectura from-scratch**: Multi-Head Attention, Positional Encoding sinusoidal, Encoder-Decoder con residual connections
- **7.4M par√°metros** ‚Äî modelo compacto para fines pedag√≥gicos
- **Tokenizaci√≥n a nivel de car√°cter** (135 tokens: 131 ASCII + 4 especiales)
- **Pipeline completo**: datos ‚Üí tokenizaci√≥n ‚Üí entrenamiento ‚Üí evaluaci√≥n ‚Üí interfaz Gradio
- **Interfaz interactiva** con Gradio Blocks para demostraci√≥n

---

## üèóÔ∏è Arquitectura del Modelo

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    TRANSFORMER (7.4M params)                ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  ENCODER (√ó4 capas)              DECODER (√ó4 capas)         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ Input Embedding   ‚îÇ            ‚îÇ Output Embedding      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ + Pos. Encoding   ‚îÇ            ‚îÇ + Pos. Encoding       ‚îÇ   ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§            ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§   ‚îÇ
‚îÇ  ‚îÇ Self-Attention    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Masked Self-Attention ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ (8 heads, d=256)  ‚îÇ           ‚îÇ Cross-Attention ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§   ‚îÇ
‚îÇ  ‚îÇ Add & LayerNorm   ‚îÇ           ‚îÇ Add & LayerNorm       ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ FFN (1024)        ‚îÇ           ‚îÇ FFN (1024)            ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ Add & LayerNorm   ‚îÇ           ‚îÇ Add & LayerNorm       ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                           ‚îÇ                 ‚îÇ
‚îÇ                                    Linear + Softmax         ‚îÇ
‚îÇ                                    (vocab_size=135)         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Hiperpar√°metros

| Par√°metro | Valor |
|-----------|-------|
| `d_model` | 256 |
| `num_heads` | 8 |
| `num_layers` | 4 (encoder) + 4 (decoder) |
| `dff` (feed-forward) | 1024 |
| `dropout_rate` | 0.2 |
| `vocab_size` | 135 (character-level) |
| `max_encoder_len` | 200 tokens |
| `max_decoder_len` | 300 tokens |
| **Total par√°metros** | **7,476,615** |

### Entrenamiento

| Aspecto | Valor |
|---------|-------|
| Optimizador | Adam (Œ≤‚ÇÅ=0.9, Œ≤‚ÇÇ=0.98, Œµ=1e-9) |
| Learning Rate | Warmup (2000 pasos) + inverse sqrt decay |
| Loss | SparseCategoricalCrossentropy + label smoothing (0.1) |
| Batch size | 32 |
| √âpocas | 89 (early stopping, patience=10) |
| Regularizaci√≥n | Dropout 0.2, decoder token masking (20%) |
| GPU | NVIDIA RTX 5060 (Blackwell) |

---

## üìä Datasets

El dataset combinado contiene **12,568 problemas** con soluciones paso a paso:

| Fuente | Dominio | Problemas | Descripci√≥n |
|--------|---------|-----------|-------------|
| [GSM8K](https://github.com/openai/grade-school-math) | Math | 8,638 | Aritm√©tica de nivel escolar con razonamiento |
| MATH (LLM-solved) | Math | 1,895 | √Ålgebra, combinatoria, geometr√≠a ‚Äî soluciones generadas con LLM |
| Physics Templates | Physics | 2,035 | Cinem√°tica, din√°mica, termodin√°mica, circuitos ‚Äî problemas param√©tricos |

**Splits**: Train 10,237 / Val 939 / Test 1,392

---

## üìà Resultados

| M√©trica | Valor |
|---------|-------|
| Token Accuracy (val) | **82.1%** |
| Token Accuracy (test) | **81.2%** |
| Train Accuracy | 73.4% |
| Val Loss | 1.37 |
| Exact Match (Answer:) | 0% (0/100) |

> **Nota importante**: El modelo alcanza ~82% de accuracy a nivel de token (predice bien el siguiente car√°cter), pero no logra respuestas num√©ricas correctas. Esto es una limitaci√≥n inherente de la tokenizaci√≥n a nivel de car√°cter con un modelo de 7.4M par√°metros. Ver la secci√≥n de Limitaciones en el notebook de demo y en el informe final.

---

## üìÅ Estructura del Repositorio

```
transformer_math_physics_tutor/
‚îú‚îÄ‚îÄ models/                          # Arquitectura Transformer from-scratch
‚îÇ   ‚îú‚îÄ‚îÄ transformer.py               #   Modelo completo Encoder-Decoder
‚îÇ   ‚îú‚îÄ‚îÄ multihead_attention.py        #   Scaled Dot-Product + Multi-Head Attention
‚îÇ   ‚îú‚îÄ‚îÄ encoder_layer.py             #   Capa encoder (Self-Attn + FFN)
‚îÇ   ‚îú‚îÄ‚îÄ decoder_layer.py             #   Capa decoder (Masked Self-Attn + Cross-Attn + FFN)
‚îÇ   ‚îú‚îÄ‚îÄ positional_encoding.py       #   Positional encoding sinusoidal
‚îÇ   ‚îú‚îÄ‚îÄ xla_dropout.py               #   Dropout compatible con XLA/Blackwell
‚îÇ   ‚îî‚îÄ‚îÄ config.py                    #   Configuraci√≥n del modelo (dataclass)
‚îÇ
‚îú‚îÄ‚îÄ data/                            # Pipeline de datos
‚îÇ   ‚îú‚îÄ‚îÄ combined_math_physics.json   #   Dataset final combinado (12,568 problemas)
‚îÇ   ‚îú‚îÄ‚îÄ tokenizer.py                 #   Tokenizador a nivel de car√°cter
‚îÇ   ‚îú‚îÄ‚îÄ dataset_builder.py           #   Constructor de tf.data.Dataset
‚îÇ   ‚îú‚îÄ‚îÄ schema.py                    #   Esquema unificado y validaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ build_combined_dataset.py    #   Script de construcci√≥n del dataset final
‚îÇ   ‚îú‚îÄ‚îÄ convert_gsm8k.py             #   Descarga y convierte GSM8K
‚îÇ   ‚îú‚îÄ‚îÄ convert_math_combined.py     #   Combina GSM8K + MATH_LLM
‚îÇ   ‚îú‚îÄ‚îÄ generate_physics_templates.py #  Genera problemas de f√≠sica param√©tricos
‚îÇ   ‚îî‚îÄ‚îÄ generate_math_solutions_llm.py # Genera soluciones con LLM para MATH
‚îÇ
‚îú‚îÄ‚îÄ training/                        # Loop de entrenamiento
‚îÇ   ‚îú‚îÄ‚îÄ train.py                     #   TransformerTrainer (GradientTape, checkpointing)
‚îÇ   ‚îú‚îÄ‚îÄ losses.py                    #   Loss con label smoothing + masked accuracy
‚îÇ   ‚îú‚îÄ‚îÄ metrics.py                   #   Exact match + validaci√≥n simb√≥lica (SymPy)
‚îÇ   ‚îî‚îÄ‚îÄ scheduler.py                 #   Learning rate: warmup + inverse sqrt decay
‚îÇ
‚îú‚îÄ‚îÄ inference/                       # Generaci√≥n de respuestas
‚îÇ   ‚îú‚îÄ‚îÄ generate.py                  #   Generaci√≥n autoregresiva (greedy, top-k, beam search)
‚îÇ   ‚îî‚îÄ‚îÄ chatbot.py                   #   Chatbot interactivo en terminal
‚îÇ
‚îú‚îÄ‚îÄ evaluation/                      # Evaluaci√≥n del modelo
‚îÇ   ‚îî‚îÄ‚îÄ evaluate_math_physics.py     #   Token accuracy + exact match por dominio
‚îÇ
‚îú‚îÄ‚îÄ notebooks/                       # Notebooks de demostraci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ 01_exploracion_datos.ipynb   #   Exploraci√≥n y an√°lisis del dataset
‚îÇ   ‚îú‚îÄ‚îÄ 02_entrenamiento.ipynb       #   Notebook de entrenamiento
‚îÇ   ‚îî‚îÄ‚îÄ 03_demo_profesor.ipynb       #   ‚≠ê DEMO: Chatbot con interfaz Gradio
‚îÇ
‚îú‚îÄ‚îÄ informe final/                   # Informe acad√©mico del proyecto
‚îÇ   ‚îî‚îÄ‚îÄ Informe_MelissaCardona_ChatbotMathPhysics.ipynb
‚îÇ
‚îú‚îÄ‚îÄ checkpoints/                     # Modelo entrenado (listo para usar)
‚îÇ   ‚îú‚îÄ‚îÄ best_model.weights.h5        #   Mejores pesos (por val_loss)
‚îÇ   ‚îú‚îÄ‚îÄ model_weights.weights.h5     #   Pesos finales
‚îÇ   ‚îú‚îÄ‚îÄ config.json                  #   Configuraci√≥n del modelo
‚îÇ   ‚îú‚îÄ‚îÄ vocab.json                   #   Vocabulario del tokenizador
‚îÇ   ‚îú‚îÄ‚îÄ training_history.json        #   Historia de entrenamiento (89 √©pocas)
‚îÇ   ‚îî‚îÄ‚îÄ evaluation_report.json       #   M√©tricas de evaluaci√≥n
‚îÇ
‚îú‚îÄ‚îÄ run_training.py                  # Script principal de entrenamiento
‚îú‚îÄ‚îÄ test_all.py                      # Suite de tests (14 tests)
‚îî‚îÄ‚îÄ requirements.txt                 # Dependencias Python
```

---

## üöÄ Gu√≠a R√°pida ‚Äî Para el Profesor

### ‚ö†Ô∏è El modelo YA viene entrenado. No necesita reentrenar nada.

#### 1. Descargar el proyecto

**Opci√≥n A ‚Äî Git**:
```bash
git clone https://github.com/MelissaCardona2003/Chat-bot-de-matemacticas-y-f-sica-con-TensorFlow.git
cd Chat-bot-de-matemacticas-y-f-sica-con-TensorFlow
```

**Opci√≥n B ‚Äî ZIP**: En GitHub ‚Üí bot√≥n verde **Code** ‚Üí **Download ZIP** ‚Üí descomprimir.

#### 2. Crear entorno e instalar dependencias

```bash
python -m venv .venv
source .venv/bin/activate   # Linux/Mac
# .venv\Scripts\activate    # Windows

pip install -r requirements.txt
```

> Funciona en **CPU**. No necesita GPU para ejecutar la demo.

#### 3. Ejecutar la demo del chatbot

```bash
jupyter notebook notebooks/03_demo_profesor.ipynb
```

Ejecutar **todas las celdas en orden** (Shift+Enter). La interfaz Gradio se abrir√° autom√°ticamente con:
- Selector de dominio (math / physics)
- Ejemplos pre-cargados para probar
- M√©tricas en tiempo real (confianza, perplexity, tiempo)
- Secci√≥n de an√°lisis y limitaciones

#### ¬øQu√© esperar?

- El modelo genera soluciones en formato **"Step 1:... Step 2:... Answer:..."**
- Las respuestas muestran el estilo correcto, pero los **valores num√©ricos** no son precisos
- Esto es esperado dado el tama√±o del modelo (7.4M params vs 117M+ de GPT-2)
- El notebook incluye un an√°lisis detallado de por qu√© ocurre y qu√© se necesitar√≠a para mejorar

---

## ‚ö†Ô∏è Limitaciones Conocidas

1. **0% Exact Match**: El modelo produce respuestas con formato correcto pero valores num√©ricos incorrectos
2. **Tokenizaci√≥n car√°cter a car√°cter**: Un problema de 100 palabras ‚Üí ~500 tokens (vs ~25 con BPE)
3. **Escala del modelo**: 7.4M par√°metros (~16x menor que GPT-2 small)
4. **Sin pre-entrenamiento**: Aprende todo desde cero

### ¬øQu√© S√ç demuestra este proyecto?

- ‚úÖ Implementaci√≥n correcta de un Transformer Encoder-Decoder completo desde cero
- ‚úÖ Pipeline de datos robusto (descarga, limpieza, validaci√≥n, schema unificado)
- ‚úÖ Entrenamiento con t√©cnicas modernas (label smoothing, LR scheduling, early stopping)
- ‚úÖ Evaluaci√≥n honesta y rigurosa con m√©tricas apropiadas
- ‚úÖ Despliegue con interfaz interactiva profesional (Gradio)

---

## üìö Referencias

- Vaswani et al., *"Attention Is All You Need"*, NeurIPS 2017
- Cobbe et al., *"Training Verifiers to Solve Math Word Problems"* (GSM8K), 2021
- Hendrycks et al., *"Measuring Mathematical Problem Solving With the MATH Dataset"*, NeurIPS 2021
- Radford et al., *"Language Models are Unsupervised Multitask Learners"* (GPT-2), 2019

---

## üõ†Ô∏è Tecnolog√≠as

- **TensorFlow 2.x** ‚Äî Framework de deep learning
- **Gradio** ‚Äî Interfaz web interactiva
- **NumPy / Matplotlib** ‚Äî C√°lculo num√©rico y visualizaci√≥n
- **SymPy** ‚Äî Validaci√≥n simb√≥lica de respuestas
- **Datasets (HuggingFace)** ‚Äî Descarga de datasets

---

*Melissa Cardona ‚Äî 2026*
