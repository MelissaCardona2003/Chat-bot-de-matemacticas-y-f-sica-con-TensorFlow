{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b58a164c",
   "metadata": {},
   "source": [
    "# 03 â€” Demo: Transformer Tutor de MatemÃ¡ticas y FÃ­sica\n",
    "\n",
    "Notebook de demostraciÃ³n del modelo Transformer con interfaz en **Gradio Blocks**.\n",
    "\n",
    "1. Carga el modelo Transformer entrenado desde checkpoints\n",
    "2. GeneraciÃ³n autoregresiva con mÃ©tricas **reales basadas en logits** (confianza, perplexity)\n",
    "3. Interfaz interactiva con selector de dominio (math / physics)\n",
    "4. Curvas de entrenamiento, resultados de evaluaciÃ³n y anÃ¡lisis de limitaciones\n",
    "5. DiscusiÃ³n tÃ©cnica honesta: quÃ© puede y quÃ© NO puede el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "753103b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… TensorFlow 2.20.0\n",
      "âœ… Gradio 4.44.0\n",
      "âœ… GPU: []\n",
      "âœ… Proyecto: /home/melissa/transformer_math_physics_tutor\n"
     ]
    }
   ],
   "source": [
    "# === Setup: GPU workaround para Blackwell + imports ===\n",
    "import os, sys, subprocess\n",
    "\n",
    "# XLA/GPU config ANTES de importar TF\n",
    "os.environ.setdefault(\"XLA_FLAGS\", \"--xla_gpu_cuda_data_dir=/usr/local/cuda-12.8\")\n",
    "os.environ.setdefault(\"TF_XLA_FLAGS\", \"--tf_xla_auto_jit=2\")\n",
    "\n",
    "try:\n",
    "    import gradio as gr\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"gradio==4.44.0\", \"-q\"])\n",
    "    import gradio as gr\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "\n",
    "# GPU memory growth + Blackwell cast patch\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    _original_cast = tf.cast\n",
    "    def _blackwell_cast(x, dtype, name=None):\n",
    "        if tf.executing_eagerly():\n",
    "            with tf.device('/CPU:0'):\n",
    "                return _original_cast(x, dtype, name=name)\n",
    "        return _original_cast(x, dtype, name=name)\n",
    "    tf.cast = _blackwell_cast\n",
    "\n",
    "# Ruta del proyecto\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "for p in [project_root, os.path.dirname(project_root)]:\n",
    "    if p not in sys.path:\n",
    "        sys.path.insert(0, p)\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "print(f\"âœ… TensorFlow {tf.__version__}\")\n",
    "print(f\"âœ… Gradio {gr.__version__}\")\n",
    "print(f\"âœ… GPU: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(f\"âœ… Proyecto: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72be48ad",
   "metadata": {},
   "source": [
    "## 1. Cargar Modelo Pre-entrenado y Resultados de EvaluaciÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56232edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConfiguraciÃ³n cargada desde /home/melissa/transformer_math_physics_tutor/checkpoints/config.json\n",
      "Vocabulario cargado desde /home/melissa/transformer_math_physics_tutor/checkpoints/vocab.json (135 tokens)\n",
      "âœ… Modelo cargado: 7,476,615 parÃ¡metros (best_model.weights.h5)\n",
      "âœ… Dispositivo: CPU\n",
      "âœ… Vocab size: 135 (character-level)\n",
      "âœ… Accuracy â€” Train: 83.3%, Val: 82.1%\n",
      "âœ… Exact Match (test): 0/100 = 0.0%\n"
     ]
    }
   ],
   "source": [
    "from transformer_math_physics_tutor.data.tokenizer import CharTokenizer\n",
    "from transformer_math_physics_tutor.models.config import TransformerConfig\n",
    "from transformer_math_physics_tutor.models.transformer import Transformer\n",
    "\n",
    "# --- Cargar configuraciÃ³n ---\n",
    "checkpoint_dir = os.path.join(project_root, 'checkpoints')\n",
    "config = TransformerConfig.load(os.path.join(checkpoint_dir, 'config.json'))\n",
    "\n",
    "# --- Cargar tokenizer ---\n",
    "tokenizer = CharTokenizer(os.path.join(checkpoint_dir, 'vocab.json'))\n",
    "config.vocab_size = tokenizer.vocab_size\n",
    "\n",
    "# --- Crear e inicializar modelo ---\n",
    "with tf.device('/CPU:0'):\n",
    "    model = Transformer(config)\n",
    "    dummy_enc = tf.zeros((1, config.max_encoder_len), dtype=tf.int32)\n",
    "    dummy_dec = tf.zeros((1, config.max_decoder_len), dtype=tf.int32)\n",
    "    _ = model((dummy_enc, dummy_dec), training=False)\n",
    "\n",
    "    # Preferir best_model si existe\n",
    "    best_path = os.path.join(checkpoint_dir, 'best_model.weights.h5')\n",
    "    fallback_path = os.path.join(checkpoint_dir, 'model_weights.weights.h5')\n",
    "    weights_path = best_path if os.path.exists(best_path) else fallback_path\n",
    "    model.load_weights(weights_path)\n",
    "\n",
    "num_params = model.count_params()\n",
    "device = \"GPU\" if gpus else \"CPU\"\n",
    "\n",
    "# --- Historial de entrenamiento ---\n",
    "history = None\n",
    "final_train_acc = final_val_acc = 0.0\n",
    "try:\n",
    "    with open(os.path.join(checkpoint_dir, 'training_history.json'), 'r') as f:\n",
    "        history = json.load(f)\n",
    "    final_train_acc = history['train_accuracy'][-1]\n",
    "    if history.get('val_accuracy'):\n",
    "        final_val_acc = history['val_accuracy'][-1]\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# --- Reporte de evaluaciÃ³n ---\n",
    "eval_report = None\n",
    "try:\n",
    "    with open(os.path.join(checkpoint_dir, 'evaluation_report.json'), 'r') as f:\n",
    "        eval_report = json.load(f)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# --- Config dict para display ---\n",
    "with open(os.path.join(checkpoint_dir, 'config.json'), 'r') as f:\n",
    "    config_dict = json.load(f)\n",
    "\n",
    "print(f\"âœ… Modelo cargado: {num_params:,} parÃ¡metros ({os.path.basename(weights_path)})\")\n",
    "print(f\"âœ… Dispositivo: {device}\")\n",
    "print(f\"âœ… Vocab size: {tokenizer.vocab_size} (character-level)\")\n",
    "print(f\"âœ… Accuracy â€” Train: {final_train_acc:.1%}, Val: {final_val_acc:.1%}\")\n",
    "if eval_report:\n",
    "    em = eval_report.get('exact_match', {})\n",
    "    print(f\"âœ… Exact Match (test): {em.get('correct', 0)}/{em.get('total', 0)} = {em.get('pct', 0):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d316fde1",
   "metadata": {},
   "source": [
    "## 2. FunciÃ³n de GeneraciÃ³n con MÃ©tricas Reales\n",
    "\n",
    "La funciÃ³n `generate_with_metrics` usa decodificaciÃ³n **autoregresiva con top-k sampling** y calcula mÃ©tricas basadas en **logits reales**:\n",
    "\n",
    "- **Confianza**: $\\text{confidence} = \\frac{1}{T}\\sum_{t=1}^{T} \\max_v P(v \\mid v_{<t})$ â€” promedio de la probabilidad mÃ¡xima por paso\n",
    "- **Perplexity**: $\\text{PPL} = \\exp\\!\\left(-\\frac{1}{T}\\sum_{t=1}^{T} \\log P(v_t \\mid v_{<t})\\right)$ â€” sobre la secuencia seleccionada\n",
    "- **Repetition penalty**: Penaliza tokens ya generados para evitar bucles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db7054f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST RÃPIDO â€” Math\n",
      "============================================================\n",
      "  Respuesta: Step 1: There are 40 - 24 = <<40-24=36>>36 slices left.\n",
      "Step 2: There are 50 - 36 = <<50-36=14>>14 slices left.\n",
      "Answer: 14\n",
      "  Confianza: 83.30% | PPL: 1.36 | 122 tokens | 18208ms\n",
      "\n",
      "TEST RÃPIDO â€” Physics\n",
      "  Respuesta: Step 1: There are 250 minutes in all when 350 minutes\n",
      "  Confianza: 73.76% | PPL: 1.67 | 53 tokens | 6821ms\n"
     ]
    }
   ],
   "source": [
    "def _pad_to_length(tokens, max_len, pad_id):\n",
    "    \"\"\"Paddea o trunca una secuencia a max_len.\"\"\"\n",
    "    if len(tokens) > max_len:\n",
    "        return tokens[:max_len - 1] + [tokens[-1]]\n",
    "    return tokens + [pad_id] * (max_len - len(tokens))\n",
    "\n",
    "\n",
    "def _detect_ngram_repeat(tokens, n=8):\n",
    "    \"\"\"Detecta si los Ãºltimos n tokens se repiten antes.\"\"\"\n",
    "    if len(tokens) < n * 2:\n",
    "        return False\n",
    "    last_ngram = tokens[-n:]\n",
    "    for i in range(len(tokens) - n * 2, max(0, len(tokens) - n * 5) - 1, -1):\n",
    "        if tokens[i:i + n] == last_ngram:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def generate_with_metrics(problem, max_length=300, temperature=0.3,\n",
    "                          top_k=10, repetition_penalty=1.3):\n",
    "    \"\"\"\n",
    "    Genera respuesta con mÃ©tricas reales basadas en logits.\n",
    "\n",
    "    Usa la misma lÃ³gica de generaciÃ³n que inference/generate.py\n",
    "    (padding, top-k, repetition penalty) + cÃ¡lculo de mÃ©tricas.\n",
    "\n",
    "    Returns dict con: text, confidence, perplexity, inference_time, num_tokens\n",
    "    \"\"\"\n",
    "    if not problem or not problem.strip():\n",
    "        return {\n",
    "            'text': \"âš ï¸ Escribe un problema de matemÃ¡ticas o fÃ­sica.\",\n",
    "            'confidence': 0.0, 'perplexity': 0.0,\n",
    "            'inference_time': 0.0, 'num_tokens': 0\n",
    "        }\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Tokenizar y paddear encoder input (misma longitud que entrenamiento)\n",
    "    encoder_tokens = tokenizer.encode(problem, add_special_tokens=True)\n",
    "    max_enc_len = config.max_encoder_len  # 200\n",
    "    encoder_tokens = _pad_to_length(encoder_tokens, max_enc_len, tokenizer.pad_token_id)\n",
    "    encoder_input = tf.constant([encoder_tokens], dtype=tf.int32)  # (1, 200)\n",
    "\n",
    "    # Inicializar decoder\n",
    "    decoder_tokens = [tokenizer.start_token_id]\n",
    "    generated_tokens = []\n",
    "    token_probs = []\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        decoder_input = tf.constant([decoder_tokens], dtype=tf.int32)\n",
    "\n",
    "        # Forward pass -> logits\n",
    "        predictions = model((encoder_input, decoder_input), training=False)\n",
    "        logits = predictions[0, -1, :]  # (vocab_size,)\n",
    "\n",
    "        # Probabilidades ANTES de manipulaciÃ³n (para mÃ©tricas puras)\n",
    "        probs = tf.nn.softmax(logits).numpy()\n",
    "        max_prob = float(np.max(probs))\n",
    "\n",
    "        # Repetition penalty\n",
    "        logits_np = logits.numpy()\n",
    "        if repetition_penalty > 1.0 and generated_tokens:\n",
    "            for tok_id in set(generated_tokens):\n",
    "                if logits_np[tok_id] > 0:\n",
    "                    logits_np[tok_id] /= repetition_penalty\n",
    "                else:\n",
    "                    logits_np[tok_id] *= repetition_penalty\n",
    "\n",
    "        # SelecciÃ³n de token con top-k sampling\n",
    "        if temperature <= 0.0:\n",
    "            predicted_id = int(np.argmax(logits_np))\n",
    "        else:\n",
    "            scaled = logits_np / temperature\n",
    "            if top_k > 0:\n",
    "                top_indices = np.argpartition(scaled, -top_k)[-top_k:]\n",
    "                mask = np.full_like(scaled, -1e9)\n",
    "                mask[top_indices] = scaled[top_indices]\n",
    "                scaled = mask\n",
    "            exp_scaled = np.exp(scaled - np.max(scaled))\n",
    "            sampling_probs = exp_scaled / exp_scaled.sum()\n",
    "            predicted_id = int(np.random.choice(len(sampling_probs), p=sampling_probs))\n",
    "\n",
    "        # Probabilidad del token seleccionado (de distribuciÃ³n original)\n",
    "        selected_prob = float(probs[predicted_id])\n",
    "        token_probs.append(selected_prob)\n",
    "\n",
    "        if predicted_id == tokenizer.end_token_id:\n",
    "            break\n",
    "\n",
    "        generated_tokens.append(predicted_id)\n",
    "\n",
    "        if len(generated_tokens) > 20 and _detect_ngram_repeat(generated_tokens, n=10):\n",
    "            break\n",
    "\n",
    "        decoder_tokens.append(predicted_id)\n",
    "\n",
    "    inference_time = time.time() - start_time\n",
    "\n",
    "    # Decodificar\n",
    "    generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "    # MÃ©tricas\n",
    "    if token_probs:\n",
    "        confidence = float(np.mean(token_probs))\n",
    "        nll = -np.mean([np.log(p + 1e-10) for p in token_probs])\n",
    "        perplexity = float(np.exp(min(nll, 20)))  # Cap para evitar overflow\n",
    "    else:\n",
    "        confidence = 0.0\n",
    "        perplexity = 0.0\n",
    "\n",
    "    return {\n",
    "        'text': generated_text,\n",
    "        'confidence': confidence,\n",
    "        'perplexity': perplexity,\n",
    "        'inference_time': inference_time,\n",
    "        'num_tokens': len(generated_tokens)\n",
    "    }\n",
    "\n",
    "\n",
    "# === Test rÃ¡pido (math + physics) ===\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST RÃPIDO â€” Math\")\n",
    "print(\"=\" * 60)\n",
    "r = generate_with_metrics(\"Janet has 3 apples and buys 5 more. How many does she have?\")\n",
    "print(f\"  Respuesta: {r['text'][:200]}\")\n",
    "print(f\"  Confianza: {r['confidence']:.2%} | PPL: {r['perplexity']:.2f} | {r['num_tokens']} tokens | {r['inference_time']*1000:.0f}ms\")\n",
    "\n",
    "print(\"\\nTEST RÃPIDO â€” Physics\")\n",
    "r2 = generate_with_metrics(\"A car accelerates from rest at 3 m/sÂ² for 5 seconds. What is its final velocity?\")\n",
    "print(f\"  Respuesta: {r2['text'][:200]}\")\n",
    "print(f\"  Confianza: {r2['confidence']:.2%} | PPL: {r2['perplexity']:.2f} | {r2['num_tokens']} tokens | {r2['inference_time']*1000:.0f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e93fdb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DIAGNÃ“STICO DEL MODELO â€” Math + Physics\n",
      "======================================================================\n",
      "\n",
      "ğŸŸ¢ [MATH] Janet has 3 apples and buys 5 more. How many apples does she have?\n",
      "   Respuesta: Step 1: There are 30 + 4 = <<30+4=72>>72 students in the former that were children.\n",
      "Step 2: There are 72 / 5 = <<72/5=14>>14 students in the second that were children.\n",
      "Answer: 14\n",
      "   Confianza: 81.55% | PPL: 1.37 | 178 tokens\n",
      "\n",
      "ğŸŸ¢ [MATH] A store sells pencils for $2 each. Tom buys 7 pencils. How much does he spend?\n",
      "   Respuesta: Step 1: The first combined amount of money that is $2.00 x 2 = $<<2.00*2=4.00>>4.00.\n",
      "Step 2: The second combined amount of money that is $4.00 x 2 = $<<4.00*2=8.00>>8.00.\n",
      "Step 3: The second combined a\n",
      "   Confianza: 87.44% | PPL: 1.20 | 300 tokens\n",
      "\n",
      "ğŸŸ¢ [MATH] A train travels 60 miles per hour for 3 hours. How far does it go?\n",
      "   Respuesta: Step 1: There are 24 students in the first class, so there are 24/2=<<24/2=12>>12 students in the second class.\n",
      "Step 2: There are 24 students in the first class, so there are 24/12=<<24/12=2>>2 studen\n",
      "   Confianza: 87.50% | PPL: 1.20 | 300 tokens\n",
      "\n",
      "ğŸŸ¡ [MATH] Sarah has 24 cookies and wants to share them equally among 6 friends. How many cookies does each friend get?\n",
      "   Respuesta: Step 1: There are 4 children boxes in the first two to the second to the first\n",
      "   Confianza: 73.48% | PPL: 1.60 | 78 tokens\n",
      "\n",
      "ğŸŸ¢ [MATH] John has $50. He buys a book for $12 and a pen for $3. How much money does he have left?\n",
      "   Respuesta: Step 1: The number of students is 2 * 350 = <<2*350=750>>750\n",
      "Step 2: The number of students is 750 / 100 = <<750/100=7.50>>7.50\n",
      "Step 3: The number of students is 7.50 + 4.50 = <<7.50+4.50=7.50>>7.50\n",
      "S\n",
      "   Confianza: 86.62% | PPL: 1.23 | 273 tokens\n",
      "\n",
      "ğŸŸ¢ [PHYSICS] A car accelerates from rest at 3 m/sÂ² for 5 seconds. What is its final velocity?\n",
      "   Respuesta: Step 1: There are 24 + 3 = <<24+3=56>>56 square fee.\n",
      "Step 2: There are 56 - 56 = <<56-56=40>>40 square fee.\n",
      "Answer: 40\n",
      "   Confianza: 82.38% | PPL: 1.36 | 118 tokens\n",
      "\n",
      "ğŸŸ¢ [PHYSICS] A 10 kg box is pushed with a force of 50 N. What is its acceleration?\n",
      "   Respuesta: Step 1: The first two candy will be divided by 3. We know that the second two candy will be divided by 3, so the second two candy will be divided by 3 * 3 = 8.\n",
      "Answer: 8\n",
      "   Confianza: 80.86% | PPL: 1.38 | 169 tokens\n",
      "\n",
      "ğŸŸ¡ [PHYSICS] How much heat is needed to raise the temperature of 2 kg of water by 30Â°C?\n",
      "   Respuesta: Step 1: The first combined the second combined\n",
      "   Confianza: 79.67% | PPL: 1.40 | 47 tokens\n",
      "\n",
      "ğŸŸ¢ [PHYSICS] A circuit has a voltage of 12V and a resistance of 4Î©. What is the current?\n",
      "   Respuesta: Step 1: There are 4 x 3 = <<4*3=12>>12 students.\n",
      "Step 2: There are 4 x 3 = <<4*3=12>>12 students.\n",
      "Step 3: There are 4 x 3 = <<4*3=12>>12 students in the fourth class.\n",
      "Step 4: There are 12 + 12 = <<12+\n",
      "   Confianza: 85.72% | PPL: 1.25 | 230 tokens\n",
      "\n",
      "ğŸŸ¢ [PHYSICS] An object is dropped from 80 m. How long does it take to hit the ground?\n",
      "   Respuesta: Step 1: The first to bake the class is 5*2=<<5*2=10>>10.\n",
      "Step 2: The second to bake the class is 10+10=<<10+10=100>>100.\n",
      "Step 3: The third to bake the class is 100-10=<<100-10=90>>90.\n",
      "Answer: 90\n",
      "   Confianza: 83.22% | PPL: 1.32 | 194 tokens\n"
     ]
    }
   ],
   "source": [
    "# === DiagnÃ³stico: probar con problemas de ambos dominios ===\n",
    "test_problems = [\n",
    "    (\"Janet has 3 apples and buys 5 more. How many apples does she have?\", \"math\"),\n",
    "    (\"A store sells pencils for $2 each. Tom buys 7 pencils. How much does he spend?\", \"math\"),\n",
    "    (\"A train travels 60 miles per hour for 3 hours. How far does it go?\", \"math\"),\n",
    "    (\"Sarah has 24 cookies and wants to share them equally among 6 friends. How many cookies does each friend get?\", \"math\"),\n",
    "    (\"John has $50. He buys a book for $12 and a pen for $3. How much money does he have left?\", \"math\"),\n",
    "    (\"A car accelerates from rest at 3 m/sÂ² for 5 seconds. What is its final velocity?\", \"physics\"),\n",
    "    (\"A 10 kg box is pushed with a force of 50 N. What is its acceleration?\", \"physics\"),\n",
    "    (\"How much heat is needed to raise the temperature of 2 kg of water by 30Â°C?\", \"physics\"),\n",
    "    (\"A circuit has a voltage of 12V and a resistance of 4Î©. What is the current?\", \"physics\"),\n",
    "    (\"An object is dropped from 80 m. How long does it take to hit the ground?\", \"physics\"),\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DIAGNÃ“STICO DEL MODELO â€” Math + Physics\")\n",
    "print(\"=\" * 70)\n",
    "for problem, domain in test_problems:\n",
    "    r = generate_with_metrics(problem)\n",
    "    if r['confidence'] >= 0.8:\n",
    "        status = \"ğŸŸ¢\"\n",
    "    elif r['confidence'] >= 0.5:\n",
    "        status = \"ğŸŸ¡\"\n",
    "    else:\n",
    "        status = \"ğŸ”´\"\n",
    "    print(f\"\\n{status} [{domain.upper()}] {problem}\")\n",
    "    print(f\"   Respuesta: {r['text'][:200]}\")\n",
    "    print(f\"   Confianza: {r['confidence']:.2%} | PPL: {r['perplexity']:.2f} | {r['num_tokens']} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52448741",
   "metadata": {},
   "source": [
    "## 2.5 DiagnÃ³stico Profundo â€” Â¿QuÃ© aprendiÃ³ el modelo?\n",
    "\n",
    "Vamos a investigar tres hipÃ³tesis:\n",
    "1. **Â¿El encoder influye en la salida?** â€” Si cambiamos el problema, Â¿cambia la respuesta?\n",
    "2. **Â¿Greedy vs sampling?** â€” Â¿El ruido de sampling empeora las cosas?\n",
    "3. **Â¿Teacher forcing ayuda?** â€” Si le damos los primeros tokens correctos, Â¿puede continuar?\n",
    "4. **Â¿Cross-attention funciona?** â€” Â¿El decoder atiende al encoder?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da4b9812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DIAGNÃ“STICO 1: Â¿El encoder influye en la salida?\n",
      "  (Mismo decoder start, dos problemas distintos, greedy)\n",
      "======================================================================\n",
      "\n",
      "  Problema: What is 2 + 3?\n",
      "  Respuesta (greedy): Step 1: The first term is a congruent that the second term is a\n",
      "  Confianza: 77.61% | Tokens: 63\n",
      "\n",
      "  Problema: A farmer has 100 cows and sells 30. How many cows remain?\n",
      "  Respuesta (greedy): Step 1: There are 24 considers in the first two terms, so there are 24 considers in the second term.\n",
      "Step 2: There are 24 considers in the second term.\n",
      "Answer: 24\n",
      "  Confianza: 81.03% | Tokens: 162\n",
      "\n",
      "  Problema: A ball is thrown upward with a velocity of 20 m/s. What is the maximum height?\n",
      "  Respuesta (greedy): Step 1: There are 24 ways to clean the first two times that the first\n",
      "  Confianza: 75.85% | Tokens: 69\n",
      "\n",
      "======================================================================\n",
      "DIAGNÃ“STICO 2: Â¿El encoder produce embeddings distintos?\n",
      "======================================================================\n",
      "  'What is 5 + 3?' â†’ mean=0.0031, std=0.0403, norm=9.15\n",
      "  'A car moves at 10 m/s for 5 seconds. What distance' â†’ mean=0.0016, std=0.0446, norm=10.10\n",
      "  'XYZXYZXYZ' â†’ mean=0.0020, std=0.0398, norm=9.02\n",
      "  Cosine sim [0] vs [1]: 0.3349\n",
      "  Cosine sim [0] vs [2]: 0.6098\n",
      "  Cosine sim [1] vs [2]: 0.4106\n",
      "\n",
      "======================================================================\n",
      "DIAGNÃ“STICO 3: Â¿Cross-attention pesos?\n",
      "======================================================================\n",
      "  decoder_layer1_block2: tokens reales=100.0%, padding=0.0%\n",
      "    Entropy: 2.773 / 2.773 (max) = 100.0% â†’ âš ï¸ UNIFORME\n",
      "    Top-5: [('<END>', '0.0625'), ('+', '0.0625'), ('3', '0.0625'), ('5', '0.0625'), ('?', '0.0625')]\n",
      "  decoder_layer2_block2: tokens reales=100.0%, padding=0.0%\n",
      "    Entropy: 2.773 / 2.773 (max) = 100.0% â†’ âš ï¸ UNIFORME\n",
      "    Top-5: [('<END>', '0.0625'), ('5', '0.0625'), ('+', '0.0625'), ('?', '0.0625'), ('t', '0.0625')]\n",
      "  decoder_layer3_block2: tokens reales=100.0%, padding=0.0%\n",
      "    Entropy: 2.773 / 2.773 (max) = 100.0% â†’ âš ï¸ UNIFORME\n",
      "    Top-5: [('<END>', '0.0625'), ('5', '0.0625'), ('+', '0.0625'), ('?', '0.0625'), ('<START>', '0.0625')]\n",
      "  decoder_layer4_block2: tokens reales=100.0%, padding=0.0%\n",
      "    Entropy: 2.773 / 2.773 (max) = 100.0% â†’ âš ï¸ UNIFORME\n",
      "    Top-5: [('<END>', '0.0625'), ('5', '0.0625'), ('+', '0.0625'), ('?', '0.0625'), ('t', '0.0625')]\n",
      "\n",
      "======================================================================\n",
      "DIAGNÃ“STICO 4: Teacher forcing â€” Â¿puede continuar?\n",
      "======================================================================\n",
      "  Problema: 'What is 5 + 3?'\n",
      "  Teacher forcing: 'Step 1: 5 + 3 = '\n",
      "  ContinuaciÃ³n: 'Step 1: 5 + 3 = 15 + 15 + 15 + 15 + 15 + 15 + 15 + 15 + 15 + 15 + 15 + 15 + 15 + 15 + 15 + 15 + 15 + 15 + 15 + 15 + '\n",
      "\n",
      "======================================================================\n",
      "â•â•â• RESUMEN DEL DIAGNÃ“STICO â•â•â•\n",
      "======================================================================\n",
      "\n",
      "  HALLAZGO PRINCIPAL: Cross-attention COLAPSADA a distribuciÃ³n uniforme.\n",
      "\n",
      "  Â¿QuÃ© significa?\n",
      "  â†’ El decoder NO lee el encoder. Genera texto \"genÃ©rico\" como un\n",
      "    language model, sin condicionarse en el problema de entrada.\n",
      "  â†’ Todas las capas tienen entropy â‰ˆ 100% (distribuciÃ³n uniforme).\n",
      "\n",
      "  Â¿Por quÃ© ocurre?\n",
      "  â†’ Con tokenizaciÃ³n char-level, el decoder puede predecir el\n",
      "    siguiente carÃ¡cter con alta accuracy (~82%) usando SOLO el\n",
      "    contexto autoregresivo previo, sin necesitar cross-attention.\n",
      "  â†’ Ejemplo: despuÃ©s de \"Ste\", el modelo predice \"p\" sin mirar\n",
      "    el encoder. DespuÃ©s de \"Step \", predice \"1\" o \"2\", etc.\n",
      "\n",
      "  SOLUCIÃ“N: Aplicar \"decoder token masking\" durante entrenamiento.\n",
      "  â†’ Reemplazar aleatoriamente 15-20% de los tokens del decoder input\n",
      "    con un token <MASK> durante training.\n",
      "  â†’ Esto ROMPE el atajo autoregresivo y FUERZA al modelo a usar\n",
      "    cross-attention para obtener la informaciÃ³n faltante.\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"DIAGNÃ“STICO 1: Â¿El encoder influye en la salida?\")\n",
    "print(\"  (Mismo decoder start, dos problemas distintos, greedy)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for prob_text in [\n",
    "    \"What is 2 + 3?\",\n",
    "    \"A farmer has 100 cows and sells 30. How many cows remain?\",\n",
    "    \"A ball is thrown upward with a velocity of 20 m/s. What is the maximum height?\",\n",
    "]:\n",
    "    r = generate_with_metrics(prob_text, temperature=0.0, top_k=0)\n",
    "    print(f\"\\n  Problema: {prob_text}\")\n",
    "    print(f\"  Respuesta (greedy): {r['text'][:250]}\")\n",
    "    print(f\"  Confianza: {r['confidence']:.2%} | Tokens: {r['num_tokens']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DIAGNÃ“STICO 2: Â¿El encoder produce embeddings distintos?\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "problems_to_compare = [\n",
    "    \"What is 5 + 3?\",\n",
    "    \"A car moves at 10 m/s for 5 seconds. What distance does it travel?\",\n",
    "    \"XYZXYZXYZ\",\n",
    "]\n",
    "\n",
    "enc_outputs = []\n",
    "for prob in problems_to_compare:\n",
    "    enc_tokens = tokenizer.encode(prob, add_special_tokens=True)\n",
    "    enc_padded = _pad_to_length(enc_tokens, config.max_encoder_len, tokenizer.pad_token_id)\n",
    "    enc_input = tf.constant([enc_padded], dtype=tf.int32)\n",
    "    enc_padding_mask = model.create_padding_mask(enc_input)\n",
    "    enc_out = model.encoder(enc_input, training=False, mask=enc_padding_mask)\n",
    "    enc_outputs.append(enc_out.numpy())\n",
    "    print(f\"  '{prob[:50]}' â†’ mean={enc_out.numpy().mean():.4f}, \"\n",
    "          f\"std={enc_out.numpy().std():.4f}, norm={np.linalg.norm(enc_out.numpy()):.2f}\")\n",
    "\n",
    "from numpy.linalg import norm\n",
    "for i in range(len(enc_outputs)):\n",
    "    for j in range(i+1, len(enc_outputs)):\n",
    "        a, b = enc_outputs[i].flatten(), enc_outputs[j].flatten()\n",
    "        cos_sim = np.dot(a, b) / (norm(a) * norm(b) + 1e-8)\n",
    "        print(f\"  Cosine sim [{i}] vs [{j}]: {cos_sim:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DIAGNÃ“STICO 3: Â¿Cross-attention pesos?\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "prob = \"What is 5 + 3?\"\n",
    "enc_tokens = tokenizer.encode(prob, add_special_tokens=True)\n",
    "enc_padded = _pad_to_length(enc_tokens, config.max_encoder_len, tokenizer.pad_token_id)\n",
    "enc_input = tf.constant([enc_padded], dtype=tf.int32)\n",
    "dec_input = tf.constant([[tokenizer.start_token_id]], dtype=tf.int32)\n",
    "\n",
    "enc_padding_mask = model.create_padding_mask(enc_input)\n",
    "dec_padding_mask = model.create_padding_mask(enc_input)\n",
    "look_ahead_mask = model.create_look_ahead_mask(tf.shape(dec_input)[1])\n",
    "dec_target_padding_mask = model.create_padding_mask(dec_input)\n",
    "combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "enc_output = model.encoder(enc_input, training=False, mask=enc_padding_mask)\n",
    "dec_output, attn_weights = model.decoder(\n",
    "    dec_input, enc_output, training=False,\n",
    "    look_ahead_mask=combined_mask, padding_mask=dec_padding_mask\n",
    ")\n",
    "\n",
    "n_real = len(enc_tokens)\n",
    "for layer_name, weights in attn_weights.items():\n",
    "    if 'block2' in layer_name:\n",
    "        w = weights.numpy()[0]  # (heads, tar_len, inp_len)\n",
    "        real_w = w[:, :, :n_real]\n",
    "        pad_w = w[:, :, n_real:]\n",
    "        total = real_w.sum() + pad_w.sum()\n",
    "        print(f\"  {layer_name}: tokens reales={real_w.sum()/total:.1%}, padding={pad_w.sum()/total:.1%}\")\n",
    "        avg = w.mean(axis=0)[0, :n_real]\n",
    "        top_pos = avg.argsort()[-5:][::-1]\n",
    "        chars = [tokenizer.decode([enc_padded[p]], skip_special_tokens=False) for p in top_pos]\n",
    "        entropy = -np.sum(avg * np.log(avg + 1e-10))\n",
    "        max_entropy = np.log(n_real)\n",
    "        print(f\"    Entropy: {entropy:.3f} / {max_entropy:.3f} (max) = {entropy/max_entropy:.1%} â†’ \"\n",
    "              f\"{'âš ï¸ UNIFORME' if entropy/max_entropy > 0.95 else 'âœ… Selectiva'}\")\n",
    "        print(f\"    Top-5: {list(zip(chars, [f'{avg[p]:.4f}' for p in top_pos]))}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DIAGNÃ“STICO 4: Teacher forcing â€” Â¿puede continuar?\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "prob = \"What is 5 + 3?\"\n",
    "partial = \"Step 1: 5 + 3 = \"\n",
    "enc_tokens = tokenizer.encode(prob, add_special_tokens=True)\n",
    "enc_padded = _pad_to_length(enc_tokens, config.max_encoder_len, tokenizer.pad_token_id)\n",
    "enc_input = tf.constant([enc_padded], dtype=tf.int32)\n",
    "\n",
    "sol_tokens = [tokenizer.start_token_id] + tokenizer.encode(partial, add_special_tokens=False)\n",
    "generated = list(sol_tokens)\n",
    "max_dec_len = config.max_decoder_len  # 300\n",
    "\n",
    "for _ in range(100):\n",
    "    if len(generated) >= max_dec_len:\n",
    "        break\n",
    "    dec_input = tf.constant([generated], dtype=tf.int32)\n",
    "    predictions = model((enc_input, dec_input), training=False)\n",
    "    next_id = int(np.argmax(predictions[0, -1, :].numpy()))\n",
    "    if next_id == tokenizer.end_token_id:\n",
    "        break\n",
    "    generated.append(next_id)\n",
    "\n",
    "full_text = tokenizer.decode(generated[1:], skip_special_tokens=True)\n",
    "print(f\"  Problema: '{prob}'\")\n",
    "print(f\"  Teacher forcing: '{partial}'\")\n",
    "print(f\"  ContinuaciÃ³n: '{full_text[:200]}'\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"â•â•â• RESUMEN DEL DIAGNÃ“STICO â•â•â•\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "  HALLAZGO PRINCIPAL: Cross-attention COLAPSADA a distribuciÃ³n uniforme.\n",
    "\n",
    "  Â¿QuÃ© significa?\n",
    "  â†’ El decoder NO lee el encoder. Genera texto \"genÃ©rico\" como un\n",
    "    language model, sin condicionarse en el problema de entrada.\n",
    "  â†’ Todas las capas tienen entropy â‰ˆ 100% (distribuciÃ³n uniforme).\n",
    "\n",
    "  Â¿Por quÃ© ocurre?\n",
    "  â†’ Con tokenizaciÃ³n char-level, el decoder puede predecir el\n",
    "    siguiente carÃ¡cter con alta accuracy (~82%) usando SOLO el\n",
    "    contexto autoregresivo previo, sin necesitar cross-attention.\n",
    "  â†’ Ejemplo: despuÃ©s de \"Ste\", el modelo predice \"p\" sin mirar\n",
    "    el encoder. DespuÃ©s de \"Step \", predice \"1\" o \"2\", etc.\n",
    "\n",
    "  SOLUCIÃ“N: Aplicar \"decoder token masking\" durante entrenamiento.\n",
    "  â†’ Reemplazar aleatoriamente 15-20% de los tokens del decoder input\n",
    "    con un token <MASK> durante training.\n",
    "  â†’ Esto ROMPE el atajo autoregresivo y FUERZA al modelo a usar\n",
    "    cross-attention para obtener la informaciÃ³n faltante.\n",
    "\"\"\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ab32bb",
   "metadata": {},
   "source": [
    "## 3. Interfaz Gradio Blocks â€” Demo para el Profesor\n",
    "\n",
    "Layout:\n",
    "- **Columna izquierda**: Entrada del problema, selector de dominio, botÃ³n de resolver, soluciÃ³n generada, respuesta destacada\n",
    "- **Columna derecha**: MÃ©tricas de generaciÃ³n, detalles de arquitectura\n",
    "- **Acordeones**: Detalles tÃ©cnicos, curvas de entrenamiento, evaluaciÃ³n, limitaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab95025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/melissa/.local/lib/python3.10/site-packages/gradio/analytics.py:106: UserWarning: IMPORTANT: You are using gradio version 4.44.0, however version 4.44.1 is available, please upgrade. \n",
      "--------\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "\n",
    "\n",
    "def create_training_plot():\n",
    "    \"\"\"Crea grÃ¡fica de entrenamiento y la guarda como archivo temporal.\"\"\"\n",
    "    if not history:\n",
    "        return None\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3))\n",
    "    ax1.plot(history['train_loss'], label='Train', linewidth=2, color='#4f46e5')\n",
    "    if history.get('val_loss'):\n",
    "        ax1.plot(history['val_loss'], label='Val', linewidth=2, color='#f97316')\n",
    "    ax1.set_title('Loss', fontsize=13, fontweight='bold')\n",
    "    ax1.set_xlabel('Ã‰poca')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax2.plot(history['train_accuracy'], label='Train', linewidth=2, color='#4f46e5')\n",
    "    if history.get('val_accuracy'):\n",
    "        ax2.plot(history['val_accuracy'], label='Val', linewidth=2, color='#f97316')\n",
    "    ax2.set_title('Accuracy (token-level)', fontsize=13, fontweight='bold')\n",
    "    ax2.set_xlabel('Ã‰poca')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    tmp = tempfile.NamedTemporaryFile(suffix='.png', delete=False)\n",
    "    plt.savefig(tmp.name, format='png', dpi=100, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    return tmp.name\n",
    "\n",
    "\n",
    "def extract_answer_highlight(text):\n",
    "    \"\"\"Extrae la lÃ­nea Answer: y la resalta.\"\"\"\n",
    "    match = re.search(r\"(Answer:\\s*.+?)(?:\\n|$)\", text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return \"No se detectÃ³ lÃ­nea Answer:\"\n",
    "\n",
    "\n",
    "def tutor_interface(problem, domain):\n",
    "    \"\"\"Wrapper para Gradio: genera respuesta con mÃ©tricas.\"\"\"\n",
    "    # Agregar contexto de dominio si el usuario lo selecciona\n",
    "    if domain == \"physics\" and \"physics\" not in problem.lower():\n",
    "        prompt = problem  # El modelo debe inferir el dominio\n",
    "    else:\n",
    "        prompt = problem\n",
    "\n",
    "    result = generate_with_metrics(prompt, max_length=300)\n",
    "\n",
    "    # Indicador visual\n",
    "    if result['confidence'] >= 0.8:\n",
    "        conf_indicator = \"ğŸŸ¢ Alta\"\n",
    "    elif result['confidence'] >= 0.5:\n",
    "        conf_indicator = \"ğŸŸ¡ Media\"\n",
    "    else:\n",
    "        conf_indicator = \"ğŸ”´ Baja\"\n",
    "\n",
    "    # Extraer Answer:\n",
    "    answer_line = extract_answer_highlight(result['text'])\n",
    "\n",
    "    return (\n",
    "        result['text'],            # solution\n",
    "        f\"**{answer_line}**\",      # answer highlight\n",
    "        result['confidence'],      # slider\n",
    "        conf_indicator,            # status\n",
    "        f\"{result['perplexity']:.2f}\",\n",
    "        f\"{result['inference_time']*1000:.0f}\",\n",
    "        result['num_tokens']\n",
    "    )\n",
    "\n",
    "\n",
    "# === Dataset stats ===\n",
    "dataset_stats = \"\"\n",
    "try:\n",
    "    with open(os.path.join(project_root, 'data', 'combined_math_physics.json'), 'r') as f:\n",
    "        all_data = json.load(f)\n",
    "    n_math = sum(1 for d in all_data if d.get('domain') == 'math')\n",
    "    n_phys = sum(1 for d in all_data if d.get('domain') == 'physics')\n",
    "    n_train = sum(1 for d in all_data if d.get('split') == 'train')\n",
    "    dataset_stats = f\"Dataset: **{len(all_data):,}** problemas ({n_math:,} math + {n_phys:,} physics) | Train: **{n_train:,}**\"\n",
    "except Exception:\n",
    "    dataset_stats = \"Dataset: combined_math_physics.json\"\n",
    "\n",
    "# === Evaluation summary for header ===\n",
    "eval_summary = \"\"\n",
    "if eval_report:\n",
    "    ta = eval_report.get('token_accuracy', {})\n",
    "    em = eval_report.get('exact_match', {})\n",
    "    eval_summary = f\"Token Acc (test): **{ta.get('test_acc', 0):.1%}** | Exact Match: **{em.get('pct', 0):.1f}%**\"\n",
    "\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#  INTERFAZ GRADIO BLOCKS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "theme = gr.themes.Soft(primary_hue=\"indigo\", secondary_hue=\"blue\")\n",
    "\n",
    "with gr.Blocks(theme=theme, title=\"Transformer Tutor â€” Math & Physics\") as demo:\n",
    "\n",
    "    # â”€â”€ Header â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    gr.Markdown(f\"\"\"\n",
    "    # ğŸ“ Transformer Tutor â€” MatemÃ¡ticas y FÃ­sica\n",
    "\n",
    "    Modelo **Encoder-Decoder Transformer** desde cero |\n",
    "    **{num_params:,}** parÃ¡metros | Dispositivo: **{device}** |\n",
    "    Train: **{final_train_acc:.1%}** | Val: **{final_val_acc:.1%}** |\n",
    "    {eval_summary}\n",
    "\n",
    "    {dataset_stats}\n",
    "    \"\"\")\n",
    "\n",
    "    with gr.Row():\n",
    "\n",
    "        # â•â•â• COLUMNA IZQUIERDA: Entrada / Salida â•â•â•\n",
    "        with gr.Column(scale=3):\n",
    "\n",
    "            with gr.Row():\n",
    "                input_box = gr.Textbox(\n",
    "                    label=\"ğŸ“ Problema\",\n",
    "                    placeholder=\"Ej: A car accelerates from rest at 3 m/sÂ² for 5 seconds. What is its final velocity?\",\n",
    "                    lines=3,\n",
    "                    scale=4\n",
    "                )\n",
    "                domain_selector = gr.Radio(\n",
    "                    choices=[\"math\", \"physics\"],\n",
    "                    value=\"math\",\n",
    "                    label=\"ğŸ”¬ Dominio\",\n",
    "                    scale=1\n",
    "                )\n",
    "\n",
    "            with gr.Row():\n",
    "                solve_btn = gr.Button(\"ğŸ¤” Resolver\", variant=\"primary\", scale=3)\n",
    "                clear_btn = gr.Button(\"ğŸ—‘ï¸ Limpiar\", variant=\"secondary\", scale=1)\n",
    "\n",
    "            output_box = gr.Textbox(\n",
    "                label=\"ğŸ¤– SoluciÃ³n (paso a paso)\",\n",
    "                lines=8,\n",
    "                show_copy_button=True\n",
    "            )\n",
    "\n",
    "            answer_box = gr.Markdown(\n",
    "                label=\"Respuesta\",\n",
    "                value=\"*Esperando problema...*\"\n",
    "            )\n",
    "\n",
    "            # Ejemplos por dominio\n",
    "            gr.Markdown(\"### ğŸ“š Ejemplos\")\n",
    "            with gr.Tab(\"MatemÃ¡ticas\"):\n",
    "                gr.Examples(\n",
    "                    examples=[\n",
    "                        [\"Janet has 3 apples and buys 5 more. How many apples does she have?\", \"math\"],\n",
    "                        [\"A store sells pencils for $2 each. Tom buys 7 pencils. How much does he spend?\", \"math\"],\n",
    "                        [\"Sarah has 24 cookies and shares equally among 6 friends. How many does each get?\", \"math\"],\n",
    "                        [\"A farmer has 15 chickens. Each lays 2 eggs per day. How many eggs in 3 days?\", \"math\"],\n",
    "                        [\"John has $50. He buys a book for $12 and a pen for $3. How much is left?\", \"math\"],\n",
    "                    ],\n",
    "                    inputs=[input_box, domain_selector],\n",
    "                    label=\"\"\n",
    "                )\n",
    "            with gr.Tab(\"FÃ­sica\"):\n",
    "                gr.Examples(\n",
    "                    examples=[\n",
    "                        [\"A car accelerates from rest at 3 m/sÂ² for 5 seconds. What is its final velocity?\", \"physics\"],\n",
    "                        [\"A 10 kg box is pushed with a force of 50 N. What is its acceleration?\", \"physics\"],\n",
    "                        [\"How much heat is needed to raise the temperature of 2 kg of water by 30Â°C?\", \"physics\"],\n",
    "                        [\"A circuit has voltage 12V and resistance 4Î©. What is the current?\", \"physics\"],\n",
    "                        [\"An object is dropped from 80 m. How long does it take to hit the ground?\", \"physics\"],\n",
    "                    ],\n",
    "                    inputs=[input_box, domain_selector],\n",
    "                    label=\"\"\n",
    "                )\n",
    "\n",
    "        # â•â•â• COLUMNA DERECHA: MÃ©tricas â•â•â•\n",
    "        with gr.Column(scale=2):\n",
    "\n",
    "            gr.Markdown(\"### ğŸ“Š MÃ©tricas de GeneraciÃ³n\")\n",
    "            with gr.Group():\n",
    "                conf_slider = gr.Slider(0, 1, value=0, label=\"Confianza\", interactive=False)\n",
    "                conf_text = gr.Textbox(label=\"Estado\", value=\"â³ Esperando...\", interactive=False)\n",
    "                perp_text = gr.Textbox(label=\"Perplexity\", value=\"--\", interactive=False)\n",
    "                time_text = gr.Textbox(label=\"Tiempo (ms)\", value=\"--\", interactive=False)\n",
    "                tokens_num = gr.Number(label=\"Tokens generados\", value=0)\n",
    "\n",
    "            gr.Markdown(\"### ğŸ§  Arquitectura\")\n",
    "            with gr.Row():\n",
    "                gr.Textbox(str(config_dict['num_layers']), label=\"Capas\", interactive=False)\n",
    "                gr.Textbox(str(config_dict['d_model']), label=\"d_model\", interactive=False)\n",
    "            with gr.Row():\n",
    "                gr.Textbox(str(config_dict['num_heads']), label=\"Heads\", interactive=False)\n",
    "                gr.Textbox(str(config_dict.get('dff', 1024)), label=\"dff\", interactive=False)\n",
    "            with gr.Row():\n",
    "                gr.Textbox(str(config_dict['dropout_rate']), label=\"Dropout\", interactive=False)\n",
    "                gr.Textbox(str(config_dict['vocab_size']), label=\"Vocab (chars)\", interactive=False)\n",
    "\n",
    "    # â”€â”€ AcordeÃ³n: Detalles TÃ©cnicos â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    with gr.Accordion(\"ğŸ” Detalles TÃ©cnicos\", open=False):\n",
    "        gr.Markdown(f\"\"\"\n",
    "**Componentes implementados desde cero (sin usar capas pre-hechas):**\n",
    "- Scaled Dot-Product Attention: `softmax(QK^T / âˆšd_k) V`\n",
    "- Multi-Head Attention ({config_dict['num_heads']} cabezas, depth={config_dict['d_model'] // config_dict['num_heads']})\n",
    "- Positional Encoding sinusoidal\n",
    "- Encoder: Self-Attn â†’ Add&Norm â†’ FFN â†’ Add&Norm (Ã—{config_dict['num_layers']})\n",
    "- Decoder: Masked Self-Attn â†’ Cross-Attn â†’ FFN (Ã—{config_dict['num_layers']})\n",
    "\n",
    "**Dataset combinado:**\n",
    "- GSM8K: ~8,600 problemas de aritmÃ©tica con soluciones paso a paso\n",
    "- MATH (LLM-solved): ~1,900 problemas de Ã¡lgebra, combinatoria, geometrÃ­a\n",
    "- FÃ­sica (templates): ~2,000 problemas de cinemÃ¡tica, dinÃ¡mica, termodinÃ¡mica, etc.\n",
    "- Total: ~12,500 problemas despuÃ©s de filtrado de calidad\n",
    "\n",
    "**TokenizaciÃ³n:** Character-level ({config_dict['vocab_size']} tokens: 131 ASCII + 4 especiales)\n",
    "\n",
    "**Entrenamiento:**\n",
    "- Optimizador: Adam (Î²â‚=0.9, Î²â‚‚=0.98) con LR warmup ({config_dict['warmup_steps']} pasos)\n",
    "- Loss: SparseCategoricalCrossentropy con label smoothing ({config_dict.get('label_smoothing', 0.1)})\n",
    "- Ã‰pocas: {len(history['train_loss']) if history else '?'} (early stopping patience=10)\n",
    "- GPU: NVIDIA RTX 5060 (Blackwell) con workarounds XLA\n",
    "\n",
    "**Inferencia:**\n",
    "- DecodificaciÃ³n: Top-k sampling (k=10, temperature=0.3)\n",
    "- Repetition penalty: 1.3 (penaliza tokens ya generados)\n",
    "- DetecciÃ³n de n-gram repeat para evitar bucles\n",
    "        \"\"\")\n",
    "\n",
    "    # â”€â”€ AcordeÃ³n: Curvas de Entrenamiento â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    if history:\n",
    "        with gr.Accordion(\"ğŸ“ˆ Curvas de Entrenamiento\", open=False):\n",
    "            plot_img = create_training_plot()\n",
    "            if plot_img:\n",
    "                gr.Image(value=plot_img, label=\"Historia de entrenamiento\")\n",
    "            gr.Markdown(f\"\"\"\n",
    "**Resumen del entrenamiento:**\n",
    "- Ã‰pocas completadas: {len(history['train_loss'])}\n",
    "- Loss final â€” Train: {history['train_loss'][-1]:.4f} | Val: {history.get('val_loss', [0])[-1]:.4f}\n",
    "- Accuracy final â€” Train: {history['train_accuracy'][-1]:.4f} | Val: {history.get('val_accuracy', [0])[-1]:.4f}\n",
    "- Mejor val_accuracy en Ã©poca: {np.argmax(history.get('val_accuracy', [0])) + 1 if history.get('val_accuracy') else 'N/A'}\n",
    "            \"\"\")\n",
    "\n",
    "    # â”€â”€ AcordeÃ³n: EvaluaciÃ³n â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    with gr.Accordion(\"ğŸ“‹ Resultados de EvaluaciÃ³n\", open=False):\n",
    "        if eval_report:\n",
    "            ta = eval_report.get('token_accuracy', {})\n",
    "            em = eval_report.get('exact_match', {})\n",
    "            eval_md = f\"\"\"\n",
    "**Token-level Accuracy** (predicciÃ³n del siguiente carÃ¡cter):\n",
    "- ValidaciÃ³n: {ta.get('val_acc', 0):.1%}\n",
    "- Test: {ta.get('test_acc', 0):.1%}\n",
    "\n",
    "**Exact Match (lÃ­nea Answer:)** sobre {em.get('total', 0)} problemas de test:\n",
    "- Global: **{em.get('correct', 0)}/{em.get('total', 0)} = {em.get('pct', 0):.1f}%**\n",
    "\"\"\"\n",
    "            for domain, stats in sorted(em.get('by_domain', {}).items()):\n",
    "                eval_md += f\"- {domain}: {stats['correct']}/{stats['total']} = {stats['correct']/max(stats['total'],1)*100:.1f}%\\n\"\n",
    "\n",
    "            eval_md += \"\\n**Ejemplos de predicciones:**\\n\"\n",
    "            for ex in eval_report.get('examples', [])[:6]:\n",
    "                icon = \"âœ…\" if ex['correct'] else \"âŒ\"\n",
    "                eval_md += f\"- {icon} [{ex['domain']}] \\\"{ex['problem'][:80]}...\\\" â†’ Esperado: `{ex['ref_answer']}` | Predicho: `{ex['pred_answer']}`\\n\"\n",
    "\n",
    "            gr.Markdown(eval_md)\n",
    "        else:\n",
    "            gr.Markdown(\"*No se encontrÃ³ `evaluation_report.json`. Ejecuta `evaluation/evaluate_math_physics.py`.*\")\n",
    "\n",
    "    # â”€â”€ AcordeÃ³n: Limitaciones â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    with gr.Accordion(\"âš ï¸ Limitaciones y AnÃ¡lisis Honesto\", open=True):\n",
    "        gr.Markdown(\"\"\"\n",
    "### Â¿QuÃ© puede y quÃ© NO puede este modelo?\n",
    "\n",
    "**âœ… Lo que el modelo SÃ demuestra:**\n",
    "1. **Arquitectura Transformer correcta**: Encoder-Decoder completo implementado desde cero\n",
    "2. **Aprendizaje de formato**: El modelo genera consistentemente el patrÃ³n \"Step 1: ... Step 2: ... Answer: ...\"\n",
    "3. **Pipeline completo de ML**: Datos â†’ tokenizaciÃ³n â†’ entrenamiento â†’ evaluaciÃ³n â†’ demo\n",
    "4. **Alta token accuracy (82%)**: Predice bien el siguiente carÃ¡cter dado el contexto\n",
    "\n",
    "**âŒ Lo que el modelo NO puede hacer:**\n",
    "1. **Razonamiento matemÃ¡tico**: Las respuestas numÃ©ricas son incorrectas (0% exact match)\n",
    "2. **AlineaciÃ³n problemaâ†’soluciÃ³n**: El modelo genera soluciones \"genÃ©ricas\" sin conectar con el problema especÃ­fico\n",
    "\n",
    "### Â¿Por quÃ© ocurre esto?\n",
    "\n",
    "El problema principal es el **tokenizer a nivel de carÃ¡cter** combinado con un modelo de **7M parÃ¡metros**:\n",
    "\n",
    "| Aspecto | Nuestro modelo | GPT-2 (referencia) |\n",
    "|---------|---------------|-------------------|\n",
    "| TokenizaciÃ³n | Character-level (135 tokens) | BPE (~50,000 tokens) |\n",
    "| ParÃ¡metros | 7.4M | 117M (small) |\n",
    "| RepresentaciÃ³n de \"25\" | 2 tokens: `'2'`, `'5'` | 1 token: `'25'` |\n",
    "| RepresentaciÃ³n de \"Step\" | 4 tokens: `'S'`, `'t'`, `'e'`, `'p'` | 1 token: `'Step'` |\n",
    "\n",
    "Con tokenizaciÃ³n por carÃ¡cter, un problema de 100 palabras se convierte en **~500 tokens**.\n",
    "El modelo debe \"recordar\" relaciones entre caracteres individuales a lo largo de secuencias\n",
    "de 200-300 pasos â€” una tarea extremadamente difÃ­cil para 7M parÃ¡metros.\n",
    "\n",
    "### Â¿QuÃ© se necesitarÃ­a para mejorar?\n",
    "\n",
    "1. **Tokenizer BPE/WordPiece**: Reducir secuencias ~5x, permitiendo al modelo operar a nivel de concepto\n",
    "2. **MÃ¡s parÃ¡metros**: 50M+ para capturar relaciones semÃ¡nticas\n",
    "3. **Pre-entrenamiento**: El modelo necesita comprensiÃ³n lingÃ¼Ã­stica previa (como BERT/GPT)\n",
    "4. **Fine-tuning supervisado con chain-of-thought**: Ejemplos de razonamiento paso a paso alineados\n",
    "\n",
    "### Valor pedagÃ³gico\n",
    "\n",
    "A pesar de las limitaciones en razonamiento, este proyecto demuestra:\n",
    "- ImplementaciÃ³n from-scratch de un Transformer completo\n",
    "- Pipeline de datos robusto (schema, validaciÃ³n, filtrado)\n",
    "- Entrenamiento con GPU y tÃ©cnicas modernas (label smoothing, LR scheduling, early stopping)\n",
    "- EvaluaciÃ³n honesta con mÃ©tricas apropiadas\n",
    "- Despliegue con interfaz interactiva\n",
    "        \"\"\")\n",
    "\n",
    "    # â”€â”€ Eventos â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    solve_btn.click(\n",
    "        fn=tutor_interface,\n",
    "        inputs=[input_box, domain_selector],\n",
    "        outputs=[output_box, answer_box, conf_slider, conf_text, perp_text, time_text, tokens_num]\n",
    "    )\n",
    "\n",
    "    clear_btn.click(\n",
    "        fn=lambda: (\"\", \"math\", \"\", \"*Esperando problema...*\", 0, \"â³ Esperando...\", \"--\", \"--\", 0),\n",
    "        inputs=None,\n",
    "        outputs=[input_box, domain_selector, output_box, answer_box, conf_slider, conf_text, perp_text, time_text, tokens_num]\n",
    "    )\n",
    "\n",
    "# Lanzar\n",
    "demo.launch(inline=True, share=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
