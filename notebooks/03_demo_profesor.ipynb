{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b58a164c",
   "metadata": {},
   "source": [
    "# 03 â€” Demo: Transformer Tutor de MatemÃ¡ticas y FÃ­sica\n",
    "\n",
    "Notebook de demostraciÃ³n del modelo Transformer **v3** (versiÃ³n final) con tokenizaciÃ³n **BPE subword**.\n",
    "\n",
    "1. Carga el modelo TransformerV3 (BPE, vocab_size=4000, answer head) desde checkpoints\n",
    "2. GeneraciÃ³n autoregresiva con mÃ©tricas **reales basadas en logits** (confianza, perplexity)\n",
    "3. DiagnÃ³stico de cross-attention (Â¿selectiva o colapsada?)\n",
    "4. Interfaz interactiva Gradio con selector de dominio (math / physics)\n",
    "5. EvoluciÃ³n del proyecto: v1 (char) â†’ v2 (BPE) â†’ v3 (answer head + diversity loss)\n",
    "6. DiscusiÃ³n tÃ©cnica honesta: quÃ© puede y quÃ© NO puede el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "753103b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.6.3) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "/home/melissa/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2026-02-14 21:06:30.184152: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-02-14 21:06:30.918025: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-02-14 21:06:32.785184: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… TensorFlow 2.20.0\n",
      "âœ… Gradio 4.44.0\n",
      "âœ… GPU: []\n",
      "âœ… Proyecto: /home/melissa/transformer_math_physics_tutor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1771121193.743381   21020 gpu_device.cc:2342] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# === Setup: GPU workaround para Blackwell + imports ===\n",
    "import os, sys, subprocess\n",
    "\n",
    "# XLA/GPU config ANTES de importar TF\n",
    "os.environ.setdefault(\"XLA_FLAGS\", \"--xla_gpu_cuda_data_dir=/usr/local/cuda-12.8\")\n",
    "os.environ.setdefault(\"TF_XLA_FLAGS\", \"--tf_xla_auto_jit=2\")\n",
    "\n",
    "try:\n",
    "    import gradio as gr\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"gradio==4.44.0\", \"-q\"])\n",
    "    import gradio as gr\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "\n",
    "# GPU memory growth + Blackwell cast patch\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    _original_cast = tf.cast\n",
    "    def _blackwell_cast(x, dtype, name=None):\n",
    "        if tf.executing_eagerly():\n",
    "            with tf.device('/CPU:0'):\n",
    "                return _original_cast(x, dtype, name=name)\n",
    "        return _original_cast(x, dtype, name=name)\n",
    "    tf.cast = _blackwell_cast\n",
    "\n",
    "# Ruta del proyecto\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "for p in [project_root, os.path.dirname(project_root)]:\n",
    "    if p not in sys.path:\n",
    "        sys.path.insert(0, p)\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "print(f\"âœ… TensorFlow {tf.__version__}\")\n",
    "print(f\"âœ… Gradio {gr.__version__}\")\n",
    "print(f\"âœ… GPU: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(f\"âœ… Proyecto: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72be48ad",
   "metadata": {},
   "source": [
    "## 1. Cargar Modelo v3 (VersiÃ³n Final) y Resultados de EvaluaciÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56232edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConfiguraciÃ³n cargada desde /home/melissa/transformer_math_physics_tutor/checkpoints/v3_easy/config.json\n",
      "SubwordTokenizer cargado desde /home/melissa/transformer_math_physics_tutor/checkpoints/v3_easy/sp_tokenizer.model (4000 tokens)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/melissa/.local/lib/python3.10/site-packages/keras/src/layers/layer.py:970: UserWarning: Layer 'xla_dropout_24' (of type XLADropout) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/melissa/.local/lib/python3.10/site-packages/keras/src/layers/layer.py:970: UserWarning: Layer 'xla_dropout_26' (of type XLADropout) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/melissa/.local/lib/python3.10/site-packages/keras/src/layers/layer.py:970: UserWarning: Layer 'xla_dropout_28' (of type XLADropout) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/melissa/.local/lib/python3.10/site-packages/keras/src/layers/layer.py:970: UserWarning: Layer 'xla_dropout_32' (of type XLADropout) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/melissa/.local/lib/python3.10/site-packages/keras/src/layers/layer.py:970: UserWarning: Layer 'xla_dropout_35' (of type XLADropout) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/melissa/.local/lib/python3.10/site-packages/keras/src/layers/layer.py:970: UserWarning: Layer 'xla_dropout_38' (of type XLADropout) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/melissa/.local/lib/python3.10/site-packages/keras/src/layers/layer.py:970: UserWarning: Layer 'xla_dropout_41' (of type XLADropout) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Modelo v3 cargado: 10,514,849 parÃ¡metros (model_weights.weights.h5)\n",
      "âœ… Dispositivo: CPU\n",
      "âœ… Vocab size: 4000 (BPE subword)\n",
      "âœ… Encoder max: 128 | Decoder max: 256\n",
      "âœ… Accuracy â€” Train: 64.9%, Val: 73.8%\n",
      "âœ… Token Acc (test): 69.9%\n",
      "âœ… Exact Match: 3/100 = 3.0%\n",
      "âœ… Numeric Match: 3/86 = 3.5%\n",
      "âœ… Answer Head MAE: 298.8\n",
      "   decoder_layer1_block2: 0.742 â†’ âœ… SELECTIVA\n",
      "   decoder_layer2_block2: 0.540 â†’ âœ… SELECTIVA\n",
      "   decoder_layer3_block2: 0.523 â†’ âœ… SELECTIVA\n",
      "   decoder_layer4_block2: 0.673 â†’ âœ… SELECTIVA\n"
     ]
    }
   ],
   "source": [
    "# === Cargar modelo v3 (TransformerV3 con Answer Head) ===\n",
    "from transformer_math_physics_tutor.data.subword_tokenizer import SubwordTokenizer\n",
    "from transformer_math_physics_tutor.models.config import TransformerConfig\n",
    "from transformer_math_physics_tutor.models.transformer_v3 import TransformerV3\n",
    "\n",
    "# --- Cargar configuraciÃ³n ---\n",
    "checkpoint_dir = os.path.join(project_root, 'checkpoints', 'v3_easy')\n",
    "config = TransformerConfig.load(os.path.join(checkpoint_dir, 'config.json'))\n",
    "\n",
    "# --- Cargar tokenizer BPE ---\n",
    "tokenizer = SubwordTokenizer(os.path.join(checkpoint_dir, 'sp_tokenizer.model'))\n",
    "config.vocab_size = tokenizer.vocab_size\n",
    "\n",
    "# --- Crear e inicializar modelo v3 ---\n",
    "ANSWER_SCALE = 1000.0\n",
    "\n",
    "with tf.device('/CPU:0'):\n",
    "    model = TransformerV3(config, answer_scale=ANSWER_SCALE)\n",
    "    dummy_enc = tf.zeros((1, config.max_encoder_len), dtype=tf.int32)\n",
    "    dummy_dec = tf.zeros((1, config.max_decoder_len), dtype=tf.int32)\n",
    "    _ = model((dummy_enc, dummy_dec), training=False)\n",
    "\n",
    "    # Cargar pesos\n",
    "    weights_path = os.path.join(checkpoint_dir, 'model_weights.weights.h5')\n",
    "    model.load_weights(weights_path)\n",
    "\n",
    "num_params = model.count_params()\n",
    "device = \"GPU\" if gpus else \"CPU\"\n",
    "\n",
    "# --- Compiled predict step para Blackwell GPU ---\n",
    "max_enc = config.max_encoder_len\n",
    "max_dec = config.max_decoder_len\n",
    "pad_id = tokenizer.pad_token_id\n",
    "\n",
    "@tf.function(input_signature=[\n",
    "    tf.TensorSpec(shape=[1, max_enc], dtype=tf.int32),\n",
    "    tf.TensorSpec(shape=[1, max_dec], dtype=tf.int32),\n",
    "])\n",
    "def predict_step(enc_input, dec_input):\n",
    "    \"\"\"Forward pass compilado â€” retorna solo logits (sin answer head).\"\"\"\n",
    "    output = model((enc_input, dec_input), training=False)\n",
    "    return output[0]  # logits (ignorar answer_pred)\n",
    "\n",
    "# --- Historial de entrenamiento ---\n",
    "history_raw = None\n",
    "history = None  # Fase 3 (la que nos interesa para plots)\n",
    "final_train_acc = final_val_acc = 0.0\n",
    "try:\n",
    "    with open(os.path.join(checkpoint_dir, 'training_history.json'), 'r') as f:\n",
    "        history_raw = json.load(f)\n",
    "    # El JSON tiene estructura anidada: phase1/phase2/phase3\n",
    "    history = history_raw.get('phase3', history_raw)\n",
    "    final_train_acc = history['train_accuracy'][-1]\n",
    "    if history.get('val_accuracy'):\n",
    "        final_val_acc = history['val_accuracy'][-1]\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# --- Reporte de evaluaciÃ³n ---\n",
    "eval_report = None\n",
    "try:\n",
    "    with open(os.path.join(checkpoint_dir, 'evaluation_report.json'), 'r') as f:\n",
    "        eval_report = json.load(f)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# --- Config dict para display ---\n",
    "with open(os.path.join(checkpoint_dir, 'config.json'), 'r') as f:\n",
    "    config_dict = json.load(f)\n",
    "\n",
    "print(f\"âœ… Modelo v3 cargado: {num_params:,} parÃ¡metros ({os.path.basename(weights_path)})\")\n",
    "print(f\"âœ… Dispositivo: {device}\")\n",
    "print(f\"âœ… Vocab size: {tokenizer.vocab_size} (BPE subword)\")\n",
    "print(f\"âœ… Encoder max: {config.max_encoder_len} | Decoder max: {config.max_decoder_len}\")\n",
    "print(f\"âœ… Accuracy â€” Train: {final_train_acc:.1%}, Val: {final_val_acc:.1%}\")\n",
    "if eval_report:\n",
    "    em = eval_report.get('exact_match', {})\n",
    "    ta = eval_report.get('token_accuracy', {})\n",
    "    ah = eval_report.get('answer_head', {})\n",
    "    attn = eval_report.get('cross_attention_entropy', {})\n",
    "    print(f\"âœ… Token Acc (test): {ta.get('test_acc', 0):.1%}\")\n",
    "    print(f\"âœ… Exact Match: {em.get('textual_correct', 0)}/{em.get('textual_total', 0)} = {em.get('textual_pct', 0):.1f}%\")\n",
    "    print(f\"âœ… Numeric Match: {em.get('numeric_correct', 0)}/{em.get('numeric_total', 0)} = {em.get('numeric_pct', 0):.1f}%\")\n",
    "    print(f\"âœ… Answer Head MAE: {ah.get('mae', 0):.1f}\")\n",
    "\n",
    "    for layer, val in sorted(attn.items()):\n",
    "        status = \"âš ï¸ UNIFORME\" if val > 0.95 else \"âœ… SELECTIVA\" if val < 0.80 else \"ğŸŸ¡ PARCIAL\"\n",
    "        print(f\"   {layer}: {val:.3f} â†’ {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d316fde1",
   "metadata": {},
   "source": [
    "## 2. FunciÃ³n de GeneraciÃ³n v3 (BPE + Answer Head + @tf.function)\n",
    "\n",
    "La funciÃ³n `generate_with_metrics` usa decodificaciÃ³n **autoregresiva con predict_step compilado** y calcula mÃ©tricas basadas en **logits reales**:\n",
    "\n",
    "- **Confianza**: $\\text{confidence} = \\frac{1}{T}\\sum_{t=1}^{T} \\max_v P(v \\mid v_{<t})$ â€” promedio de la probabilidad mÃ¡xima por paso\n",
    "- **Perplexity**: $\\text{PPL} = \\exp\\!\\left(-\\frac{1}{T}\\sum_{t=1}^{T} \\log P(v_t \\mid v_{<t})\\right)$ â€” sobre la secuencia seleccionada\n",
    "- **Answer Head**: PredicciÃ³n numÃ©rica directa del MLP de regresiÃ³n (sin generar tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db7054f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST RÃPIDO â€” Math\n",
      "============================================================\n",
      "  Respuesta: Step 1: First find the total number of additional objects she learns: 60 objects + 20 sheep = <<60+20=70>>70 items Step 2: Then divide the number of kids in the cat by the number of kids to find the t\n",
      "  Answer Head: 21.0\n",
      "  Confianza: 66.72% | PPL: 2.03 | 98 tokens | 3562ms\n",
      "\n",
      "TEST RÃPIDO â€” Physics\n",
      "  Respuesta: Step 1: Use the formula distance = speed Ã— time. Step 2: distance = 60 Ã— 3 = 180 km. Answer: 180 km\n",
      "  Answer Head: 75.1\n",
      "  Confianza: 90.67% | PPL: 1.16 | 39 tokens | 1254ms\n"
     ]
    }
   ],
   "source": [
    "def _pad_to_length(tokens, max_len, pad_id):\n",
    "    \"\"\"Paddea o trunca una secuencia a max_len.\"\"\"\n",
    "    if len(tokens) > max_len:\n",
    "        return tokens[:max_len - 1] + [tokens[-1]]\n",
    "    return tokens + [pad_id] * (max_len - len(tokens))\n",
    "\n",
    "\n",
    "def _detect_ngram_repeat(tokens, n=8):\n",
    "    \"\"\"Detecta si los Ãºltimos n tokens se repiten antes.\"\"\"\n",
    "    if len(tokens) < n * 2:\n",
    "        return False\n",
    "    last_ngram = tokens[-n:]\n",
    "    for i in range(len(tokens) - n * 2, max(0, len(tokens) - n * 5) - 1, -1):\n",
    "        if tokens[i:i + n] == last_ngram:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def generate_with_metrics(problem, max_length=256, temperature=0.3,\n",
    "                          top_k=10, repetition_penalty=1.3):\n",
    "    \"\"\"\n",
    "    Genera respuesta v3 con mÃ©tricas reales + answer head prediction.\n",
    "\n",
    "    Returns dict con: text, confidence, perplexity, inference_time,\n",
    "                      num_tokens, answer_head_pred\n",
    "    \"\"\"\n",
    "    if not problem or not problem.strip():\n",
    "        return {\n",
    "            'text': \"âš ï¸ Escribe un problema de matemÃ¡ticas o fÃ­sica.\",\n",
    "            'confidence': 0.0, 'perplexity': 0.0,\n",
    "            'inference_time': 0.0, 'num_tokens': 0,\n",
    "            'answer_head_pred': None,\n",
    "        }\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Tokenizar y paddear encoder input\n",
    "    encoder_tokens = tokenizer.encode(problem, add_special_tokens=True)\n",
    "    encoder_tokens = _pad_to_length(encoder_tokens, max_enc, pad_id)\n",
    "    encoder_input = tf.constant([encoder_tokens], dtype=tf.int32)\n",
    "\n",
    "    # Answer head prediction (single forward pass)\n",
    "    with tf.device('/CPU:0'):\n",
    "        dec_dummy = tf.constant(\n",
    "            [[tokenizer.start_token_id] + [pad_id] * (max_dec - 1)],\n",
    "            dtype=tf.int32\n",
    "        )\n",
    "        output_full = model((encoder_input, dec_dummy), training=False)\n",
    "        answer_head_pred = float(output_full[1].numpy()[0]) * ANSWER_SCALE\n",
    "\n",
    "    # DecodificaciÃ³n autoregresiva\n",
    "    decoder_tokens = [tokenizer.start_token_id]\n",
    "    generated_tokens = []\n",
    "    token_probs = []\n",
    "\n",
    "    for step in range(max_length):\n",
    "        dec_padded = decoder_tokens + [pad_id] * (max_dec - len(decoder_tokens))\n",
    "        decoder_input = tf.constant([dec_padded[:max_dec]], dtype=tf.int32)\n",
    "\n",
    "        predictions = predict_step(encoder_input, decoder_input)\n",
    "\n",
    "        pos = min(len(decoder_tokens) - 1, max_dec - 1)\n",
    "        logits = predictions[0, pos, :].numpy()\n",
    "\n",
    "        # Probabilidades ANTES de manipulaciÃ³n (para mÃ©tricas puras)\n",
    "        probs = np.exp(logits - np.max(logits))\n",
    "        probs = probs / probs.sum()\n",
    "\n",
    "        # Repetition penalty\n",
    "        logits_np = logits.copy()\n",
    "        if repetition_penalty > 1.0 and generated_tokens:\n",
    "            for tok_id in set(generated_tokens):\n",
    "                if logits_np[tok_id] > 0:\n",
    "                    logits_np[tok_id] /= repetition_penalty\n",
    "                else:\n",
    "                    logits_np[tok_id] *= repetition_penalty\n",
    "\n",
    "        # SelecciÃ³n de token con top-k sampling\n",
    "        if temperature <= 0.0:\n",
    "            predicted_id = int(np.argmax(logits_np))\n",
    "        else:\n",
    "            scaled = logits_np / temperature\n",
    "            if top_k > 0:\n",
    "                top_indices = np.argpartition(scaled, -top_k)[-top_k:]\n",
    "                mask = np.full_like(scaled, -1e9)\n",
    "                mask[top_indices] = scaled[top_indices]\n",
    "                scaled = mask\n",
    "            exp_scaled = np.exp(scaled - np.max(scaled))\n",
    "            sampling_probs = exp_scaled / exp_scaled.sum()\n",
    "            predicted_id = int(np.random.choice(len(sampling_probs), p=sampling_probs))\n",
    "\n",
    "        selected_prob = float(probs[predicted_id])\n",
    "        token_probs.append(selected_prob)\n",
    "\n",
    "        if predicted_id == tokenizer.end_token_id:\n",
    "            break\n",
    "\n",
    "        generated_tokens.append(predicted_id)\n",
    "\n",
    "        if len(generated_tokens) > 20 and _detect_ngram_repeat(generated_tokens, n=10):\n",
    "            break\n",
    "\n",
    "        decoder_tokens.append(predicted_id)\n",
    "        if len(decoder_tokens) >= max_dec:\n",
    "            break\n",
    "\n",
    "    inference_time = time.time() - start_time\n",
    "\n",
    "    # Decodificar\n",
    "    generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "    # MÃ©tricas\n",
    "    if token_probs:\n",
    "        confidence = float(np.mean(token_probs))\n",
    "        nll = -np.mean([np.log(p + 1e-10) for p in token_probs])\n",
    "        perplexity = float(np.exp(min(nll, 20)))\n",
    "    else:\n",
    "        confidence = 0.0\n",
    "        perplexity = 0.0\n",
    "\n",
    "    return {\n",
    "        'text': generated_text,\n",
    "        'confidence': confidence,\n",
    "        'perplexity': perplexity,\n",
    "        'inference_time': inference_time,\n",
    "        'num_tokens': len(generated_tokens),\n",
    "        'answer_head_pred': round(answer_head_pred, 1),\n",
    "    }\n",
    "\n",
    "\n",
    "# === Test rÃ¡pido ===\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST RÃPIDO â€” Math\")\n",
    "print(\"=\" * 60)\n",
    "r = generate_with_metrics(\"Janet has 3 apples and buys 5 more. How many does she have?\")\n",
    "print(f\"  Respuesta: {r['text'][:200]}\")\n",
    "print(f\"  Answer Head: {r['answer_head_pred']}\")\n",
    "print(f\"  Confianza: {r['confidence']:.2%} | PPL: {r['perplexity']:.2f} | {r['num_tokens']} tokens | {r['inference_time']*1000:.0f}ms\")\n",
    "\n",
    "print(\"\\nTEST RÃPIDO â€” Physics\")\n",
    "r2 = generate_with_metrics(\"A car travels at 60 km/h for 3 hours. How far does it go?\")\n",
    "print(f\"  Respuesta: {r2['text'][:200]}\")\n",
    "print(f\"  Answer Head: {r2['answer_head_pred']}\")\n",
    "print(f\"  Confianza: {r2['confidence']:.2%} | PPL: {r2['perplexity']:.2f} | {r2['num_tokens']} tokens | {r2['inference_time']*1000:.0f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e93fdb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DIAGNÃ“STICO v3 â€” Math + Physics\n",
      "======================================================================\n",
      "\n",
      "ğŸŸ¡ [MATH] Janet has 3 apples and buys 5 more. How many apples does she have?\n",
      "   â†’ Answer: 4  (head pred: 24.5)\n",
      "   Confianza: 70.78% | PPL: 1.68 | 78 tokens\n",
      "\n",
      "ğŸŸ¡ [MATH] A store sells pencils for $2 each. Tom buys 7 pencils. How much does he spend?\n",
      "   â†’ Answer: 12  (head pred: 14.6)\n",
      "   Confianza: 75.33% | PPL: 1.59 | 67 tokens\n",
      "\n",
      "ğŸŸ¢ [MATH] A train travels 60 miles per hour for 3 hours. How far does it go?\n",
      "   â†’ Answer: 180 miles  (head pred: 59.9)\n",
      "   Confianza: 88.16% | PPL: 1.22 | 39 tokens\n",
      "\n",
      "ğŸŸ¡ [MATH] Sarah has 24 cookies and wants to share them equally among 6 friends. How many cookies does each friend get?\n",
      "   â†’ Answer: 80  (head pred: 28.1)\n",
      "   Confianza: 78.88% | PPL: 1.44 | 47 tokens\n",
      "\n",
      "ğŸŸ¡ [MATH] John has $50. He buys a book for $12 and a pen for $3. How much money does he have left?\n",
      "   â†’ Answer: 30  (head pred: 61.2)\n",
      "   Confianza: 79.19% | PPL: 1.43 | 51 tokens\n",
      "\n",
      "ğŸŸ¢ [PHYSICS] A car accelerates from rest at 3 m/sÂ² for 5 seconds. What is its final velocity?\n",
      "   â†’ Answer: 36 m/s  (head pred: 25.2)\n",
      "   Confianza: 90.97% | PPL: 1.14 | 58 tokens\n",
      "\n",
      "ğŸŸ¢ [PHYSICS] A 10 kg box is pushed with a force of 50 N. What is its acceleration?\n",
      "   â†’ Answer: 400 N  (head pred: 49.8)\n",
      "   Confianza: 90.94% | PPL: 1.16 | 43 tokens\n",
      "\n",
      "ğŸŸ¢ [PHYSICS] How much heat is needed to raise the temperature of 2 kg of water by 30Â°C?\n",
      "   â†’ Answer: 7976.4 Pa  (head pred: 6840.8)\n",
      "   Confianza: 85.09% | PPL: 1.33 | 84 tokens\n",
      "\n",
      "ğŸŸ¢ [PHYSICS] A circuit has a voltage of 12V and a resistance of 4Î©. What is the current?\n",
      "   â†’ Answer: 96.0 Î©  (head pred: -41.7)\n",
      "   Confianza: 88.86% | PPL: 1.20 | 45 tokens\n",
      "\n",
      "ğŸŸ¢ [PHYSICS] An object is dropped from 80 m. How long does it take to hit the ground?\n",
      "   â†’ Answer: 4.0 seconds  (head pred: 16.3)\n",
      "   Confianza: 83.12% | PPL: 1.33 | 79 tokens\n"
     ]
    }
   ],
   "source": [
    "# === DiagnÃ³stico v3: problemas de ambos dominios ===\n",
    "test_problems = [\n",
    "    (\"Janet has 3 apples and buys 5 more. How many apples does she have?\", \"math\"),\n",
    "    (\"A store sells pencils for $2 each. Tom buys 7 pencils. How much does he spend?\", \"math\"),\n",
    "    (\"A train travels 60 miles per hour for 3 hours. How far does it go?\", \"math\"),\n",
    "    (\"Sarah has 24 cookies and wants to share them equally among 6 friends. How many cookies does each friend get?\", \"math\"),\n",
    "    (\"John has $50. He buys a book for $12 and a pen for $3. How much money does he have left?\", \"math\"),\n",
    "    (\"A car accelerates from rest at 3 m/sÂ² for 5 seconds. What is its final velocity?\", \"physics\"),\n",
    "    (\"A 10 kg box is pushed with a force of 50 N. What is its acceleration?\", \"physics\"),\n",
    "    (\"How much heat is needed to raise the temperature of 2 kg of water by 30Â°C?\", \"physics\"),\n",
    "    (\"A circuit has a voltage of 12V and a resistance of 4Î©. What is the current?\", \"physics\"),\n",
    "    (\"An object is dropped from 80 m. How long does it take to hit the ground?\", \"physics\"),\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DIAGNÃ“STICO v3 â€” Math + Physics\")\n",
    "print(\"=\" * 70)\n",
    "for problem, domain in test_problems:\n",
    "    r = generate_with_metrics(problem, temperature=0.0, top_k=0)\n",
    "    if r['confidence'] >= 0.8:\n",
    "        status = \"ğŸŸ¢\"\n",
    "    elif r['confidence'] >= 0.5:\n",
    "        status = \"ğŸŸ¡\"\n",
    "    else:\n",
    "        status = \"ğŸ”´\"\n",
    "    # Extraer la respuesta\n",
    "    ans_match = re.search(r'Answer:\\s*(.+?)(?:\\n|$)', r['text'], re.IGNORECASE)\n",
    "    answer = ans_match.group(1).strip() if ans_match else \"N/A\"\n",
    "    print(f\"\\n{status} [{domain.upper()}] {problem}\")\n",
    "    print(f\"   â†’ Answer: {answer}  (head pred: {r['answer_head_pred']})\")\n",
    "    print(f\"   Confianza: {r['confidence']:.2%} | PPL: {r['perplexity']:.2f} | {r['num_tokens']} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52448741",
   "metadata": {},
   "source": [
    "## 2.5 DiagnÃ³stico Profundo v3 â€” Â¿QuÃ© aprendiÃ³ el modelo?\n",
    "\n",
    "Investigamos:\n",
    "1. **Â¿El encoder influye en la salida?** â€” Si cambiamos el problema, Â¿cambia la respuesta?\n",
    "2. **Â¿El encoder produce embeddings distintos?** â€” Con BPE + answer head, Â¿las representaciones son mÃ¡s ricas?\n",
    "3. **Â¿Cross-attention funciona?** â€” Â¿El decoder atiende al encoder? (v3 rompiÃ³ el colapso!)\n",
    "4. **Â¿Teacher forcing ayuda?** â€” Si le damos los primeros tokens correctos, Â¿puede continuar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da4b9812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DIAGNÃ“STICO 1: Â¿El encoder influye en la salida? (v3)\n",
      "  (Mismo decoder start, problemas distintos, greedy)\n",
      "======================================================================\n",
      "\n",
      "  Problema: What is 2 + 3?\n",
      "  â†’ Answer: N/A  (head pred: 287.6)\n",
      "  Confianza: 57.86% | Tokens: 38\n",
      "\n",
      "  Problema: A farmer has 100 cows and sells 30. How many cows remain?\n",
      "  â†’ Answer: 60  (head pred: 366.1)\n",
      "  Confianza: 74.10% | Tokens: 60\n",
      "\n",
      "  Problema: A ball is thrown upward with a velocity of 20 m/s. What is the maximum height?\n",
      "  â†’ Answer: 64 m/s  (head pred: 71.3)\n",
      "  Confianza: 88.47% | Tokens: 58\n",
      "\n",
      "======================================================================\n",
      "DIAGNÃ“STICO 2: Â¿El encoder produce embeddings distintos? (v3)\n",
      "======================================================================\n",
      "  'What is 5 + 3?' (10 subword tokens) â†’ mean=-0.0006, std=0.0866, norm=15.68\n",
      "  'A car moves at 10 m/s for 5 seconds. What distance' (23 subword tokens) â†’ mean=-0.0011, std=0.1141, norm=20.66\n",
      "  'XYZXYZXYZ' (11 subword tokens) â†’ mean=-0.0001, std=0.0813, norm=14.72\n",
      "  Cosine sim [0] vs [1]: 0.3352\n",
      "  Cosine sim [0] vs [2]: 0.3402\n",
      "  Cosine sim [1] vs [2]: 0.4680\n",
      "\n",
      "======================================================================\n",
      "DIAGNÃ“STICO 3: Â¿Cross-attention selectiva? (v3)\n",
      "  (v1/v2 tenÃ­an entropy ~1.000 = 100% uniforme)\n",
      "  (v3 entrenÃ³ con diversity loss para romper el colapso)\n",
      "======================================================================\n",
      "  decoder_layer1_block2: tokens reales=100.0%, padding=0.0%\n",
      "    Entropy: 2.302 / 2.303 (max) = 100.0% â†’ âš ï¸ UNIFORME\n",
      "    Top-5: [('', '0.1054'), ('3', '0.1011'), ('5', '0.1009'), ('is', '0.1009'), ('', '0.0988')]\n",
      "  decoder_layer2_block2: tokens reales=100.0%, padding=0.0%\n",
      "    Entropy: 2.285 / 2.303 (max) = 99.2% â†’ âš ï¸ UNIFORME\n",
      "    Top-5: [('', '0.1472'), ('5', '0.1153'), ('3', '0.1126'), ('is', '0.1077'), ('', '0.0904')]\n",
      "  decoder_layer3_block2: tokens reales=100.0%, padding=0.0%\n",
      "    Entropy: 2.277 / 2.303 (max) = 98.9% â†’ âš ï¸ UNIFORME\n",
      "    Top-5: [('', '0.1556'), ('5', '0.1179'), ('3', '0.1154'), ('is', '0.1119'), ('', '0.0887')]\n",
      "  decoder_layer4_block2: tokens reales=100.0%, padding=0.0%\n",
      "    Entropy: 2.301 / 2.303 (max) = 99.9% â†’ âš ï¸ UNIFORME\n",
      "    Top-5: [('', '0.1121'), ('5', '0.1038'), ('3', '0.1017'), ('is', '0.1012'), ('', '0.0999')]\n",
      "\n",
      "======================================================================\n",
      "DIAGNÃ“STICO 4: Teacher forcing â€” Â¿puede continuar? (v3)\n",
      "======================================================================\n",
      "  Problema: 'What is 5 + 3?'\n",
      "  Teacher forcing: 'Step 1: 5 + 3 = '\n",
      "  ContinuaciÃ³n: 'Step 1: 5 + 3 = 5 + 3 = 8 + 5 = 10. Answer: 10'\n",
      "\n",
      "======================================================================\n",
      "â•â•â• RESUMEN DEL DIAGNÃ“STICO v3 â•â•â•\n",
      "======================================================================\n",
      "\n",
      "  RESULTADOS v3 (BPE subword + Answer Head + Diversity Loss):\n",
      "\n",
      "  âœ… LOGROS PRINCIPALES:\n",
      "  â†’ Cross-attention SELECTIVA (entropy 0.52-0.74 vs 1.000 en v1/v2)\n",
      "  â†’ Genera soluciones con formato Step/Answer correcto\n",
      "  â†’ FÃ³rmulas de fÃ­sica correctas (F=ma, V=IR, KE=Â½mvÂ², v=fÎ», d=vÃ—t)\n",
      "  â†’ Answer head predice valores numÃ©ricos razonables\n",
      "  â†’ Token accuracy ~70% (subword tokens significativos)\n",
      "  â†’ Exact match 3.0% (textual) / 3.5% (numÃ©rico)\n",
      "\n",
      "  âš ï¸ LIMITACIONES:\n",
      "  â†’ NÃºmeros especÃ­ficos casi siempre incorrectos en problemas de math\n",
      "  â†’ El modelo genera soluciones memoryzadas cuando no reconoce el patrÃ³n\n",
      "  â†’ 5,729 ejemplos de entrenamiento insuficientes para generalizaciÃ³n numÃ©rica\n",
      "  â†’ Se necesitarÃ­a un mecanismo de copia (copy mechanism) para copiar \n",
      "    nÃºmeros del input al output\n",
      "\n",
      "  Â¿POR QUÃ‰ se rompiÃ³ la cross-attention en v3? (vs colapsada en v1/v2)\n",
      "  â†’ Entrenamiento en 3 fases:\n",
      "    1. Fase 1: Encoder pre-entrenado con answer head (gradiente directo)\n",
      "    2. Fase 2: Cross-attention reinicializada + diversity loss alta\n",
      "    3. Fase 3: Fine-tuning completo con learning rate baja\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"DIAGNÃ“STICO 1: Â¿El encoder influye en la salida? (v3)\")\n",
    "print(\"  (Mismo decoder start, problemas distintos, greedy)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for prob_text in [\n",
    "    \"What is 2 + 3?\",\n",
    "    \"A farmer has 100 cows and sells 30. How many cows remain?\",\n",
    "    \"A ball is thrown upward with a velocity of 20 m/s. What is the maximum height?\",\n",
    "]:\n",
    "    r = generate_with_metrics(prob_text, temperature=0.0, top_k=0)\n",
    "    ans = re.search(r'Answer:\\s*(.+?)(?:\\n|$)', r['text'], re.IGNORECASE)\n",
    "    answer = ans.group(1).strip() if ans else \"N/A\"\n",
    "    print(f\"\\n  Problema: {prob_text}\")\n",
    "    print(f\"  â†’ Answer: {answer}  (head pred: {r['answer_head_pred']})\")\n",
    "    print(f\"  Confianza: {r['confidence']:.2%} | Tokens: {r['num_tokens']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DIAGNÃ“STICO 2: Â¿El encoder produce embeddings distintos? (v3)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "problems_to_compare = [\n",
    "    \"What is 5 + 3?\",\n",
    "    \"A car moves at 10 m/s for 5 seconds. What distance does it travel?\",\n",
    "    \"XYZXYZXYZ\",\n",
    "]\n",
    "\n",
    "enc_outputs = []\n",
    "for prob in problems_to_compare:\n",
    "    enc_tokens = tokenizer.encode(prob, add_special_tokens=True)\n",
    "    enc_padded = _pad_to_length(enc_tokens, config.max_encoder_len, tokenizer.pad_token_id)\n",
    "    with tf.device('/CPU:0'):\n",
    "        enc_input = tf.constant([enc_padded], dtype=tf.int32)\n",
    "        enc_padding_mask = model.create_padding_mask(enc_input)\n",
    "        enc_out = model.encoder(enc_input, training=False, mask=enc_padding_mask)\n",
    "    enc_outputs.append(enc_out.numpy())\n",
    "    n_real = len(tokenizer.encode(prob, add_special_tokens=True))\n",
    "    print(f\"  '{prob[:50]}' ({n_real} subword tokens) â†’ \"\n",
    "          f\"mean={enc_out.numpy().mean():.4f}, std={enc_out.numpy().std():.4f}, \"\n",
    "          f\"norm={np.linalg.norm(enc_out.numpy()):.2f}\")\n",
    "\n",
    "from numpy.linalg import norm\n",
    "for i in range(len(enc_outputs)):\n",
    "    for j in range(i+1, len(enc_outputs)):\n",
    "        a, b = enc_outputs[i].flatten(), enc_outputs[j].flatten()\n",
    "        cos_sim = np.dot(a, b) / (norm(a) * norm(b) + 1e-8)\n",
    "        print(f\"  Cosine sim [{i}] vs [{j}]: {cos_sim:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DIAGNÃ“STICO 3: Â¿Cross-attention selectiva? (v3)\")\n",
    "print(\"  (v1/v2 tenÃ­an entropy ~1.000 = 100% uniforme)\")\n",
    "print(\"  (v3 entrenÃ³ con diversity loss para romper el colapso)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "prob = \"What is 5 + 3?\"\n",
    "enc_tokens = tokenizer.encode(prob, add_special_tokens=True)\n",
    "enc_padded = _pad_to_length(enc_tokens, config.max_encoder_len, tokenizer.pad_token_id)\n",
    "n_real = len(enc_tokens)\n",
    "with tf.device('/CPU:0'):\n",
    "    enc_input = tf.constant([enc_padded], dtype=tf.int32)\n",
    "    dec_input = tf.constant([[tokenizer.start_token_id]], dtype=tf.int32)\n",
    "\n",
    "    enc_padding_mask = model.create_padding_mask(enc_input)\n",
    "    dec_padding_mask = model.create_padding_mask(enc_input)\n",
    "    look_ahead_mask = model.create_look_ahead_mask(tf.shape(dec_input)[1])\n",
    "    dec_target_padding_mask = model.create_padding_mask(dec_input)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "    enc_output = model.encoder(enc_input, training=False, mask=enc_padding_mask)\n",
    "    dec_output, attn_weights = model.decoder(\n",
    "        dec_input, enc_output, training=False,\n",
    "        look_ahead_mask=combined_mask, padding_mask=dec_padding_mask\n",
    "    )\n",
    "\n",
    "for layer_name, weights in attn_weights.items():\n",
    "    if 'block2' in layer_name:\n",
    "        w = weights.numpy()[0]  # (heads, tar_len, inp_len)\n",
    "        real_w = w[:, :, :n_real]\n",
    "        pad_w = w[:, :, n_real:]\n",
    "        total = real_w.sum() + pad_w.sum()\n",
    "        print(f\"  {layer_name}: tokens reales={real_w.sum()/total:.1%}, padding={pad_w.sum()/total:.1%}\")\n",
    "        avg = w.mean(axis=0)[0, :n_real]\n",
    "        avg = avg / (avg.sum() + 1e-10)\n",
    "        top_pos = avg.argsort()[-5:][::-1]\n",
    "        tokens_str = [tokenizer.decode([enc_padded[p]], skip_special_tokens=False) for p in top_pos]\n",
    "        entropy = -np.sum(avg * np.log(avg + 1e-10))\n",
    "        max_entropy = np.log(n_real)\n",
    "        print(f\"    Entropy: {entropy:.3f} / {max_entropy:.3f} (max) = {entropy/max_entropy:.1%} â†’ \"\n",
    "              f\"{'âš ï¸ UNIFORME' if entropy/max_entropy > 0.95 else 'âœ… Selectiva'}\")\n",
    "        print(f\"    Top-5: {list(zip(tokens_str, [f'{avg[p]:.4f}' for p in top_pos]))}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DIAGNÃ“STICO 4: Teacher forcing â€” Â¿puede continuar? (v3)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "prob = \"What is 5 + 3?\"\n",
    "partial = \"Step 1: 5 + 3 = \"\n",
    "enc_tokens = tokenizer.encode(prob, add_special_tokens=True)\n",
    "enc_padded = _pad_to_length(enc_tokens, config.max_encoder_len, pad_id)\n",
    "enc_input = tf.constant([enc_padded], dtype=tf.int32)\n",
    "\n",
    "sol_tokens = [tokenizer.start_token_id] + tokenizer.encode(partial, add_special_tokens=False)\n",
    "generated = list(sol_tokens)\n",
    "\n",
    "for _ in range(80):\n",
    "    if len(generated) >= max_dec:\n",
    "        break\n",
    "    dec_padded = generated + [pad_id] * (max_dec - len(generated))\n",
    "    dec_input = tf.constant([dec_padded[:max_dec]], dtype=tf.int32)\n",
    "    predictions = predict_step(enc_input, dec_input)\n",
    "    pos = len(generated) - 1\n",
    "    next_id = int(np.argmax(predictions[0, pos, :].numpy()))\n",
    "    if next_id == tokenizer.end_token_id:\n",
    "        break\n",
    "    generated.append(next_id)\n",
    "\n",
    "full_text = tokenizer.decode(generated[1:], skip_special_tokens=True)\n",
    "print(f\"  Problema: '{prob}'\")\n",
    "print(f\"  Teacher forcing: '{partial}'\")\n",
    "print(f\"  ContinuaciÃ³n: '{full_text[:200]}'\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"â•â•â• RESUMEN DEL DIAGNÃ“STICO v3 â•â•â•\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "  RESULTADOS v3 (BPE subword + Answer Head + Diversity Loss):\n",
    "\n",
    "  âœ… LOGROS PRINCIPALES:\n",
    "  â†’ Cross-attention SELECTIVA (entropy 0.52-0.74 vs 1.000 en v1/v2)\n",
    "  â†’ Genera soluciones con formato Step/Answer correcto\n",
    "  â†’ FÃ³rmulas de fÃ­sica correctas (F=ma, V=IR, KE=Â½mvÂ², v=fÎ», d=vÃ—t)\n",
    "  â†’ Answer head predice valores numÃ©ricos razonables\n",
    "  â†’ Token accuracy ~70% (subword tokens significativos)\n",
    "  â†’ Exact match 3.0% (textual) / 3.5% (numÃ©rico)\n",
    "\n",
    "  âš ï¸ LIMITACIONES:\n",
    "  â†’ NÃºmeros especÃ­ficos casi siempre incorrectos en problemas de math\n",
    "  â†’ El modelo genera soluciones memoryzadas cuando no reconoce el patrÃ³n\n",
    "  â†’ 5,729 ejemplos de entrenamiento insuficientes para generalizaciÃ³n numÃ©rica\n",
    "  â†’ Se necesitarÃ­a un mecanismo de copia (copy mechanism) para copiar \n",
    "    nÃºmeros del input al output\n",
    "\n",
    "  Â¿POR QUÃ‰ se rompiÃ³ la cross-attention en v3? (vs colapsada en v1/v2)\n",
    "  â†’ Entrenamiento en 3 fases:\n",
    "    1. Fase 1: Encoder pre-entrenado con answer head (gradiente directo)\n",
    "    2. Fase 2: Cross-attention reinicializada + diversity loss alta\n",
    "    3. Fase 3: Fine-tuning completo con learning rate baja\n",
    "\"\"\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ab32bb",
   "metadata": {},
   "source": [
    "## 3. Interfaz Gradio â€” Demo Interactiva\n",
    "\n",
    "Layout:\n",
    "- **Columna izquierda**: Entrada del problema, selector de dominio, botÃ³n de resolver, soluciÃ³n generada, respuesta destacada\n",
    "- **Columna derecha**: MÃ©tricas de generaciÃ³n, predicciÃ³n del answer head, detalles de arquitectura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ab95025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/melissa/.local/lib/python3.10/site-packages/gradio/analytics.py:106: UserWarning: IMPORTANT: You are using gradio version 4.44.0, however version 4.44.1 is available, please upgrade. \n",
      "--------\n",
      "  warnings.warn(\n",
      "/home/melissa/.local/lib/python3.10/site-packages/keras/src/layers/layer.py:970: UserWarning: Layer 'xla_dropout_24' (of type XLADropout) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/melissa/.local/lib/python3.10/site-packages/keras/src/layers/layer.py:970: UserWarning: Layer 'xla_dropout_26' (of type XLADropout) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/melissa/.local/lib/python3.10/site-packages/keras/src/layers/layer.py:970: UserWarning: Layer 'xla_dropout_28' (of type XLADropout) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/melissa/.local/lib/python3.10/site-packages/keras/src/layers/layer.py:970: UserWarning: Layer 'xla_dropout_32' (of type XLADropout) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/melissa/.local/lib/python3.10/site-packages/keras/src/layers/layer.py:970: UserWarning: Layer 'xla_dropout_35' (of type XLADropout) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/melissa/.local/lib/python3.10/site-packages/keras/src/layers/layer.py:970: UserWarning: Layer 'xla_dropout_38' (of type XLADropout) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/melissa/.local/lib/python3.10/site-packages/keras/src/layers/layer.py:970: UserWarning: Layer 'xla_dropout_41' (of type XLADropout) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "\n",
    "\n",
    "def create_training_plot():\n",
    "    \"\"\"Crea grÃ¡fica de entrenamiento y la guarda como archivo temporal.\"\"\"\n",
    "    if not history:\n",
    "        return None\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(14, 3.5))\n",
    "\n",
    "    # Loss\n",
    "    axes[0].plot(history['train_loss'], label='Train', linewidth=2, color='#4f46e5')\n",
    "    if history.get('val_loss'):\n",
    "        axes[0].plot(history['val_loss'], label='Val', linewidth=2, color='#f97316')\n",
    "    axes[0].set_title('Loss', fontsize=13, fontweight='bold')\n",
    "    axes[0].set_xlabel('Ã‰poca')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Accuracy\n",
    "    axes[1].plot(history['train_accuracy'], label='Train', linewidth=2, color='#4f46e5')\n",
    "    if history.get('val_accuracy'):\n",
    "        axes[1].plot(history['val_accuracy'], label='Val', linewidth=2, color='#f97316')\n",
    "    axes[1].set_title('Token Accuracy (subword)', fontsize=13, fontweight='bold')\n",
    "    axes[1].set_xlabel('Ã‰poca')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Answer MAE\n",
    "    if history.get('train_answer_mae'):\n",
    "        axes[2].plot(history['train_answer_mae'], label='Train MAE', linewidth=2, color='#10b981')\n",
    "        axes[2].set_title('Answer Head MAE', fontsize=13, fontweight='bold')\n",
    "        axes[2].set_xlabel('Ã‰poca')\n",
    "        axes[2].legend()\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    tmp = tempfile.NamedTemporaryFile(suffix='.png', delete=False)\n",
    "    plt.savefig(tmp.name, format='png', dpi=100, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    return tmp.name\n",
    "\n",
    "\n",
    "def extract_answer_highlight(text):\n",
    "    \"\"\"Extrae la lÃ­nea Answer: y la resalta.\"\"\"\n",
    "    match = re.search(r\"(Answer:\\s*.+?)(?:\\n|$)\", text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return \"No se detectÃ³ lÃ­nea Answer:\"\n",
    "\n",
    "\n",
    "def tutor_interface(problem, domain):\n",
    "    \"\"\"Wrapper para Gradio: genera respuesta con mÃ©tricas.\"\"\"\n",
    "    result = generate_with_metrics(problem, max_length=config.max_decoder_len)\n",
    "\n",
    "    if result['confidence'] >= 0.8:\n",
    "        conf_indicator = \"ğŸŸ¢ Alta\"\n",
    "    elif result['confidence'] >= 0.5:\n",
    "        conf_indicator = \"ğŸŸ¡ Media\"\n",
    "    else:\n",
    "        conf_indicator = \"ğŸ”´ Baja\"\n",
    "\n",
    "    answer_line = extract_answer_highlight(result['text'])\n",
    "    head_info = f\"ğŸ§  Answer Head â‰ˆ {result['answer_head_pred']}\" if result.get('answer_head_pred') is not None else \"\"\n",
    "    if head_info:\n",
    "        answer_line += f\"\\n\\n{head_info}\"\n",
    "\n",
    "    return (\n",
    "        result['text'],\n",
    "        f\"**{answer_line}**\",\n",
    "        result['confidence'],\n",
    "        conf_indicator,\n",
    "        f\"{result['perplexity']:.2f}\",\n",
    "        f\"{result['inference_time']*1000:.0f}\",\n",
    "        result['num_tokens']\n",
    "    )\n",
    "\n",
    "\n",
    "# === Dataset stats ===\n",
    "dataset_stats = \"\"\n",
    "try:\n",
    "    with open(os.path.join(project_root, 'data', 'combined_easy.json'), 'r') as f:\n",
    "        all_data = json.load(f)\n",
    "    n_math = sum(1 for d in all_data if d.get('domain') == 'math')\n",
    "    n_phys = sum(1 for d in all_data if d.get('domain') == 'physics')\n",
    "    n_train = sum(1 for d in all_data if d.get('split') == 'train')\n",
    "    dataset_stats = f\"Dataset: **{len(all_data):,}** problemas ({n_math:,} math + {n_phys:,} physics) | Train: **{n_train:,}**\"\n",
    "except Exception:\n",
    "    dataset_stats = \"Dataset: combined_easy.json\"\n",
    "\n",
    "# === Evaluation summary ===\n",
    "eval_summary = \"\"\n",
    "if eval_report:\n",
    "    ta = eval_report.get('token_accuracy', {})\n",
    "    em = eval_report.get('exact_match', {})\n",
    "    ah = eval_report.get('answer_head', {})\n",
    "    eval_summary = (f\"Token Acc (test): **{ta.get('test_acc', 0):.1%}** | \"\n",
    "                    f\"Exact Match: **{em.get('textual_pct', 0):.1f}%** | \"\n",
    "                    f\"Numeric: **{em.get('numeric_pct', 0):.1f}%** | \"\n",
    "                    f\"Head MAE: **{ah.get('mae', 0):.1f}**\")\n",
    "\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#  INTERFAZ GRADIO BLOCKS â€” v3 (Modelo Final)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "theme = gr.themes.Soft(primary_hue=\"indigo\", secondary_hue=\"blue\")\n",
    "\n",
    "with gr.Blocks(theme=theme, title=\"Transformer Tutor â€” Math & Physics\") as demo:\n",
    "\n",
    "    gr.Markdown(f\"\"\"\n",
    "    # ğŸ“ Transformer Tutor â€” MatemÃ¡ticas y FÃ­sica\n",
    "\n",
    "    Modelo **Encoder-Decoder Transformer v3** desde cero |\n",
    "    **{num_params:,}** parÃ¡metros | TokenizaciÃ³n: **BPE ({tokenizer.vocab_size} tokens)** |\n",
    "    Dispositivo: **{device}** |\n",
    "    Train: **{final_train_acc:.1%}** | Val: **{final_val_acc:.1%}** |\n",
    "    {eval_summary}\n",
    "\n",
    "    {dataset_stats}\n",
    "    \"\"\")\n",
    "\n",
    "    with gr.Row():\n",
    "\n",
    "        # â•â•â• COLUMNA IZQUIERDA: Entrada / Salida â•â•â•\n",
    "        with gr.Column(scale=3):\n",
    "\n",
    "            with gr.Row():\n",
    "                input_box = gr.Textbox(\n",
    "                    label=\"ğŸ“ Problema\",\n",
    "                    placeholder=\"Ej: A car travels at 60 km/h for 3 hours. How far does it go?\",\n",
    "                    lines=3,\n",
    "                    scale=4\n",
    "                )\n",
    "                domain_selector = gr.Radio(\n",
    "                    choices=[\"math\", \"physics\"],\n",
    "                    value=\"math\",\n",
    "                    label=\"ğŸ”¬ Dominio\",\n",
    "                    scale=1\n",
    "                )\n",
    "\n",
    "            with gr.Row():\n",
    "                solve_btn = gr.Button(\"ğŸ¤” Resolver\", variant=\"primary\", scale=3)\n",
    "                clear_btn = gr.Button(\"ğŸ—‘ï¸ Limpiar\", variant=\"secondary\", scale=1)\n",
    "\n",
    "            output_box = gr.Textbox(\n",
    "                label=\"ğŸ¤– SoluciÃ³n (paso a paso)\",\n",
    "                lines=8,\n",
    "                show_copy_button=True\n",
    "            )\n",
    "\n",
    "            answer_box = gr.Markdown(\n",
    "                label=\"Respuesta\",\n",
    "                value=\"*Esperando problema...*\"\n",
    "            )\n",
    "\n",
    "            gr.Markdown(\"### ğŸ“š Ejemplos\")\n",
    "            with gr.Tab(\"MatemÃ¡ticas\"):\n",
    "                gr.Examples(\n",
    "                    examples=[\n",
    "                        [\"Janet has 3 apples and buys 5 more. How many apples does she have?\", \"math\"],\n",
    "                        [\"A store sells pencils for $2 each. Tom buys 7 pencils. How much does he spend?\", \"math\"],\n",
    "                        [\"Sarah has 24 cookies and shares equally among 6 friends. How many does each get?\", \"math\"],\n",
    "                        [\"A farmer has 15 chickens. Each lays 2 eggs per day. How many eggs in 3 days?\", \"math\"],\n",
    "                        [\"John has $50. He buys a book for $12 and a pen for $3. How much is left?\", \"math\"],\n",
    "                    ],\n",
    "                    inputs=[input_box, domain_selector],\n",
    "                    label=\"\"\n",
    "                )\n",
    "            with gr.Tab(\"FÃ­sica\"):\n",
    "                gr.Examples(\n",
    "                    examples=[\n",
    "                        [\"A car travels at 60 km/h for 3 hours. How far does it go?\", \"physics\"],\n",
    "                        [\"A 10 kg box is pushed with a force of 50 N. What is its acceleration?\", \"physics\"],\n",
    "                        [\"A wave has frequency 100 Hz and wavelength 0.1 m. What is the wave speed?\", \"physics\"],\n",
    "                        [\"A circuit has voltage 12V and resistance 4Î©. What is the current?\", \"physics\"],\n",
    "                        [\"What is the kinetic energy of a 4 kg object moving at 5 m/s?\", \"physics\"],\n",
    "                    ],\n",
    "                    inputs=[input_box, domain_selector],\n",
    "                    label=\"\"\n",
    "                )\n",
    "\n",
    "        # â•â•â• COLUMNA DERECHA: MÃ©tricas â•â•â•\n",
    "        with gr.Column(scale=2):\n",
    "\n",
    "            gr.Markdown(\"### ğŸ“Š MÃ©tricas de GeneraciÃ³n\")\n",
    "            with gr.Group():\n",
    "                conf_slider = gr.Slider(0, 1, value=0, label=\"Confianza\", interactive=False)\n",
    "                conf_text = gr.Textbox(label=\"Estado\", value=\"â³ Esperando...\", interactive=False)\n",
    "                perp_text = gr.Textbox(label=\"Perplexity\", value=\"--\", interactive=False)\n",
    "                time_text = gr.Textbox(label=\"Tiempo (ms)\", value=\"--\", interactive=False)\n",
    "                tokens_num = gr.Number(label=\"Tokens generados\", value=0)\n",
    "\n",
    "            gr.Markdown(\"### ğŸ§  Arquitectura v3\")\n",
    "            with gr.Row():\n",
    "                gr.Textbox(str(config_dict['num_layers']), label=\"Capas\", interactive=False)\n",
    "                gr.Textbox(str(config_dict['d_model']), label=\"d_model\", interactive=False)\n",
    "            with gr.Row():\n",
    "                gr.Textbox(str(config_dict['num_heads']), label=\"Heads\", interactive=False)\n",
    "                gr.Textbox(str(config_dict.get('dff', 1024)), label=\"dff\", interactive=False)\n",
    "            with gr.Row():\n",
    "                gr.Textbox(str(config_dict['dropout_rate']), label=\"Dropout\", interactive=False)\n",
    "                gr.Textbox(str(tokenizer.vocab_size), label=\"Vocab (BPE)\", interactive=False)\n",
    "\n",
    "    # â”€â”€ AcordeÃ³n: Detalles TÃ©cnicos â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    with gr.Accordion(\"ğŸ” Detalles TÃ©cnicos\", open=False):\n",
    "        gr.Markdown(f\"\"\"\n",
    "**Componentes implementados desde cero (sin usar capas pre-hechas):**\n",
    "- Scaled Dot-Product Attention: `softmax(QK^T / âˆšd_k) V`\n",
    "- Multi-Head Attention ({config_dict['num_heads']} cabezas, depth={config_dict['d_model'] // config_dict['num_heads']})\n",
    "- Positional Encoding sinusoidal\n",
    "- Encoder: Self-Attn â†’ Add&Norm â†’ FFN â†’ Add&Norm (Ã—{config_dict['num_layers']})\n",
    "- Decoder: Masked Self-Attn â†’ Cross-Attn â†’ FFN (Ã—{config_dict['num_layers']})\n",
    "- Answer Regression Head: MLP que predice el valor numÃ©rico de la respuesta\n",
    "\n",
    "**Dataset (easy subset):**\n",
    "- GSM8K: problemas de aritmÃ©tica con soluciones paso a paso\n",
    "- FÃ­sica (templates): problemas de cinemÃ¡tica, dinÃ¡mica, termodinÃ¡mica, ondas, electricidad\n",
    "- Total: ~6,881 problemas fÃ¡ciles (train: ~5,729)\n",
    "\n",
    "**TokenizaciÃ³n:** SentencePiece BPE ({tokenizer.vocab_size} tokens, split_digits=True, byte_fallback=True)\n",
    "- Encoder max length: {config.max_encoder_len} subword tokens\n",
    "- Decoder max length: {config.max_decoder_len} subword tokens\n",
    "\n",
    "**Entrenamiento v3 (3 fases):**\n",
    "1. **Fase 1 â€” Encoder pre-training**: Decoder congelado, solo encoder + answer head (30 epochs)\n",
    "2. **Fase 2 â€” Cross-attention**: Encoder congelado, cross-attention reinicializada, diversity loss alta (100 epochs)\n",
    "3. **Fase 3 â€” Fine-tuning**: Todo descongelado con learning rate baja (50 epochs)\n",
    "- Optimizador: Adam (Î²â‚=0.9, Î²â‚‚=0.98) con warmup + inverse sqrt decay\n",
    "- Loss combinada: seq2seq + Î»Â·answer_loss + Î¼Â·diversity_loss\n",
    "- GPU: NVIDIA RTX 5060 (Blackwell) con workarounds XLA/CUDA\n",
    "\n",
    "**Inferencia:**\n",
    "- Forward pass compilado con @tf.function + XLA (Blackwell compatible)\n",
    "- DecodificaciÃ³n: Top-k sampling (k=10, temperature=0.3)\n",
    "- Repetition penalty: 1.3\n",
    "        \"\"\")\n",
    "\n",
    "    # â”€â”€ AcordeÃ³n: Curvas de Entrenamiento â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    if history:\n",
    "        with gr.Accordion(\"ğŸ“ˆ Curvas de Entrenamiento\", open=False):\n",
    "            plot_img = create_training_plot()\n",
    "            if plot_img:\n",
    "                gr.Image(value=plot_img, label=\"Historia de entrenamiento v3\")\n",
    "            gr.Markdown(f\"\"\"\n",
    "**Resumen del entrenamiento v3 (Fase 3 â€” fine-tuning):**\n",
    "- Ã‰pocas completadas: {len(history.get('train_loss', []))}\n",
    "- Loss final â€” Train: {history.get('train_loss', [0])[-1]:.4f} | Val: {history.get('val_loss', [0])[-1]:.4f}\n",
    "- Accuracy final â€” Train: {history.get('train_accuracy', [0])[-1]:.4f} | Val: {history.get('val_accuracy', [0])[-1]:.4f}\n",
    "- Answer MAE final: {history.get('train_answer_mae', [0])[-1]:.1f}\n",
    "            \"\"\")\n",
    "\n",
    "    # â”€â”€ AcordeÃ³n: EvaluaciÃ³n â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    with gr.Accordion(\"ğŸ“‹ Resultados de EvaluaciÃ³n\", open=False):\n",
    "        if eval_report:\n",
    "            ta = eval_report.get('token_accuracy', {})\n",
    "            em = eval_report.get('exact_match', {})\n",
    "            ah = eval_report.get('answer_head', {})\n",
    "            at = eval_report.get('cross_attention_entropy', {})\n",
    "            eval_md = f\"\"\"\n",
    "**Token-level Accuracy** (predicciÃ³n del siguiente subword token):\n",
    "- ValidaciÃ³n: {ta.get('val_acc', 0):.1%}\n",
    "- Test: {ta.get('test_acc', 0):.1%}\n",
    "\n",
    "**Exact Match (lÃ­nea Answer:)** sobre {em.get('textual_total', 0)} problemas de test:\n",
    "- Textual: **{em.get('textual_correct', 0)}/{em.get('textual_total', 0)} = {em.get('textual_pct', 0):.1f}%**\n",
    "- NumÃ©rico (Â±0.5): **{em.get('numeric_correct', 0)}/{em.get('numeric_total', 0)} = {em.get('numeric_pct', 0):.1f}%**\n",
    "\n",
    "**Answer Head Regression:**\n",
    "- MAE: {ah.get('mae', 0):.1f}\n",
    "- Exactos (Â±0.5): {ah.get('exact', 0)}/{ah.get('total', 0)} = {ah.get('exact_pct', 0):.1f}%\n",
    "\n",
    "**Cross-Attention Entropy** (0.0 = perfectly focused, 1.0 = uniform/collapsed):\n",
    "\"\"\"\n",
    "            for layer, val in sorted(at.items()):\n",
    "                status = \"âš ï¸ UNIFORME\" if val > 0.95 else \"âœ… SELECTIVA\" if val < 0.80 else \"ğŸŸ¡ PARCIAL\"\n",
    "                eval_md += f\"- {layer}: **{val:.3f}** â†’ {status}\\n\"\n",
    "\n",
    "            eval_md += \"\\n**Ejemplos de predicciones:**\\n\"\n",
    "            for ex in eval_report.get('examples', [])[:8]:\n",
    "                icon = \"âœ…\" if ex.get('correct', False) else \"âŒ\"\n",
    "                eval_md += f\"- {icon} [{ex.get('domain','')}] \\\"{ex.get('problem','')[:60]}...\\\" â†’ Esperado: `{ex.get('ref_answer','')}` | Predicho: `{ex.get('pred_answer','')}`\\n\"\n",
    "\n",
    "            gr.Markdown(eval_md)\n",
    "        else:\n",
    "            gr.Markdown(\"*No se encontrÃ³ `evaluation_report.json`.*\")\n",
    "\n",
    "    # â”€â”€ AcordeÃ³n: EvoluciÃ³n v1 â†’ v2 â†’ v3 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    with gr.Accordion(\"ğŸ”„ EvoluciÃ³n v1 â†’ v2 â†’ v3\", open=False):\n",
    "        gr.Markdown(\"\"\"\n",
    "### EvoluciÃ³n del modelo\n",
    "\n",
    "| Aspecto | v1 (char-level) | v2 (BPE subword) | v3 (answer head + diversity) |\n",
    "|---------|----------------|-----------------|------------------------------|\n",
    "| **TokenizaciÃ³n** | Character (135 tokens) | BPE (4,000 tokens) | BPE (4,000 tokens) |\n",
    "| **ParÃ¡metros** | ~5.5M | ~7.5M | ~10.5M |\n",
    "| **Token Accuracy** | ~82% (chars) | ~70% (subwords) | ~70% (subwords) |\n",
    "| **Exact Match** | 0% | ~2% | **~3.5%** |\n",
    "| **Cross-attention** | âš ï¸ Colapsada (1.000) | âš ï¸ Colapsada (1.000) | **âœ… Selectiva (0.52-0.74)** |\n",
    "| **FÃ³rmulas fÃ­sica** | âŒ No | ğŸŸ¡ A veces | **âœ… Consistentes** |\n",
    "| **Answer Head** | âŒ No | âŒ No | **âœ… MAE ~299** |\n",
    "\n",
    "### Â¿QuÃ© cambiÃ³ en cada versiÃ³n?\n",
    "- **v1 â†’ v2**: TokenizaciÃ³n BPE (SentencePiece) en lugar de character-level\n",
    "- **v2 â†’ v3**: Answer regression head + diversity loss + entrenamiento en 3 fases\n",
    "\n",
    "### Logro principal de v3:\n",
    "**Romper el colapso de cross-attention** (entropy 1.000 â†’ 0.52-0.74). En v1 y v2, el decoder\n",
    "ignoraba completamente el encoder. En v3, el decoder atiende selectivamente a tokens del input.\n",
    "        \"\"\")\n",
    "\n",
    "    # â”€â”€ AcordeÃ³n: Limitaciones â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    with gr.Accordion(\"âš ï¸ Limitaciones y AnÃ¡lisis Honesto\", open=True):\n",
    "        gr.Markdown(\"\"\"\n",
    "### Â¿QuÃ© puede y quÃ© NO puede este modelo?\n",
    "\n",
    "**âœ… Lo que el modelo SÃ demuestra:**\n",
    "1. **Arquitectura Transformer correcta**: Encoder-Decoder completo implementado desde cero\n",
    "2. **TokenizaciÃ³n BPE funcional**: SentencePiece con 4,000 tokens, split_digits, byte_fallback\n",
    "3. **Formato aprendido**: Genera consistentemente \"Step 1: ... <<calc>> ... Answer: ...\"\n",
    "4. **FÃ³rmulas de fÃ­sica**: Identifica y aplica F=ma, V=IR, KE=Â½mvÂ², v=fÎ», d=vÃ—t correctamente\n",
    "5. **Cross-attention selectiva**: Primer modelo que realmente atiende al input (entropy 0.52-0.74)\n",
    "6. **Answer Head**: PredicciÃ³n numÃ©rica directa sin generar tokens\n",
    "7. **Pipeline completo de ML**: Datos â†’ tokenizaciÃ³n â†’ 3 fases de entrenamiento â†’ evaluaciÃ³n â†’ demo\n",
    "\n",
    "**âŒ Lo que el modelo NO puede hacer:**\n",
    "1. **CÃ¡lculos numÃ©ricos correctos**: Los nÃºmeros especÃ­ficos son casi siempre incorrectos (~3.5% exact match)\n",
    "2. **GeneralizaciÃ³n a problemas nuevos**: Tiende a generar soluciones memorizadas\n",
    "3. **Copiar nÃºmeros del input**: No tiene mecanismo de copia (copy mechanism / pointer network)\n",
    "\n",
    "### Â¿Por quÃ© los nÃºmeros siguen incorrectos?\n",
    "\n",
    "- **5,729 ejemplos** de entrenamiento son insuficientes para ~10.5M parÃ¡metros\n",
    "- Sin **copy mechanism**, el modelo debe \"re-generar\" cada nÃºmero del input token por token\n",
    "- Los Transformers pequeÃ±os son buenos para aprender **patrones estructurales** (fÃ³rmulas, formato)\n",
    "  pero no para **cÃ¡lculo numÃ©rico exacto**\n",
    "- Modelos como GPT-4 usan 175B+ parÃ¡metros y trillones de tokens de entrenamiento\n",
    "\n",
    "### Valor pedagÃ³gico\n",
    "\n",
    "Este proyecto demuestra la implementaciÃ³n completa de un Transformer from-scratch,\n",
    "incluyendo tÃ©cnicas avanzadas como:\n",
    "- Entrenamiento multi-fase con freezing/unfreezing selectivo\n",
    "- ReinicializaciÃ³n de cross-attention para romper simetrÃ­a\n",
    "- Diversity loss para prevenir colapso de atenciÃ³n\n",
    "- Compatibilidad con GPU Blackwell (workarounds XLA/CUDA)\n",
    "- EvaluaciÃ³n honesta con mÃ©tricas apropiadas\n",
    "        \"\"\")\n",
    "\n",
    "    # â”€â”€ Eventos â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    solve_btn.click(\n",
    "        fn=tutor_interface,\n",
    "        inputs=[input_box, domain_selector],\n",
    "        outputs=[output_box, answer_box, conf_slider, conf_text, perp_text, time_text, tokens_num]\n",
    "    )\n",
    "\n",
    "    clear_btn.click(\n",
    "        fn=lambda: (\"\", \"math\", \"\", \"*Esperando problema...*\", 0, \"â³ Esperando...\", \"--\", \"--\", 0),\n",
    "        inputs=None,\n",
    "        outputs=[input_box, domain_selector, output_box, answer_box, conf_slider, conf_text, perp_text, time_text, tokens_num]\n",
    "    )\n",
    "\n",
    "# Lanzar\n",
    "demo.launch(inline=True, share=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
